{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "949dae75-7cc1-4e51-ae83-20e89b9f7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import layers\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import os, shutil\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd64620-f05a-4e11-b576-a0e27e1b6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\david\\\\OneDrive\\\\Inholland\\\\DeepLearning\\\\Assignment 1\\\\palmerpenguins_original.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fb98f-c73f-4f7e-9141-83a88dc26038",
   "metadata": {},
   "source": [
    "# 1 Preprocessing \n",
    "Firstly the data must be analysed and cleaned.\n",
    "Thereafter, to decide what variables to use as predictors we must employ descriptive statistics to analyse what variables are relevant.\n",
    "# 1.1 Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e24a9c-97ac-4a1c-b513-69a2dcdea479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts for each column:\n",
      "species               0\n",
      "island                0\n",
      "bill_length_mm        2\n",
      "bill_depth_mm         2\n",
      "flipper_length_mm     2\n",
      "body_mass_g           2\n",
      "sex                  11\n",
      "year                  0\n",
      "dtype: int64\n",
      "Rows with null values:\n",
      "    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "3    Adelie  Torgersen             NaN            NaN                NaN   \n",
      "8    Adelie  Torgersen            34.1           18.1              193.0   \n",
      "9    Adelie  Torgersen            42.0           20.2              190.0   \n",
      "10   Adelie  Torgersen            37.8           17.1              186.0   \n",
      "11   Adelie  Torgersen            37.8           17.3              180.0   \n",
      "47   Adelie      Dream            37.5           18.9              179.0   \n",
      "178  Gentoo     Biscoe            44.5           14.3              216.0   \n",
      "218  Gentoo     Biscoe            46.2           14.4              214.0   \n",
      "256  Gentoo     Biscoe            47.3           13.8              216.0   \n",
      "268  Gentoo     Biscoe            44.5           15.7              217.0   \n",
      "271  Gentoo     Biscoe             NaN            NaN                NaN   \n",
      "\n",
      "     body_mass_g  sex  year  \n",
      "3            NaN  NaN  2007  \n",
      "8         3475.0  NaN  2007  \n",
      "9         4250.0  NaN  2007  \n",
      "10        3300.0  NaN  2007  \n",
      "11        3700.0  NaN  2007  \n",
      "47        2975.0  NaN  2007  \n",
      "178       4100.0  NaN  2007  \n",
      "218       4650.0  NaN  2008  \n",
      "256       4725.0  NaN  2009  \n",
      "268       4875.0  NaN  2009  \n",
      "271          NaN  NaN  2009  \n",
      "Shape of the DataFrame:\n",
      "(344, 8)\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column and sum them up\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Print the count of null values for each column\n",
    "print(\"Null value counts for each column:\")\n",
    "print(null_counts)\n",
    "\n",
    "# Find rows with null values\n",
    "rows_with_null = df[df.isnull().any(axis=1)]\n",
    "\n",
    "# Display rows with null values\n",
    "print(\"Rows with null values:\")\n",
    "print(rows_with_null)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(\"Shape of the DataFrame:\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d948793-19cb-49ca-990b-273a54b5aca3",
   "metadata": {},
   "source": [
    "# 1.2 Data Cleaning and encoding\n",
    "Given the results of the null values the decision I decided was to drop the null rows which contain no body_mass as that is the target variable. Furthermore, I decided to add sex to the data based on the mode to compensate for the null values.\n",
    "To do certain statistics on the categorical data the data must be encoded. With Pandas get_dummies uses \"One-Hot encoding\" to encode the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fdd11e2-b44c-40c8-bbb3-25c310118c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species              0\n",
      "island               0\n",
      "bill_length_mm       0\n",
      "bill_depth_mm        0\n",
      "flipper_length_mm    0\n",
      "body_mass_g          0\n",
      "sex                  0\n",
      "year                 0\n",
      "dtype: int64\n",
      "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0              39.1           18.7              181.0       3750.0   \n",
      "1              39.5           17.4              186.0       3800.0   \n",
      "2              40.3           18.0              195.0       3250.0   \n",
      "4              36.7           19.3              193.0       3450.0   \n",
      "5              39.3           20.6              190.0       3650.0   \n",
      "..              ...            ...                ...          ...   \n",
      "339            55.8           19.8              207.0       4000.0   \n",
      "340            43.5           18.1              202.0       3400.0   \n",
      "341            49.6           18.2              193.0       3775.0   \n",
      "342            50.8           19.0              210.0       4100.0   \n",
      "343            50.2           18.7              198.0       3775.0   \n",
      "\n",
      "     species_Chinstrap  species_Gentoo  island_Dream  island_Torgersen  \\\n",
      "0                    0               0             0                 1   \n",
      "1                    0               0             0                 1   \n",
      "2                    0               0             0                 1   \n",
      "4                    0               0             0                 1   \n",
      "5                    0               0             0                 1   \n",
      "..                 ...             ...           ...               ...   \n",
      "339                  1               0             1                 0   \n",
      "340                  1               0             1                 0   \n",
      "341                  1               0             1                 0   \n",
      "342                  1               0             1                 0   \n",
      "343                  1               0             1                 0   \n",
      "\n",
      "     sex_male  year_2008  year_2009  \n",
      "0           1          0          0  \n",
      "1           0          0          0  \n",
      "2           0          0          0  \n",
      "4           0          0          0  \n",
      "5           1          0          0  \n",
      "..        ...        ...        ...  \n",
      "339         1          0          1  \n",
      "340         0          0          1  \n",
      "341         1          0          1  \n",
      "342         1          0          1  \n",
      "343         0          0          1  \n",
      "\n",
      "[342 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=[\"body_mass_g\"], inplace=True)\n",
    "df[\"sex\"].fillna(df[\"sex\"].mode()[0], inplace=True)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "encoded_values = pd.get_dummies(df, columns=[\"species\", \"island\",\"sex\",\"year\"], drop_first=True)\n",
    "\n",
    "print(encoded_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c1ef7-f218-4789-954d-8db9464abe9f",
   "metadata": {},
   "source": [
    "# 1.3 Data Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14814fd1-79fb-4a58-ab3f-b043ecd934e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    342.000000\n",
      "mean      43.921930\n",
      "std        5.459584\n",
      "min       32.100000\n",
      "25%       39.225000\n",
      "50%       44.450000\n",
      "75%       48.500000\n",
      "max       59.600000\n",
      "Name: bill_length_mm, dtype: float64 2\n",
      "count    342.000000\n",
      "mean      17.151170\n",
      "std        1.974793\n",
      "min       13.100000\n",
      "25%       15.600000\n",
      "50%       17.300000\n",
      "75%       18.700000\n",
      "max       21.500000\n",
      "Name: bill_depth_mm, dtype: float64 2\n",
      "count    342.000000\n",
      "mean     200.915205\n",
      "std       14.061714\n",
      "min      172.000000\n",
      "25%      190.000000\n",
      "50%      197.000000\n",
      "75%      213.000000\n",
      "max      231.000000\n",
      "Name: flipper_length_mm, dtype: float64 2\n",
      "count     342.000000\n",
      "mean     4201.754386\n",
      "std       801.954536\n",
      "min      2700.000000\n",
      "25%      3550.000000\n",
      "50%      4050.000000\n",
      "75%      4750.000000\n",
      "max      6300.000000\n",
      "Name: body_mass_g, dtype: float64 2\n",
      "Adelie       151\n",
      "Gentoo       123\n",
      "Chinstrap     68\n",
      "Name: species, dtype: int64\n",
      "Biscoe       167\n",
      "Dream        124\n",
      "Torgersen     51\n",
      "Name: island, dtype: int64\n",
      "male      177\n",
      "female    165\n",
      "Name: sex, dtype: int64\n",
      "2009    119\n",
      "2008    114\n",
      "2007    109\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stats_bill_length = df['bill_length_mm'].describe()\n",
    "stats_bill_depth = df['bill_depth_mm'].describe()\n",
    "stats_flipper_length = df['flipper_length_mm'].describe()\n",
    "\n",
    "#target variable \n",
    "stats_body_mass = df['body_mass_g'].describe()\n",
    "\n",
    "print(stats_bill_length,2)\n",
    "print(stats_bill_depth,2)\n",
    "print(stats_flipper_length,2)\n",
    "\n",
    "print(stats_body_mass,2)\n",
    "\n",
    "species_count = df['species'].value_counts()\n",
    "island_count = df['island'].value_counts()\n",
    "sex_count = df['sex'].value_counts()\n",
    "year_count = df['year'].value_counts()\n",
    "\n",
    "print(species_count)\n",
    "print(island_count)\n",
    "print(sex_count)\n",
    "print(year_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63158220-d4ab-4d13-a8e8-96322da4cff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficients between target variable and predictor variables:\n",
      "bill_length_mm       0.595110\n",
      "bill_depth_mm       -0.471916\n",
      "flipper_length_mm    0.871202\n",
      "body_mass_g          1.000000\n",
      "species_Chinstrap   -0.291561\n",
      "species_Gentoo       0.818198\n",
      "island_Dream        -0.460411\n",
      "island_Torgersen    -0.258979\n",
      "sex_male             0.409315\n",
      "year_2008            0.057319\n",
      "year_2009            0.007790\n",
      "Name: body_mass_g, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlation coefficients between target variable and predictor variables\n",
    "correlation_matrix = encoded_values.corr()\n",
    "\n",
    "# Extract correlation coefficients for the target variable\n",
    "target_correlation = correlation_matrix['body_mass_g']\n",
    "\n",
    "# Print correlation coefficients\n",
    "print(\"Correlation coefficients between target variable and predictor variables:\")\n",
    "print(target_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d791fd-98b5-4f35-8279-2c81f7fbd3b6",
   "metadata": {},
   "source": [
    "Given these results it is concluded that flipper length,bill_length and the gentoo species, has a high positive correlation. Meanwhile, the bill_depth and dream island pinguins give a negative correlation. it is also important to note that the sex has a slight correlation to the weight. Because of this the predictor variable that will be selected is flipper length, bill length and species. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745bb9d-0dbf-44de-82b7-9e9cc30f92f1",
   "metadata": {},
   "source": [
    "# 1.4 Splitting Dataset\n",
    "\n",
    "Here the data is split in validation and testing sets to evaluate the model performance during training and after training. The validation set is used to monitor the model's performance during training. The testing set is used to evaluate the final model's performance on unseen data. Furthermore, before splitting the data, Normalization techniques were applied to scale the features to a similar range, usually between 0 and 1 or with a mean of 0 and a standard deviation of 1. Species is not normalized because it already consists of binary values (0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09c80e86-0804-48a4-8864-8373978e9b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor variables (X):\n",
      "   bill_length_mm  flipper_length_mm  species_Chinstrap  species_Gentoo\n",
      "0            39.1              181.0                  0               0\n",
      "1            39.5              186.0                  0               0\n",
      "2            40.3              195.0                  0               0\n",
      "4            36.7              193.0                  0               0\n",
      "5            39.3              190.0                  0               0\n",
      "Gentoo type: uint8\n",
      "Chinstrap type: uint8\n",
      "bill_length type: float64\n",
      "flipper_legth: float64\n",
      "\n",
      "Target variable (y):\n",
      "0    3750.0\n",
      "1    3800.0\n",
      "2    3250.0\n",
      "4    3450.0\n",
      "5    3650.0\n",
      "Name: body_mass_g, dtype: float64\n",
      "(342,)\n",
      "Normalized Predictor variables (X):\n",
      "(342, 4)\n",
      "Normalized target variables (y):\n",
      "(342,)\n",
      "Training set - Predictor variables: (205, 4)\n",
      "Training set - Target variable: (205, 1)\n",
      "Testing set - Predictor variables: (69, 4)\n",
      "Testing set - Target variable: (69, 1)\n",
      "Validation set - Predictor variables: (68, 4)\n",
      "Validation set - Target variable: (68, 1)\n"
     ]
    }
   ],
   "source": [
    "# Selecting predictor variables ('species', 'bill_length_mm', 'flipper_length_mm') and target variable ('body_mass_g')\n",
    "X = df[['species', 'bill_length_mm', 'flipper_length_mm']]\n",
    "y = df['body_mass_g']\n",
    "\n",
    "# One-hot encode categorical variable 'species'\n",
    "X_encoded = pd.get_dummies(X, columns=['species'], drop_first=True)\n",
    "\n",
    "print(\"Predictor variables (X):\")\n",
    "print(X_encoded .head())\n",
    "print(f\"Gentoo type: {X_encoded['species_Gentoo'].dtype}\\n\"\n",
    "      f\"Chinstrap type: {X_encoded['species_Chinstrap'].dtype}\\n\"\n",
    "      f\"bill_length type: {X_encoded['bill_length_mm'].dtype}\\n\"\n",
    "      f\"flipper_legth: {X_encoded['flipper_length_mm'].dtype}\")\n",
    "\n",
    "print(\"\\nTarget variable (y):\")\n",
    "print(y.head())\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical columns of X_encoded\n",
    "numerical_cols = X_encoded.columns.drop(['species_Gentoo', 'species_Chinstrap'])\n",
    "X_encoded[numerical_cols] = scaler.fit_transform(X_encoded[numerical_cols])\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "inputs = np.array(X_encoded)\n",
    "targets = np.array(y_normalized)\n",
    "\n",
    "print(targets.shape)\n",
    "\n",
    "\n",
    "# Print the normalized predictor variables\n",
    "print(\"Normalized Predictor variables (X):\")\n",
    "print(inputs.shape)\n",
    "\n",
    "print(\"Normalized target variables (y):\")\n",
    "print(targets.shape)\n",
    "\n",
    "\n",
    "# Split the dataset into 60% training 20% validation and 20% testing\n",
    "X_train, X_split, y_train, y_split = train_test_split(inputs, targets.reshape(342, 1), test_size=0.4, random_state=42)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_split,y_split,test_size=.5)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training set - Predictor variables:\", X_train.shape)\n",
    "print(\"Training set - Target variable:\", y_train.shape)\n",
    "print(\"Testing set - Predictor variables:\", X_test.shape)\n",
    "print(\"Testing set - Target variable:\", y_test.shape)\n",
    "print(\"Validation set - Predictor variables:\", X_val.shape)\n",
    "print(\"Validation set - Target variable:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5e245-7e90-48cd-89aa-a6811ca0ac89",
   "metadata": {},
   "source": [
    "# 2 A custom neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d8d15-d449-45cd-a9d3-62e6b2acd499",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9d3a41-1efe-4233-8b39-7c95ae48d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def dydxrelu(x):\n",
    "    dx = np.where(x > 0, 1, 0)\n",
    "    return dx\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dydxsig(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent activation function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (numpy.ndarray): Input values.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Hyperbolic tangent values.\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of the hyperbolic tangent activation function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (numpy.ndarray): Input values.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Derivative values.\n",
    "    \"\"\"\n",
    "    return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9307b35-f8f7-46d7-a585-b2b951d08a02",
   "metadata": {},
   "source": [
    "# Loss\n",
    "y= true value                                                 \n",
    "y_hat = predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b2b9bf6-13ab-4a68-97de-641b7ab74864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, y_hat):\n",
    "        return np.mean((y - y_hat) ** 2)\n",
    "\n",
    "def mse_derivative(y, y_hat):\n",
    "    return 2 * (y_hat - y)\n",
    "\n",
    "def mae(y, y_hat):\n",
    "    return np.mean(np.abs(y - y_hat))\n",
    "\n",
    "def mae_derivative(y, y_hat):\n",
    "    diff = y_hat - y\n",
    "    derivative = np.where(diff > 0, 1, -1)\n",
    "    return derivative / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a0659-5355-4fb7-97d0-f193f563675c",
   "metadata": {},
   "source": [
    "# Network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e1b2d445-a437-4cd3-9d2e-5e2531fb5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "    biases_input_hidden = np.zeros((1, hidden_size))\n",
    "    weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "    biases_hidden_output = np.zeros((1, output_size))\n",
    "    return weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output\n",
    "\n",
    "def forward_pass(X, weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output):\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + biases_input_hidden\n",
    "    hidden_layer_output = relu(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + biases_hidden_output\n",
    "\n",
    "    return output_layer_input, hidden_layer_input, hidden_layer_output\n",
    "\n",
    "def backpropagation(X, output, output_delta, hidden_layer_input, hidden_layer_output, weights_hidden_output, biases_hidden_output, weights_input_hidden, biases_input_hidden, learning_rate):\n",
    "    hidden_error = np.dot(output_delta, weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * dydxrelu(hidden_layer_input)\n",
    "    weights_hidden_output -= learning_rate * np.dot(hidden_layer_output.T, output_delta)\n",
    "    biases_hidden_output -= learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "    weights_input_hidden -= learning_rate * np.dot(X.T, hidden_delta)\n",
    "    biases_input_hidden -= learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "\n",
    "def train_model(X_train, y_train, hidden_size=10, learning_rate=0.0001, epochs=1000):\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = y_train.shape[1]\n",
    "    weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output = initialize_weights(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        output_layer_input, hidden_layer_input, hidden_layer_output = forward_pass(X_train, weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = mae(y_train, output_layer_input)\n",
    "    \n",
    "        # Backpropagation\n",
    "        output_error = mse_derivative(y_train, output_layer_input)\n",
    "        backpropagation(X_train, output_layer_input, output_error, hidden_layer_input, hidden_layer_output, weights_hidden_output, biases_hidden_output, weights_input_hidden, biases_input_hidden, learning_rate)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")\n",
    "\n",
    "    return weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cd964-e681-43e2-b244-44a54238e837",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ad12e410-c516-432a-9ed7-d940d05ffe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 2.35636224413465\n",
      "Epoch 2/1000, Loss: 1.3310178523222298\n",
      "Epoch 3/1000, Loss: 1.0013798803838596\n",
      "Epoch 4/1000, Loss: 0.8698427752099659\n",
      "Epoch 5/1000, Loss: 0.8054843165986524\n",
      "Epoch 6/1000, Loss: 0.7617883062366858\n",
      "Epoch 7/1000, Loss: 0.7280204944787032\n",
      "Epoch 8/1000, Loss: 0.6994482731871595\n",
      "Epoch 9/1000, Loss: 0.6742269058056858\n",
      "Epoch 10/1000, Loss: 0.651897415694025\n",
      "Epoch 11/1000, Loss: 0.6320072941167806\n",
      "Epoch 12/1000, Loss: 0.6141609101140239\n",
      "Epoch 13/1000, Loss: 0.5988577046305774\n",
      "Epoch 14/1000, Loss: 0.5852045743420091\n",
      "Epoch 15/1000, Loss: 0.5724473295964503\n",
      "Epoch 16/1000, Loss: 0.5606504259530986\n",
      "Epoch 17/1000, Loss: 0.549884928807439\n",
      "Epoch 18/1000, Loss: 0.5400062985220216\n",
      "Epoch 19/1000, Loss: 0.5309369583653369\n",
      "Epoch 20/1000, Loss: 0.5225350720714799\n",
      "Epoch 21/1000, Loss: 0.5148965891160755\n",
      "Epoch 22/1000, Loss: 0.5078613740906212\n",
      "Epoch 23/1000, Loss: 0.5013120108634657\n",
      "Epoch 24/1000, Loss: 0.49509444570172356\n",
      "Epoch 25/1000, Loss: 0.4893373354826579\n",
      "Epoch 26/1000, Loss: 0.48394375695922204\n",
      "Epoch 27/1000, Loss: 0.4790137834091135\n",
      "Epoch 28/1000, Loss: 0.47429093675302864\n",
      "Epoch 29/1000, Loss: 0.4697657329307599\n",
      "Epoch 30/1000, Loss: 0.46547784876348425\n",
      "Epoch 31/1000, Loss: 0.4613970007792255\n",
      "Epoch 32/1000, Loss: 0.45745325781063295\n",
      "Epoch 33/1000, Loss: 0.45368441559219524\n",
      "Epoch 34/1000, Loss: 0.4500333553095643\n",
      "Epoch 35/1000, Loss: 0.4464950795258196\n",
      "Epoch 36/1000, Loss: 0.44306550232767716\n",
      "Epoch 37/1000, Loss: 0.43984385614348365\n",
      "Epoch 38/1000, Loss: 0.436799650793471\n",
      "Epoch 39/1000, Loss: 0.43394999990988087\n",
      "Epoch 40/1000, Loss: 0.43131235285995234\n",
      "Epoch 41/1000, Loss: 0.4287452154727965\n",
      "Epoch 42/1000, Loss: 0.4262459907573918\n",
      "Epoch 43/1000, Loss: 0.423912646588764\n",
      "Epoch 44/1000, Loss: 0.4217312528719986\n",
      "Epoch 45/1000, Loss: 0.41969022641636033\n",
      "Epoch 46/1000, Loss: 0.41771229862738646\n",
      "Epoch 47/1000, Loss: 0.4157901897620732\n",
      "Epoch 48/1000, Loss: 0.4139477491055097\n",
      "Epoch 49/1000, Loss: 0.4121749482459182\n",
      "Epoch 50/1000, Loss: 0.4104595690599079\n",
      "Epoch 51/1000, Loss: 0.4088614061636413\n",
      "Epoch 52/1000, Loss: 0.40744727454549057\n",
      "Epoch 53/1000, Loss: 0.40606291745539447\n",
      "Epoch 54/1000, Loss: 0.4047074168113869\n",
      "Epoch 55/1000, Loss: 0.4033774306834354\n",
      "Epoch 56/1000, Loss: 0.40206917530556197\n",
      "Epoch 57/1000, Loss: 0.40078681349379836\n",
      "Epoch 58/1000, Loss: 0.3995297999097301\n",
      "Epoch 59/1000, Loss: 0.39829935590121696\n",
      "Epoch 60/1000, Loss: 0.3971451264548359\n",
      "Epoch 61/1000, Loss: 0.39601265935273383\n",
      "Epoch 62/1000, Loss: 0.3949189345210111\n",
      "Epoch 63/1000, Loss: 0.3938914100315744\n",
      "Epoch 64/1000, Loss: 0.39288402788426924\n",
      "Epoch 65/1000, Loss: 0.3919004895550059\n",
      "Epoch 66/1000, Loss: 0.39093446742165344\n",
      "Epoch 67/1000, Loss: 0.3900088020012002\n",
      "Epoch 68/1000, Loss: 0.3891096077659\n",
      "Epoch 69/1000, Loss: 0.3882544736681385\n",
      "Epoch 70/1000, Loss: 0.3874168288590766\n",
      "Epoch 71/1000, Loss: 0.38659562432617606\n",
      "Epoch 72/1000, Loss: 0.38580425670725865\n",
      "Epoch 73/1000, Loss: 0.3850572565580164\n",
      "Epoch 74/1000, Loss: 0.38433698858704884\n",
      "Epoch 75/1000, Loss: 0.3836321521687071\n",
      "Epoch 76/1000, Loss: 0.3829377539934122\n",
      "Epoch 77/1000, Loss: 0.3822535080676489\n",
      "Epoch 78/1000, Loss: 0.38158478130738793\n",
      "Epoch 79/1000, Loss: 0.3809384859465745\n",
      "Epoch 80/1000, Loss: 0.380298757683334\n",
      "Epoch 81/1000, Loss: 0.37966614311775465\n",
      "Epoch 82/1000, Loss: 0.37904095442945607\n",
      "Epoch 83/1000, Loss: 0.37842334886092166\n",
      "Epoch 84/1000, Loss: 0.3778391917582756\n",
      "Epoch 85/1000, Loss: 0.3772840687392411\n",
      "Epoch 86/1000, Loss: 0.37673525364965493\n",
      "Epoch 87/1000, Loss: 0.3761988395704328\n",
      "Epoch 88/1000, Loss: 0.3756706529839795\n",
      "Epoch 89/1000, Loss: 0.3751483042932361\n",
      "Epoch 90/1000, Loss: 0.37463170699024845\n",
      "Epoch 91/1000, Loss: 0.37412077072712036\n",
      "Epoch 92/1000, Loss: 0.373622293612582\n",
      "Epoch 93/1000, Loss: 0.3731335883456524\n",
      "Epoch 94/1000, Loss: 0.3726499118560747\n",
      "Epoch 95/1000, Loss: 0.3721711815529138\n",
      "Epoch 96/1000, Loss: 0.37169849651138975\n",
      "Epoch 97/1000, Loss: 0.3712377277214112\n",
      "Epoch 98/1000, Loss: 0.3708051258325937\n",
      "Epoch 99/1000, Loss: 0.37038205177549155\n",
      "Epoch 100/1000, Loss: 0.36996259845864493\n",
      "Epoch 101/1000, Loss: 0.369546670052595\n",
      "Epoch 102/1000, Loss: 0.36913418906656886\n",
      "Epoch 103/1000, Loss: 0.3687250897385142\n",
      "Epoch 104/1000, Loss: 0.3683249255954822\n",
      "Epoch 105/1000, Loss: 0.36793118893043875\n",
      "Epoch 106/1000, Loss: 0.36754050715172615\n",
      "Epoch 107/1000, Loss: 0.36715283754306227\n",
      "Epoch 108/1000, Loss: 0.3667681392365666\n",
      "Epoch 109/1000, Loss: 0.3663863727717611\n",
      "Epoch 110/1000, Loss: 0.36600749980449526\n",
      "Epoch 111/1000, Loss: 0.36563148291393605\n",
      "Epoch 112/1000, Loss: 0.36525828547414046\n",
      "Epoch 113/1000, Loss: 0.3648878715682925\n",
      "Epoch 114/1000, Loss: 0.36452020593114404\n",
      "Epoch 115/1000, Loss: 0.36416976997974604\n",
      "Epoch 116/1000, Loss: 0.3637868610948497\n",
      "Epoch 117/1000, Loss: 0.3634140670343751\n",
      "Epoch 118/1000, Loss: 0.36304758452850444\n",
      "Epoch 119/1000, Loss: 0.36268550458543\n",
      "Epoch 120/1000, Loss: 0.36232673079531647\n",
      "Epoch 121/1000, Loss: 0.36197074983797145\n",
      "Epoch 122/1000, Loss: 0.36161430064238426\n",
      "Epoch 123/1000, Loss: 0.361263478485466\n",
      "Epoch 124/1000, Loss: 0.36092970748645115\n",
      "Epoch 125/1000, Loss: 0.3606064391604352\n",
      "Epoch 126/1000, Loss: 0.36028530456258984\n",
      "Epoch 127/1000, Loss: 0.3599662001413961\n",
      "Epoch 128/1000, Loss: 0.35964907386124045\n",
      "Epoch 129/1000, Loss: 0.35933376132651446\n",
      "Epoch 130/1000, Loss: 0.3590186063543995\n",
      "Epoch 131/1000, Loss: 0.358705506223122\n",
      "Epoch 132/1000, Loss: 0.3583944056767135\n",
      "Epoch 133/1000, Loss: 0.3580852645568298\n",
      "Epoch 134/1000, Loss: 0.35777805018494246\n",
      "Epoch 135/1000, Loss: 0.35747273380864736\n",
      "Epoch 136/1000, Loss: 0.35716928194268976\n",
      "Epoch 137/1000, Loss: 0.35686767340292413\n",
      "Epoch 138/1000, Loss: 0.3565678879048083\n",
      "Epoch 139/1000, Loss: 0.356269902266499\n",
      "Epoch 140/1000, Loss: 0.3559736941108605\n",
      "Epoch 141/1000, Loss: 0.35567924177321675\n",
      "Epoch 142/1000, Loss: 0.355385832207478\n",
      "Epoch 143/1000, Loss: 0.3550858704377345\n",
      "Epoch 144/1000, Loss: 0.3547881100377614\n",
      "Epoch 145/1000, Loss: 0.3544923068341088\n",
      "Epoch 146/1000, Loss: 0.3541983386774432\n",
      "Epoch 147/1000, Loss: 0.3539063468024369\n",
      "Epoch 148/1000, Loss: 0.35361925316216125\n",
      "Epoch 149/1000, Loss: 0.35333380325655456\n",
      "Epoch 150/1000, Loss: 0.35304846691197495\n",
      "Epoch 151/1000, Loss: 0.35276062593332064\n",
      "Epoch 152/1000, Loss: 0.3524745513977128\n",
      "Epoch 153/1000, Loss: 0.3521901488487884\n",
      "Epoch 154/1000, Loss: 0.35190736601294387\n",
      "Epoch 155/1000, Loss: 0.3516261698063602\n",
      "Epoch 156/1000, Loss: 0.3513465358821424\n",
      "Epoch 157/1000, Loss: 0.35106844392901465\n",
      "Epoch 158/1000, Loss: 0.35079187558933234\n",
      "Epoch 159/1000, Loss: 0.35051681355704356\n",
      "Epoch 160/1000, Loss: 0.3502432411982585\n",
      "Epoch 161/1000, Loss: 0.3499711423971107\n",
      "Epoch 162/1000, Loss: 0.3497039104926267\n",
      "Epoch 163/1000, Loss: 0.34943969103035394\n",
      "Epoch 164/1000, Loss: 0.349176837394027\n",
      "Epoch 165/1000, Loss: 0.3489140510930971\n",
      "Epoch 166/1000, Loss: 0.3486403760307534\n",
      "Epoch 167/1000, Loss: 0.3483772102669882\n",
      "Epoch 168/1000, Loss: 0.3481153097029274\n",
      "Epoch 169/1000, Loss: 0.3478589755682099\n",
      "Epoch 170/1000, Loss: 0.34761072990896946\n",
      "Epoch 171/1000, Loss: 0.34735297399456916\n",
      "Epoch 172/1000, Loss: 0.3470968486320278\n",
      "Epoch 173/1000, Loss: 0.3468420263117248\n",
      "Epoch 174/1000, Loss: 0.3465882766676556\n",
      "Epoch 175/1000, Loss: 0.3463359422944637\n",
      "Epoch 176/1000, Loss: 0.34608477252613157\n",
      "Epoch 177/1000, Loss: 0.34583475570569416\n",
      "Epoch 178/1000, Loss: 0.34558588374873755\n",
      "Epoch 179/1000, Loss: 0.34533814975862326\n",
      "Epoch 180/1000, Loss: 0.3450915470922713\n",
      "Epoch 181/1000, Loss: 0.3448480004331291\n",
      "Epoch 182/1000, Loss: 0.34461036112879423\n",
      "Epoch 183/1000, Loss: 0.3443839786489755\n",
      "Epoch 184/1000, Loss: 0.34416258542402156\n",
      "Epoch 185/1000, Loss: 0.3439423650076214\n",
      "Epoch 186/1000, Loss: 0.3437232727605168\n",
      "Epoch 187/1000, Loss: 0.343505254057238\n",
      "Epoch 188/1000, Loss: 0.34328825996150725\n",
      "Epoch 189/1000, Loss: 0.3430722514010083\n",
      "Epoch 190/1000, Loss: 0.34285892590843736\n",
      "Epoch 191/1000, Loss: 0.3426528727507277\n",
      "Epoch 192/1000, Loss: 0.34245219955935374\n",
      "Epoch 193/1000, Loss: 0.34225246122807107\n",
      "Epoch 194/1000, Loss: 0.3420690353594477\n",
      "Epoch 195/1000, Loss: 0.34188609828516225\n",
      "Epoch 196/1000, Loss: 0.341705777160785\n",
      "Epoch 197/1000, Loss: 0.3415283569034068\n",
      "Epoch 198/1000, Loss: 0.3413517562323882\n",
      "Epoch 199/1000, Loss: 0.34117596059145094\n",
      "Epoch 200/1000, Loss: 0.34100094210696463\n",
      "Epoch 201/1000, Loss: 0.3408266733401322\n",
      "Epoch 202/1000, Loss: 0.34065491367443246\n",
      "Epoch 203/1000, Loss: 0.3404881877586102\n",
      "Epoch 204/1000, Loss: 0.3403232976095598\n",
      "Epoch 205/1000, Loss: 0.34015800307258404\n",
      "Epoch 206/1000, Loss: 0.3399930127567134\n",
      "Epoch 207/1000, Loss: 0.33982851831724925\n",
      "Epoch 208/1000, Loss: 0.3396646103377434\n",
      "Epoch 209/1000, Loss: 0.3395013316165757\n",
      "Epoch 210/1000, Loss: 0.3393388980657234\n",
      "Epoch 211/1000, Loss: 0.33917772799511126\n",
      "Epoch 212/1000, Loss: 0.33901719609428205\n",
      "Epoch 213/1000, Loss: 0.3388576893708058\n",
      "Epoch 214/1000, Loss: 0.3386983403464209\n",
      "Epoch 215/1000, Loss: 0.3385396331646764\n",
      "Epoch 216/1000, Loss: 0.3383815684784221\n",
      "Epoch 217/1000, Loss: 0.33822414691630925\n",
      "Epoch 218/1000, Loss: 0.3380673687080405\n",
      "Epoch 219/1000, Loss: 0.33791123360528835\n",
      "Epoch 220/1000, Loss: 0.3377557409144576\n",
      "Epoch 221/1000, Loss: 0.337600889562893\n",
      "Epoch 222/1000, Loss: 0.33744687398339196\n",
      "Epoch 223/1000, Loss: 0.3372935866051709\n",
      "Epoch 224/1000, Loss: 0.3371406435079788\n",
      "Epoch 225/1000, Loss: 0.3369883408556345\n",
      "Epoch 226/1000, Loss: 0.3368366730689932\n",
      "Epoch 227/1000, Loss: 0.3366879323550537\n",
      "Epoch 228/1000, Loss: 0.33654376977377876\n",
      "Epoch 229/1000, Loss: 0.33640019216808476\n",
      "Epoch 230/1000, Loss: 0.3362571973010578\n",
      "Epoch 231/1000, Loss: 0.33611494391244773\n",
      "Epoch 232/1000, Loss: 0.33597341278514914\n",
      "Epoch 233/1000, Loss: 0.33583212865035506\n",
      "Epoch 234/1000, Loss: 0.33568936854998893\n",
      "Epoch 235/1000, Loss: 0.3355473681897488\n",
      "Epoch 236/1000, Loss: 0.33540791627358213\n",
      "Epoch 237/1000, Loss: 0.33527283713307926\n",
      "Epoch 238/1000, Loss: 0.33514005620790555\n",
      "Epoch 239/1000, Loss: 0.33500790736214453\n",
      "Epoch 240/1000, Loss: 0.33487662918034183\n",
      "Epoch 241/1000, Loss: 0.33474605981702815\n",
      "Epoch 242/1000, Loss: 0.3346158571148991\n",
      "Epoch 243/1000, Loss: 0.33448624789283943\n",
      "Epoch 244/1000, Loss: 0.33435561409512865\n",
      "Epoch 245/1000, Loss: 0.33422503411983123\n",
      "Epoch 246/1000, Loss: 0.3340949424534775\n",
      "Epoch 247/1000, Loss: 0.3339691376320406\n",
      "Epoch 248/1000, Loss: 0.33384310846862963\n",
      "Epoch 249/1000, Loss: 0.33371734811113024\n",
      "Epoch 250/1000, Loss: 0.3335914657293905\n",
      "Epoch 251/1000, Loss: 0.33346601816751636\n",
      "Epoch 252/1000, Loss: 0.33334107359846255\n",
      "Epoch 253/1000, Loss: 0.33321666146831713\n",
      "Epoch 254/1000, Loss: 0.3330927930819827\n",
      "Epoch 255/1000, Loss: 0.33296947125844156\n",
      "Epoch 256/1000, Loss: 0.33284669486491025\n",
      "Epoch 257/1000, Loss: 0.3327246858924382\n",
      "Epoch 258/1000, Loss: 0.33260318089807245\n",
      "Epoch 259/1000, Loss: 0.3324822314006966\n",
      "Epoch 260/1000, Loss: 0.33236188718196275\n",
      "Epoch 261/1000, Loss: 0.3322420796056698\n",
      "Epoch 262/1000, Loss: 0.33212279958584556\n",
      "Epoch 263/1000, Loss: 0.33200404002460376\n",
      "Epoch 264/1000, Loss: 0.3318857950017364\n",
      "Epoch 265/1000, Loss: 0.33176826998027903\n",
      "Epoch 266/1000, Loss: 0.33165123025939003\n",
      "Epoch 267/1000, Loss: 0.3315344947096715\n",
      "Epoch 268/1000, Loss: 0.33141825974995914\n",
      "Epoch 269/1000, Loss: 0.3313025189702366\n",
      "Epoch 270/1000, Loss: 0.33118726748449856\n",
      "Epoch 271/1000, Loss: 0.3310725011596388\n",
      "Epoch 272/1000, Loss: 0.3309567487180206\n",
      "Epoch 273/1000, Loss: 0.33083880833669355\n",
      "Epoch 274/1000, Loss: 0.3307218635341245\n",
      "Epoch 275/1000, Loss: 0.33060873084412656\n",
      "Epoch 276/1000, Loss: 0.3304961086333518\n",
      "Epoch 277/1000, Loss: 0.33038331884184335\n",
      "Epoch 278/1000, Loss: 0.3302651135413509\n",
      "Epoch 279/1000, Loss: 0.33015417075062753\n",
      "Epoch 280/1000, Loss: 0.330043988676345\n",
      "Epoch 281/1000, Loss: 0.3299344909520598\n",
      "Epoch 282/1000, Loss: 0.32982518326342447\n",
      "Epoch 283/1000, Loss: 0.32971630873670565\n",
      "Epoch 284/1000, Loss: 0.3296079472619218\n",
      "Epoch 285/1000, Loss: 0.32950008872167563\n",
      "Epoch 286/1000, Loss: 0.3293927241413492\n",
      "Epoch 287/1000, Loss: 0.32928624642655546\n",
      "Epoch 288/1000, Loss: 0.3291784373121287\n",
      "Epoch 289/1000, Loss: 0.329072113880007\n",
      "Epoch 290/1000, Loss: 0.3289669357488524\n",
      "Epoch 291/1000, Loss: 0.32886279231830573\n",
      "Epoch 292/1000, Loss: 0.32875951987752333\n",
      "Epoch 293/1000, Loss: 0.32865700365429773\n",
      "Epoch 294/1000, Loss: 0.3285551615870358\n",
      "Epoch 295/1000, Loss: 0.3284580235561098\n",
      "Epoch 296/1000, Loss: 0.3283624799778227\n",
      "Epoch 297/1000, Loss: 0.3282646331238842\n",
      "Epoch 298/1000, Loss: 0.3281668997119514\n",
      "Epoch 299/1000, Loss: 0.3280731757557629\n",
      "Epoch 300/1000, Loss: 0.32798305210374046\n",
      "Epoch 301/1000, Loss: 0.3278934278334833\n",
      "Epoch 302/1000, Loss: 0.3278044691996294\n",
      "Epoch 303/1000, Loss: 0.32771587784639455\n",
      "Epoch 304/1000, Loss: 0.32762756330564397\n",
      "Epoch 305/1000, Loss: 0.3275396614582804\n",
      "Epoch 306/1000, Loss: 0.32745216172203534\n",
      "Epoch 307/1000, Loss: 0.3273650561918778\n",
      "Epoch 308/1000, Loss: 0.3272783440327425\n",
      "Epoch 309/1000, Loss: 0.32719234617049986\n",
      "Epoch 310/1000, Loss: 0.32710638234951467\n",
      "Epoch 311/1000, Loss: 0.3270210728666634\n",
      "Epoch 312/1000, Loss: 0.32693653584992827\n",
      "Epoch 313/1000, Loss: 0.32685233634371386\n",
      "Epoch 314/1000, Loss: 0.3267684754250279\n",
      "Epoch 315/1000, Loss: 0.32668504661675735\n",
      "Epoch 316/1000, Loss: 0.3266020989363024\n",
      "Epoch 317/1000, Loss: 0.32651924511802066\n",
      "Epoch 318/1000, Loss: 0.32643672957817815\n",
      "Epoch 319/1000, Loss: 0.32635454826267685\n",
      "Epoch 320/1000, Loss: 0.32627269813920895\n",
      "Epoch 321/1000, Loss: 0.32619117662673397\n",
      "Epoch 322/1000, Loss: 0.3261122224476439\n",
      "Epoch 323/1000, Loss: 0.3260355226768494\n",
      "Epoch 324/1000, Loss: 0.32596201254459706\n",
      "Epoch 325/1000, Loss: 0.32588885980363924\n",
      "Epoch 326/1000, Loss: 0.3258197690177411\n",
      "Epoch 327/1000, Loss: 0.3257507717877086\n",
      "Epoch 328/1000, Loss: 0.32568207451992803\n",
      "Epoch 329/1000, Loss: 0.3256135900493885\n",
      "Epoch 330/1000, Loss: 0.32554513150365316\n",
      "Epoch 331/1000, Loss: 0.325476888315827\n",
      "Epoch 332/1000, Loss: 0.3254088638344764\n",
      "Epoch 333/1000, Loss: 0.3253410603593711\n",
      "Epoch 334/1000, Loss: 0.325273580062657\n",
      "Epoch 335/1000, Loss: 0.3252064320938679\n",
      "Epoch 336/1000, Loss: 0.3251392944954436\n",
      "Epoch 337/1000, Loss: 0.3250723853557398\n",
      "Epoch 338/1000, Loss: 0.3250057030223534\n",
      "Epoch 339/1000, Loss: 0.32493924667550256\n",
      "Epoch 340/1000, Loss: 0.32487310456392926\n",
      "Epoch 341/1000, Loss: 0.3248060044563062\n",
      "Epoch 342/1000, Loss: 0.32473789039638584\n",
      "Epoch 343/1000, Loss: 0.3246705285917019\n",
      "Epoch 344/1000, Loss: 0.3246036319904816\n",
      "Epoch 345/1000, Loss: 0.3245370663263182\n",
      "Epoch 346/1000, Loss: 0.3244708771101117\n",
      "Epoch 347/1000, Loss: 0.3244050066191036\n",
      "Epoch 348/1000, Loss: 0.3243391691832578\n",
      "Epoch 349/1000, Loss: 0.3242735544778353\n",
      "Epoch 350/1000, Loss: 0.32420815778586665\n",
      "Epoch 351/1000, Loss: 0.3241429768741061\n",
      "Epoch 352/1000, Loss: 0.32407816843887527\n",
      "Epoch 353/1000, Loss: 0.3240135476484218\n",
      "Epoch 354/1000, Loss: 0.3239490044709501\n",
      "Epoch 355/1000, Loss: 0.323885305643808\n",
      "Epoch 356/1000, Loss: 0.32381701031451715\n",
      "Epoch 357/1000, Loss: 0.3237492715013486\n",
      "Epoch 358/1000, Loss: 0.3236821949865901\n",
      "Epoch 359/1000, Loss: 0.3236152860934539\n",
      "Epoch 360/1000, Loss: 0.32354865107192365\n",
      "Epoch 361/1000, Loss: 0.3234823231670188\n",
      "Epoch 362/1000, Loss: 0.3234162882824891\n",
      "Epoch 363/1000, Loss: 0.3233506343410486\n",
      "Epoch 364/1000, Loss: 0.32328534041167256\n",
      "Epoch 365/1000, Loss: 0.32322013328408267\n",
      "Epoch 366/1000, Loss: 0.3231541396826362\n",
      "Epoch 367/1000, Loss: 0.3230879076216453\n",
      "Epoch 368/1000, Loss: 0.32302193752702973\n",
      "Epoch 369/1000, Loss: 0.3229564554515666\n",
      "Epoch 370/1000, Loss: 0.3228910414809266\n",
      "Epoch 371/1000, Loss: 0.322825835524333\n",
      "Epoch 372/1000, Loss: 0.3227608839329516\n",
      "Epoch 373/1000, Loss: 0.32269618275485523\n",
      "Epoch 374/1000, Loss: 0.32263117852479206\n",
      "Epoch 375/1000, Loss: 0.32256255771383574\n",
      "Epoch 376/1000, Loss: 0.32249407184089857\n",
      "Epoch 377/1000, Loss: 0.32242584891627135\n",
      "Epoch 378/1000, Loss: 0.32235788677654137\n",
      "Epoch 379/1000, Loss: 0.3222903855818669\n",
      "Epoch 380/1000, Loss: 0.32222327193159866\n",
      "Epoch 381/1000, Loss: 0.32215621106782955\n",
      "Epoch 382/1000, Loss: 0.3220895491637775\n",
      "Epoch 383/1000, Loss: 0.32202486765697286\n",
      "Epoch 384/1000, Loss: 0.3219628539499669\n",
      "Epoch 385/1000, Loss: 0.32189724045379353\n",
      "Epoch 386/1000, Loss: 0.3218313370834842\n",
      "Epoch 387/1000, Loss: 0.3217653352713878\n",
      "Epoch 388/1000, Loss: 0.3216993459034019\n",
      "Epoch 389/1000, Loss: 0.3216334364307218\n",
      "Epoch 390/1000, Loss: 0.32156764979202235\n",
      "Epoch 391/1000, Loss: 0.32150201439151904\n",
      "Epoch 392/1000, Loss: 0.32143654958353085\n",
      "Epoch 393/1000, Loss: 0.32137126883281975\n",
      "Epoch 394/1000, Loss: 0.3213061816271442\n",
      "Epoch 395/1000, Loss: 0.3212412946896195\n",
      "Epoch 396/1000, Loss: 0.32117661277875387\n",
      "Epoch 397/1000, Loss: 0.3211121392337671\n",
      "Epoch 398/1000, Loss: 0.321050771831189\n",
      "Epoch 399/1000, Loss: 0.32099011348791445\n",
      "Epoch 400/1000, Loss: 0.3209271100866464\n",
      "Epoch 401/1000, Loss: 0.3208643532627176\n",
      "Epoch 402/1000, Loss: 0.32080183593263273\n",
      "Epoch 403/1000, Loss: 0.3207395534849883\n",
      "Epoch 404/1000, Loss: 0.3206775025820077\n",
      "Epoch 405/1000, Loss: 0.3206156805690039\n",
      "Epoch 406/1000, Loss: 0.32055408517689454\n",
      "Epoch 407/1000, Loss: 0.32049271436781873\n",
      "Epoch 408/1000, Loss: 0.3204315662519394\n",
      "Epoch 409/1000, Loss: 0.32037122832649867\n",
      "Epoch 410/1000, Loss: 0.3203122323301119\n",
      "Epoch 411/1000, Loss: 0.3202534571262597\n",
      "Epoch 412/1000, Loss: 0.3201948997358616\n",
      "Epoch 413/1000, Loss: 0.3201365578535166\n",
      "Epoch 414/1000, Loss: 0.32007842952917515\n",
      "Epoch 415/1000, Loss: 0.320020513010631\n",
      "Epoch 416/1000, Loss: 0.31996280666372073\n",
      "Epoch 417/1000, Loss: 0.31990530893059166\n",
      "Epoch 418/1000, Loss: 0.3198480183069915\n",
      "Epoch 419/1000, Loss: 0.31979093332931896\n",
      "Epoch 420/1000, Loss: 0.31973405256685944\n",
      "Epoch 421/1000, Loss: 0.3196773746168918\n",
      "Epoch 422/1000, Loss: 0.3196208981014567\n",
      "Epoch 423/1000, Loss: 0.319564621665136\n",
      "Epoch 424/1000, Loss: 0.3195085439734684\n",
      "Epoch 425/1000, Loss: 0.31945266371178355\n",
      "Epoch 426/1000, Loss: 0.31939697958431773\n",
      "Epoch 427/1000, Loss: 0.319341036265625\n",
      "Epoch 428/1000, Loss: 0.319279596376794\n",
      "Epoch 429/1000, Loss: 0.3192184454996926\n",
      "Epoch 430/1000, Loss: 0.3191603271559983\n",
      "Epoch 431/1000, Loss: 0.3191021159196379\n",
      "Epoch 432/1000, Loss: 0.31904044331748016\n",
      "Epoch 433/1000, Loss: 0.3189833913727149\n",
      "Epoch 434/1000, Loss: 0.3189265607462571\n",
      "Epoch 435/1000, Loss: 0.31886999190938775\n",
      "Epoch 436/1000, Loss: 0.3188136076351824\n",
      "Epoch 437/1000, Loss: 0.3187574202054547\n",
      "Epoch 438/1000, Loss: 0.3187014350350283\n",
      "Epoch 439/1000, Loss: 0.3186456541438574\n",
      "Epoch 440/1000, Loss: 0.31859007783984955\n",
      "Epoch 441/1000, Loss: 0.31853470554678825\n",
      "Epoch 442/1000, Loss: 0.31847953622030845\n",
      "Epoch 443/1000, Loss: 0.31842456856285917\n",
      "Epoch 444/1000, Loss: 0.3183711046777185\n",
      "Epoch 445/1000, Loss: 0.3183197867658324\n",
      "Epoch 446/1000, Loss: 0.3182686364209311\n",
      "Epoch 447/1000, Loss: 0.3182176587634973\n",
      "Epoch 448/1000, Loss: 0.3181672541534868\n",
      "Epoch 449/1000, Loss: 0.31811796075944854\n",
      "Epoch 450/1000, Loss: 0.31806951672606126\n",
      "Epoch 451/1000, Loss: 0.3180234407114429\n",
      "Epoch 452/1000, Loss: 0.3179774924222953\n",
      "Epoch 453/1000, Loss: 0.3179316782879076\n",
      "Epoch 454/1000, Loss: 0.3178860022078467\n",
      "Epoch 455/1000, Loss: 0.3178404665721552\n",
      "Epoch 456/1000, Loss: 0.3177950728271673\n",
      "Epoch 457/1000, Loss: 0.31774982180462996\n",
      "Epoch 458/1000, Loss: 0.3177047139226514\n",
      "Epoch 459/1000, Loss: 0.31765974931402324\n",
      "Epoch 460/1000, Loss: 0.3176165789588614\n",
      "Epoch 461/1000, Loss: 0.31757620879814086\n",
      "Epoch 462/1000, Loss: 0.31753590877687515\n",
      "Epoch 463/1000, Loss: 0.31749569724808724\n",
      "Epoch 464/1000, Loss: 0.3174555852905428\n",
      "Epoch 465/1000, Loss: 0.31741564146093176\n",
      "Epoch 466/1000, Loss: 0.3173784694552565\n",
      "Epoch 467/1000, Loss: 0.3173412138417084\n",
      "Epoch 468/1000, Loss: 0.31730396581980164\n",
      "Epoch 469/1000, Loss: 0.3172667656277603\n",
      "Epoch 470/1000, Loss: 0.31722963043957\n",
      "Epoch 471/1000, Loss: 0.31719256723554295\n",
      "Epoch 472/1000, Loss: 0.3171555786731002\n",
      "Epoch 473/1000, Loss: 0.3171186657145843\n",
      "Epoch 474/1000, Loss: 0.31708182876575614\n",
      "Epoch 475/1000, Loss: 0.3170450681401042\n",
      "Epoch 476/1000, Loss: 0.3170083842252962\n",
      "Epoch 477/1000, Loss: 0.316971777523622\n",
      "Epoch 478/1000, Loss: 0.3169352486434795\n",
      "Epoch 479/1000, Loss: 0.3168987982753864\n",
      "Epoch 480/1000, Loss: 0.3168624271662398\n",
      "Epoch 481/1000, Loss: 0.31682613609679927\n",
      "Epoch 482/1000, Loss: 0.31678992586364535\n",
      "Epoch 483/1000, Loss: 0.3167537972654076\n",
      "Epoch 484/1000, Loss: 0.31671775109258216\n",
      "Epoch 485/1000, Loss: 0.3166817881201918\n",
      "Epoch 486/1000, Loss: 0.3166459091026234\n",
      "Epoch 487/1000, Loss: 0.3166101147701017\n",
      "Epoch 488/1000, Loss: 0.3165744058263755\n",
      "Epoch 489/1000, Loss: 0.3165387829472951\n",
      "Epoch 490/1000, Loss: 0.31650324678003877\n",
      "Epoch 491/1000, Loss: 0.31646779794280977\n",
      "Epoch 492/1000, Loss: 0.3164324370248702\n",
      "Epoch 493/1000, Loss: 0.316397164586815\n",
      "Epoch 494/1000, Loss: 0.31636198116101377\n",
      "Epoch 495/1000, Loss: 0.31632688725216895\n",
      "Epoch 496/1000, Loss: 0.3162918833379512\n",
      "Epoch 497/1000, Loss: 0.31625696986968527\n",
      "Epoch 498/1000, Loss: 0.3162221472730651\n",
      "Epoch 499/1000, Loss: 0.31618741594888433\n",
      "Epoch 500/1000, Loss: 0.3161527762737711\n",
      "Epoch 501/1000, Loss: 0.31611822860092115\n",
      "Epoch 502/1000, Loss: 0.3160841551230931\n",
      "Epoch 503/1000, Loss: 0.3160506864047163\n",
      "Epoch 504/1000, Loss: 0.3160174042972643\n",
      "Epoch 505/1000, Loss: 0.3159842749339257\n",
      "Epoch 506/1000, Loss: 0.31595323621472665\n",
      "Epoch 507/1000, Loss: 0.3159234615560444\n",
      "Epoch 508/1000, Loss: 0.3158937724568781\n",
      "Epoch 509/1000, Loss: 0.31586429719500747\n",
      "Epoch 510/1000, Loss: 0.3158358686768944\n",
      "Epoch 511/1000, Loss: 0.31580750969199767\n",
      "Epoch 512/1000, Loss: 0.31577922280932225\n",
      "Epoch 513/1000, Loss: 0.3157510094459736\n",
      "Epoch 514/1000, Loss: 0.3157228704532295\n",
      "Epoch 515/1000, Loss: 0.31569480639609676\n",
      "Epoch 516/1000, Loss: 0.3156668176877397\n",
      "Epoch 517/1000, Loss: 0.31563890465495004\n",
      "Epoch 518/1000, Loss: 0.3156124548787669\n",
      "Epoch 519/1000, Loss: 0.3155880244509001\n",
      "Epoch 520/1000, Loss: 0.31556371601024663\n",
      "Epoch 521/1000, Loss: 0.31553951108285927\n",
      "Epoch 522/1000, Loss: 0.3155153993191922\n",
      "Epoch 523/1000, Loss: 0.31549137462309707\n",
      "Epoch 524/1000, Loss: 0.3154674332154236\n",
      "Epoch 525/1000, Loss: 0.3154433640314216\n",
      "Epoch 526/1000, Loss: 0.3154148945211974\n",
      "Epoch 527/1000, Loss: 0.3153884514032658\n",
      "Epoch 528/1000, Loss: 0.3153630062273596\n",
      "Epoch 529/1000, Loss: 0.31533807420261656\n",
      "Epoch 530/1000, Loss: 0.3153134265031912\n",
      "Epoch 531/1000, Loss: 0.3152889548390217\n",
      "Epoch 532/1000, Loss: 0.31526460770179543\n",
      "Epoch 533/1000, Loss: 0.3152403603505735\n",
      "Epoch 534/1000, Loss: 0.3152162006799448\n",
      "Epoch 535/1000, Loss: 0.3151921225648725\n",
      "Epoch 536/1000, Loss: 0.31516812272531913\n",
      "Epoch 537/1000, Loss: 0.31514241607186366\n",
      "Epoch 538/1000, Loss: 0.3151017839203936\n",
      "Epoch 539/1000, Loss: 0.3150638430125197\n",
      "Epoch 540/1000, Loss: 0.3150273241454811\n",
      "Epoch 541/1000, Loss: 0.31499162622876814\n",
      "Epoch 542/1000, Loss: 0.3149564628524619\n",
      "Epoch 543/1000, Loss: 0.3149216958551617\n",
      "Epoch 544/1000, Loss: 0.3148872569566852\n",
      "Epoch 545/1000, Loss: 0.31485311086146534\n",
      "Epoch 546/1000, Loss: 0.31481949084078065\n",
      "Epoch 547/1000, Loss: 0.31478716117083844\n",
      "Epoch 548/1000, Loss: 0.314755049163737\n",
      "Epoch 549/1000, Loss: 0.31472640225987647\n",
      "Epoch 550/1000, Loss: 0.3147006991567231\n",
      "Epoch 551/1000, Loss: 0.3146751622951656\n",
      "Epoch 552/1000, Loss: 0.3146493261020158\n",
      "Epoch 553/1000, Loss: 0.3146215772857901\n",
      "Epoch 554/1000, Loss: 0.31459407943135076\n",
      "Epoch 555/1000, Loss: 0.31456678572031754\n",
      "Epoch 556/1000, Loss: 0.31453967715209963\n",
      "Epoch 557/1000, Loss: 0.31451274651942046\n",
      "Epoch 558/1000, Loss: 0.314485991221189\n",
      "Epoch 559/1000, Loss: 0.31445941014234896\n",
      "Epoch 560/1000, Loss: 0.3144330023788509\n",
      "Epoch 561/1000, Loss: 0.31440670931526016\n",
      "Epoch 562/1000, Loss: 0.31437902936078577\n",
      "Epoch 563/1000, Loss: 0.3143516623868584\n",
      "Epoch 564/1000, Loss: 0.31432456660733205\n",
      "Epoch 565/1000, Loss: 0.3142977132981442\n",
      "Epoch 566/1000, Loss: 0.3142710818523337\n",
      "Epoch 567/1000, Loss: 0.3142442174923466\n",
      "Epoch 568/1000, Loss: 0.314215350416343\n",
      "Epoch 569/1000, Loss: 0.3141865710228953\n",
      "Epoch 570/1000, Loss: 0.3141568248210402\n",
      "Epoch 571/1000, Loss: 0.31412727466201074\n",
      "Epoch 572/1000, Loss: 0.3140979020517534\n",
      "Epoch 573/1000, Loss: 0.3140687665950991\n",
      "Epoch 574/1000, Loss: 0.31404419863019095\n",
      "Epoch 575/1000, Loss: 0.31401968982543776\n",
      "Epoch 576/1000, Loss: 0.31399530062351994\n",
      "Epoch 577/1000, Loss: 0.313971051200314\n",
      "Epoch 578/1000, Loss: 0.31394694474093715\n",
      "Epoch 579/1000, Loss: 0.31392297788931567\n",
      "Epoch 580/1000, Loss: 0.3138991452900758\n",
      "Epoch 581/1000, Loss: 0.3138754414473197\n",
      "Epoch 582/1000, Loss: 0.31385186139330135\n",
      "Epoch 583/1000, Loss: 0.31382840085097635\n",
      "Epoch 584/1000, Loss: 0.31380505619829363\n",
      "Epoch 585/1000, Loss: 0.3137818243686702\n",
      "Epoch 586/1000, Loss: 0.3137587027431656\n",
      "Epoch 587/1000, Loss: 0.31373568905475924\n",
      "Epoch 588/1000, Loss: 0.3137127813101071\n",
      "Epoch 589/1000, Loss: 0.3136899777281912\n",
      "Epoch 590/1000, Loss: 0.31366727669330247\n",
      "Epoch 591/1000, Loss: 0.3136450672555164\n",
      "Epoch 592/1000, Loss: 0.31362449524095054\n",
      "Epoch 593/1000, Loss: 0.31360399235662384\n",
      "Epoch 594/1000, Loss: 0.3135835737710145\n",
      "Epoch 595/1000, Loss: 0.31356324213911685\n",
      "Epoch 596/1000, Loss: 0.3135429953667884\n",
      "Epoch 597/1000, Loss: 0.31352282996756803\n",
      "Epoch 598/1000, Loss: 0.3135027424217917\n",
      "Epoch 599/1000, Loss: 0.31348272965294643\n",
      "Epoch 600/1000, Loss: 0.3134627891313039\n",
      "Epoch 601/1000, Loss: 0.31344291883380765\n",
      "Epoch 602/1000, Loss: 0.3134231171597151\n",
      "Epoch 603/1000, Loss: 0.31340338284267955\n",
      "Epoch 604/1000, Loss: 0.31338371487388056\n",
      "Epoch 605/1000, Loss: 0.31336411243972334\n",
      "Epoch 606/1000, Loss: 0.3133445748732907\n",
      "Epoch 607/1000, Loss: 0.3133251016173418\n",
      "Epoch 608/1000, Loss: 0.31330569219647725\n",
      "Epoch 609/1000, Loss: 0.31328634619636875\n",
      "Epoch 610/1000, Loss: 0.31326706324832837\n",
      "Epoch 611/1000, Loss: 0.31324784301788255\n",
      "Epoch 612/1000, Loss: 0.31322868519632013\n",
      "Epoch 613/1000, Loss: 0.3132095894944479\n",
      "Epoch 614/1000, Loss: 0.31319055563797354\n",
      "Epoch 615/1000, Loss: 0.31317158336409356\n",
      "Epoch 616/1000, Loss: 0.3131526724189656\n",
      "Epoch 617/1000, Loss: 0.31313382255583533\n",
      "Epoch 618/1000, Loss: 0.3131150335336437\n",
      "Epoch 619/1000, Loss: 0.3130963051159892\n",
      "Epoch 620/1000, Loss: 0.31307766264464004\n",
      "Epoch 621/1000, Loss: 0.31305886367302593\n",
      "Epoch 622/1000, Loss: 0.3130402508175622\n",
      "Epoch 623/1000, Loss: 0.3130220340995294\n",
      "Epoch 624/1000, Loss: 0.3130035423324077\n",
      "Epoch 625/1000, Loss: 0.31298518078479565\n",
      "Epoch 626/1000, Loss: 0.3129666553814024\n",
      "Epoch 627/1000, Loss: 0.31294777035176147\n",
      "Epoch 628/1000, Loss: 0.3129290103245922\n",
      "Epoch 629/1000, Loss: 0.3129105988727516\n",
      "Epoch 630/1000, Loss: 0.31289188816716673\n",
      "Epoch 631/1000, Loss: 0.31287328782775986\n",
      "Epoch 632/1000, Loss: 0.31285478116750065\n",
      "Epoch 633/1000, Loss: 0.31283635951076977\n",
      "Epoch 634/1000, Loss: 0.3128182982525674\n",
      "Epoch 635/1000, Loss: 0.31279991194225737\n",
      "Epoch 636/1000, Loss: 0.3127816260084338\n",
      "Epoch 637/1000, Loss: 0.3127634261987884\n",
      "Epoch 638/1000, Loss: 0.31274530337703615\n",
      "Epoch 639/1000, Loss: 0.31272726562609693\n",
      "Epoch 640/1000, Loss: 0.31270955323250865\n",
      "Epoch 641/1000, Loss: 0.31269152435512615\n",
      "Epoch 642/1000, Loss: 0.31267359042993736\n",
      "Epoch 643/1000, Loss: 0.31265573836954214\n",
      "Epoch 644/1000, Loss: 0.312637969634063\n",
      "Epoch 645/1000, Loss: 0.31262053456512456\n",
      "Epoch 646/1000, Loss: 0.3126031292647156\n",
      "Epoch 647/1000, Loss: 0.3125867922598789\n",
      "Epoch 648/1000, Loss: 0.3125704331569236\n",
      "Epoch 649/1000, Loss: 0.31255408397389006\n",
      "Epoch 650/1000, Loss: 0.31253800640458085\n",
      "Epoch 651/1000, Loss: 0.3125215731832801\n",
      "Epoch 652/1000, Loss: 0.3125052005296068\n",
      "Epoch 653/1000, Loss: 0.31248888188337826\n",
      "Epoch 654/1000, Loss: 0.3124726365803664\n",
      "Epoch 655/1000, Loss: 0.31245667547812933\n",
      "Epoch 656/1000, Loss: 0.3124403985436223\n",
      "Epoch 657/1000, Loss: 0.3124242008437626\n",
      "Epoch 658/1000, Loss: 0.31240808333859527\n",
      "Epoch 659/1000, Loss: 0.31239228103117883\n",
      "Epoch 660/1000, Loss: 0.31237616314410416\n",
      "Epoch 661/1000, Loss: 0.31236013035751964\n",
      "Epoch 662/1000, Loss: 0.3123441752407653\n",
      "Epoch 663/1000, Loss: 0.31232855050720054\n",
      "Epoch 664/1000, Loss: 0.3123126076691666\n",
      "Epoch 665/1000, Loss: 0.3122967513550288\n",
      "Epoch 666/1000, Loss: 0.3122809700942408\n",
      "Epoch 667/1000, Loss: 0.31226552583661826\n",
      "Epoch 668/1000, Loss: 0.3122497622873661\n",
      "Epoch 669/1000, Loss: 0.31223408540953346\n",
      "Epoch 670/1000, Loss: 0.3122184829584451\n",
      "Epoch 671/1000, Loss: 0.3122032175766592\n",
      "Epoch 672/1000, Loss: 0.3121881713740695\n",
      "Epoch 673/1000, Loss: 0.3121819942452019\n",
      "Epoch 674/1000, Loss: 0.3121734680553682\n",
      "Epoch 675/1000, Loss: 0.3121641051031813\n",
      "Epoch 676/1000, Loss: 0.31215385223617503\n",
      "Epoch 677/1000, Loss: 0.3121433887767694\n",
      "Epoch 678/1000, Loss: 0.3121328396417468\n",
      "Epoch 679/1000, Loss: 0.3121225109947994\n",
      "Epoch 680/1000, Loss: 0.31211179948864\n",
      "Epoch 681/1000, Loss: 0.3121011143346798\n",
      "Epoch 682/1000, Loss: 0.31209045900956217\n",
      "Epoch 683/1000, Loss: 0.31208007141651856\n",
      "Epoch 684/1000, Loss: 0.3120693327982301\n",
      "Epoch 685/1000, Loss: 0.31205863494151725\n",
      "Epoch 686/1000, Loss: 0.31204797819226543\n",
      "Epoch 687/1000, Loss: 0.31203758872860227\n",
      "Epoch 688/1000, Loss: 0.31202685740500324\n",
      "Epoch 689/1000, Loss: 0.3120161709716338\n",
      "Epoch 690/1000, Loss: 0.312005531916344\n",
      "Epoch 691/1000, Loss: 0.3119951580173455\n",
      "Epoch 692/1000, Loss: 0.3119844499953509\n",
      "Epoch 693/1000, Loss: 0.31197379032872324\n",
      "Epoch 694/1000, Loss: 0.3119631835746172\n",
      "Epoch 695/1000, Loss: 0.31195283995010475\n",
      "Epoch 696/1000, Loss: 0.3119421692439212\n",
      "Epoch 697/1000, Loss: 0.3119315499418122\n",
      "Epoch 698/1000, Loss: 0.3119209884292878\n",
      "Epoch 699/1000, Loss: 0.3119106880709879\n",
      "Epoch 700/1000, Loss: 0.31190006697769485\n",
      "Epoch 701/1000, Loss: 0.31188949990565873\n",
      "Epoch 702/1000, Loss: 0.3118789948733372\n",
      "Epoch 703/1000, Loss: 0.3118687490130315\n",
      "Epoch 704/1000, Loss: 0.3118581881235344\n",
      "Epoch 705/1000, Loss: 0.3118476834684967\n",
      "Epoch 706/1000, Loss: 0.31183724455207684\n",
      "Epoch 707/1000, Loss: 0.311827062771386\n",
      "Epoch 708/1000, Loss: 0.3118165711096293\n",
      "Epoch 709/1000, Loss: 0.31180613753362874\n",
      "Epoch 710/1000, Loss: 0.31179577293393124\n",
      "Epoch 711/1000, Loss: 0.3117856633286183\n",
      "Epoch 712/1000, Loss: 0.3117752485233579\n",
      "Epoch 713/1000, Loss: 0.31176489333605817\n",
      "Epoch 714/1000, Loss: 0.31175460999231447\n",
      "Epoch 715/1000, Loss: 0.3117445793405704\n",
      "Epoch 716/1000, Loss: 0.3117342477924354\n",
      "Epoch 717/1000, Loss: 0.31172397711494193\n",
      "Epoch 718/1000, Loss: 0.31171455826896266\n",
      "Epoch 719/1000, Loss: 0.3117060003478704\n",
      "Epoch 720/1000, Loss: 0.3116968682602372\n",
      "Epoch 721/1000, Loss: 0.31168780654087114\n",
      "Epoch 722/1000, Loss: 0.3116788268194485\n",
      "Epoch 723/1000, Loss: 0.3116703382379838\n",
      "Epoch 724/1000, Loss: 0.31166128072333604\n",
      "Epoch 725/1000, Loss: 0.3116522957041847\n",
      "Epoch 726/1000, Loss: 0.31164386407644634\n",
      "Epoch 727/1000, Loss: 0.3116348474032529\n",
      "Epoch 728/1000, Loss: 0.31162590777046656\n",
      "Epoch 729/1000, Loss: 0.31161703091721504\n",
      "Epoch 730/1000, Loss: 0.3116086947177556\n",
      "Epoch 731/1000, Loss: 0.3116007615680152\n",
      "Epoch 732/1000, Loss: 0.31159421147108013\n",
      "Epoch 733/1000, Loss: 0.31158747727091146\n",
      "Epoch 734/1000, Loss: 0.31158146050387214\n",
      "Epoch 735/1000, Loss: 0.31157467392515187\n",
      "Epoch 736/1000, Loss: 0.3115679945284044\n",
      "Epoch 737/1000, Loss: 0.3115621779578575\n",
      "Epoch 738/1000, Loss: 0.3115571637990651\n",
      "Epoch 739/1000, Loss: 0.3115513894819911\n",
      "Epoch 740/1000, Loss: 0.31154569669494603\n",
      "Epoch 741/1000, Loss: 0.31154007196012623\n",
      "Epoch 742/1000, Loss: 0.3115351169187127\n",
      "Epoch 743/1000, Loss: 0.3115294025211146\n",
      "Epoch 744/1000, Loss: 0.31152376694263434\n",
      "Epoch 745/1000, Loss: 0.31151819884915327\n",
      "Epoch 746/1000, Loss: 0.31151329408542056\n",
      "Epoch 747/1000, Loss: 0.31150763346169874\n",
      "Epoch 748/1000, Loss: 0.3115020508756837\n",
      "Epoch 749/1000, Loss: 0.3114965367808677\n",
      "Epoch 750/1000, Loss: 0.31149168032814606\n",
      "Epoch 751/1000, Loss: 0.31148607235510345\n",
      "Epoch 752/1000, Loss: 0.31148054214589604\n",
      "Epoch 753/1000, Loss: 0.31147508202167945\n",
      "Epoch 754/1000, Loss: 0.31147027366561975\n",
      "Epoch 755/1000, Loss: 0.311464718470682\n",
      "Epoch 756/1000, Loss: 0.3114592408697989\n",
      "Epoch 757/1000, Loss: 0.3114538352751258\n",
      "Epoch 758/1000, Loss: 0.31144878699261813\n",
      "Epoch 759/1000, Loss: 0.3114406743831526\n",
      "Epoch 760/1000, Loss: 0.3114329583066732\n",
      "Epoch 761/1000, Loss: 0.31142554756966645\n",
      "Epoch 762/1000, Loss: 0.31141895147977067\n",
      "Epoch 763/1000, Loss: 0.3114117463435219\n",
      "Epoch 764/1000, Loss: 0.3114047179189163\n",
      "Epoch 765/1000, Loss: 0.3113982609238034\n",
      "Epoch 766/1000, Loss: 0.31139303433225163\n",
      "Epoch 767/1000, Loss: 0.31138707079043515\n",
      "Epoch 768/1000, Loss: 0.31138119012227655\n",
      "Epoch 769/1000, Loss: 0.31137537933231635\n",
      "Epoch 770/1000, Loss: 0.31137023309194356\n",
      "Epoch 771/1000, Loss: 0.31136435306840776\n",
      "Epoch 772/1000, Loss: 0.31135855874567736\n",
      "Epoch 773/1000, Loss: 0.311352830848765\n",
      "Epoch 774/1000, Loss: 0.31134718091225894\n",
      "Epoch 775/1000, Loss: 0.3113421451572229\n",
      "Epoch 776/1000, Loss: 0.31133639230807025\n",
      "Epoch 777/1000, Loss: 0.31133072011987145\n",
      "Epoch 778/1000, Loss: 0.31132512988348\n",
      "Epoch 779/1000, Loss: 0.3113201675707782\n",
      "Epoch 780/1000, Loss: 0.31131448986295607\n",
      "Epoch 781/1000, Loss: 0.3113088951083368\n",
      "Epoch 782/1000, Loss: 0.3113033790507018\n",
      "Epoch 783/1000, Loss: 0.31129849877279453\n",
      "Epoch 784/1000, Loss: 0.3112929016651005\n",
      "Epoch 785/1000, Loss: 0.31128738736125616\n",
      "Epoch 786/1000, Loss: 0.31128194714559004\n",
      "Epoch 787/1000, Loss: 0.31127714861155775\n",
      "Epoch 788/1000, Loss: 0.3112716312544309\n",
      "Epoch 789/1000, Loss: 0.31126619592016364\n",
      "Epoch 790/1000, Loss: 0.311260829997431\n",
      "Epoch 791/1000, Loss: 0.3112561106138233\n",
      "Epoch 792/1000, Loss: 0.3112506705274034\n",
      "Epoch 793/1000, Loss: 0.3112453115370736\n",
      "Epoch 794/1000, Loss: 0.3112400175827478\n",
      "Epoch 795/1000, Loss: 0.3112353742142632\n",
      "Epoch 796/1000, Loss: 0.3112300085823156\n",
      "Epoch 797/1000, Loss: 0.31122472311454835\n",
      "Epoch 798/1000, Loss: 0.31121949873104104\n",
      "Epoch 799/1000, Loss: 0.31121434736815\n",
      "Epoch 800/1000, Loss: 0.31120979105574526\n",
      "Epoch 801/1000, Loss: 0.31120453005962173\n",
      "Epoch 802/1000, Loss: 0.3111993437462389\n",
      "Epoch 803/1000, Loss: 0.3111942363605267\n",
      "Epoch 804/1000, Loss: 0.31118973326858235\n",
      "Epoch 805/1000, Loss: 0.3111845291245486\n",
      "Epoch 806/1000, Loss: 0.3111794020184503\n",
      "Epoch 807/1000, Loss: 0.31117435309511243\n",
      "Epoch 808/1000, Loss: 0.3111699119362996\n",
      "Epoch 809/1000, Loss: 0.311164770580398\n",
      "Epoch 810/1000, Loss: 0.3111597063505022\n",
      "Epoch 811/1000, Loss: 0.3111547184224084\n",
      "Epoch 812/1000, Loss: 0.3111503396795954\n",
      "Epoch 813/1000, Loss: 0.31114526119524766\n",
      "Epoch 814/1000, Loss: 0.31114025933824807\n",
      "Epoch 815/1000, Loss: 0.311135331938265\n",
      "Epoch 816/1000, Loss: 0.311131013970007\n",
      "Epoch 817/1000, Loss: 0.3111259969171849\n",
      "Epoch 818/1000, Loss: 0.3111210558520989\n",
      "Epoch 819/1000, Loss: 0.31111618775843847\n",
      "Epoch 820/1000, Loss: 0.31111192840990687\n",
      "Epoch 821/1000, Loss: 0.31110697100756624\n",
      "Epoch 822/1000, Loss: 0.3111022052651444\n",
      "Epoch 823/1000, Loss: 0.31109779013263256\n",
      "Epoch 824/1000, Loss: 0.3110939834338423\n",
      "Epoch 825/1000, Loss: 0.31108948070728404\n",
      "Epoch 826/1000, Loss: 0.31108505320708363\n",
      "Epoch 827/1000, Loss: 0.31108069654329956\n",
      "Epoch 828/1000, Loss: 0.3110769441662182\n",
      "Epoch 829/1000, Loss: 0.311072494696953\n",
      "Epoch 830/1000, Loss: 0.3110681174141545\n",
      "Epoch 831/1000, Loss: 0.31106380849008025\n",
      "Epoch 832/1000, Loss: 0.31106010003101897\n",
      "Epoch 833/1000, Loss: 0.31105569531238964\n",
      "Epoch 834/1000, Loss: 0.311051361491216\n",
      "Epoch 835/1000, Loss: 0.311047095425301\n",
      "Epoch 836/1000, Loss: 0.31104342656706524\n",
      "Epoch 837/1000, Loss: 0.3110390635445153\n",
      "Epoch 838/1000, Loss: 0.3110347708531168\n",
      "Epoch 839/1000, Loss: 0.3110305462357355\n",
      "Epoch 840/1000, Loss: 0.31102691522345977\n",
      "Epoch 841/1000, Loss: 0.3110225927746518\n",
      "Epoch 842/1000, Loss: 0.3110183403001426\n",
      "Epoch 843/1000, Loss: 0.3110141567693353\n",
      "Epoch 844/1000, Loss: 0.3110105625737352\n",
      "Epoch 845/1000, Loss: 0.31100628010458514\n",
      "Epoch 846/1000, Loss: 0.31100206730534713\n",
      "Epoch 847/1000, Loss: 0.310997924764336\n",
      "Epoch 848/1000, Loss: 0.31099436652089407\n",
      "Epoch 849/1000, Loss: 0.31099012355092504\n",
      "Epoch 850/1000, Loss: 0.3109859499560234\n",
      "Epoch 851/1000, Loss: 0.3109818483530836\n",
      "Epoch 852/1000, Loss: 0.31097832521118407\n",
      "Epoch 853/1000, Loss: 0.3109741212643859\n",
      "Epoch 854/1000, Loss: 0.3109699863975359\n",
      "Epoch 855/1000, Loss: 0.31096592567193393\n",
      "Epoch 856/1000, Loss: 0.3109624367624427\n",
      "Epoch 857/1000, Loss: 0.31095827134535775\n",
      "Epoch 858/1000, Loss: 0.3109541755213296\n",
      "Epoch 859/1000, Loss: 0.31095070670921326\n",
      "Epoch 860/1000, Loss: 0.31094708235594015\n",
      "Epoch 861/1000, Loss: 0.31095108004781796\n",
      "Epoch 862/1000, Loss: 0.310942659098582\n",
      "Epoch 863/1000, Loss: 0.31093719624662636\n",
      "Epoch 864/1000, Loss: 0.3109321431202842\n",
      "Epoch 865/1000, Loss: 0.31092767198956356\n",
      "Epoch 866/1000, Loss: 0.31092406606909956\n",
      "Epoch 867/1000, Loss: 0.3109286469409689\n",
      "Epoch 868/1000, Loss: 0.3109201410958351\n",
      "Epoch 869/1000, Loss: 0.31091407479726313\n",
      "Epoch 870/1000, Loss: 0.310909177386323\n",
      "Epoch 871/1000, Loss: 0.31090538721088457\n",
      "Epoch 872/1000, Loss: 0.3109019997159538\n",
      "Epoch 873/1000, Loss: 0.31090571504528575\n",
      "Epoch 874/1000, Loss: 0.31089736561305753\n",
      "Epoch 875/1000, Loss: 0.3108919722275904\n",
      "Epoch 876/1000, Loss: 0.3108870097055194\n",
      "Epoch 877/1000, Loss: 0.31088294813370304\n",
      "Epoch 878/1000, Loss: 0.31088718325838993\n",
      "Epoch 879/1000, Loss: 0.31087938647633767\n",
      "Epoch 880/1000, Loss: 0.31087331769775556\n",
      "Epoch 881/1000, Loss: 0.31086843857520524\n",
      "Epoch 882/1000, Loss: 0.31086414835460163\n",
      "Epoch 883/1000, Loss: 0.31086167357236993\n",
      "Epoch 884/1000, Loss: 0.31086525928914616\n",
      "Epoch 885/1000, Loss: 0.310856930538638\n",
      "Epoch 886/1000, Loss: 0.310851046169107\n",
      "Epoch 887/1000, Loss: 0.3108468359074838\n",
      "Epoch 888/1000, Loss: 0.3108432467242667\n",
      "Epoch 889/1000, Loss: 0.3108470577814135\n",
      "Epoch 890/1000, Loss: 0.31083877635900714\n",
      "Epoch 891/1000, Loss: 0.31083345607020796\n",
      "Epoch 892/1000, Loss: 0.3108288428429565\n",
      "Epoch 893/1000, Loss: 0.31082639468541134\n",
      "Epoch 894/1000, Loss: 0.31083231757644814\n",
      "Epoch 895/1000, Loss: 0.31082498791407465\n",
      "Epoch 896/1000, Loss: 0.31082297868807623\n",
      "Epoch 897/1000, Loss: 0.3108216279356811\n",
      "Epoch 898/1000, Loss: 0.3108219370274835\n",
      "Epoch 899/1000, Loss: 0.31082849463171874\n",
      "Epoch 900/1000, Loss: 0.31082256995897634\n",
      "Epoch 901/1000, Loss: 0.3108190177461771\n",
      "Epoch 902/1000, Loss: 0.3108172906725775\n",
      "Epoch 903/1000, Loss: 0.31081592386526974\n",
      "Epoch 904/1000, Loss: 0.3108217687170936\n",
      "Epoch 905/1000, Loss: 0.3108154298237265\n",
      "Epoch 906/1000, Loss: 0.3108122814693963\n",
      "Epoch 907/1000, Loss: 0.3108093424466616\n",
      "Epoch 908/1000, Loss: 0.3108080664611422\n",
      "Epoch 909/1000, Loss: 0.31081432738039055\n",
      "Epoch 910/1000, Loss: 0.3108076770434793\n",
      "Epoch 911/1000, Loss: 0.3108035588813602\n",
      "Epoch 912/1000, Loss: 0.31080161217293223\n",
      "Epoch 913/1000, Loss: 0.31080752846921805\n",
      "Epoch 914/1000, Loss: 0.310800782524681\n",
      "Epoch 915/1000, Loss: 0.3107962022372333\n",
      "Epoch 916/1000, Loss: 0.3107989484486236\n",
      "Epoch 917/1000, Loss: 0.31081475191255725\n",
      "Epoch 918/1000, Loss: 0.3108310583517466\n",
      "Epoch 919/1000, Loss: 0.31083058376453016\n",
      "Epoch 920/1000, Loss: 0.3108279751394297\n",
      "Epoch 921/1000, Loss: 0.310824628397042\n",
      "Epoch 922/1000, Loss: 0.31082014402979485\n",
      "Epoch 923/1000, Loss: 0.31082162464898794\n",
      "Epoch 924/1000, Loss: 0.31080929082346487\n",
      "Epoch 925/1000, Loss: 0.31079942890429024\n",
      "Epoch 926/1000, Loss: 0.31079049332795655\n",
      "Epoch 927/1000, Loss: 0.3107886972645235\n",
      "Epoch 928/1000, Loss: 0.31077486710338564\n",
      "Epoch 929/1000, Loss: 0.31076255750314596\n",
      "Epoch 930/1000, Loss: 0.31075206657510834\n",
      "Epoch 931/1000, Loss: 0.3107446913703757\n",
      "Epoch 932/1000, Loss: 0.3107464466278488\n",
      "Epoch 933/1000, Loss: 0.3107369607272339\n",
      "Epoch 934/1000, Loss: 0.31072915583534116\n",
      "Epoch 935/1000, Loss: 0.3107236862968703\n",
      "Epoch 936/1000, Loss: 0.31072596151029713\n",
      "Epoch 937/1000, Loss: 0.3107163563392275\n",
      "Epoch 938/1000, Loss: 0.31070849188649596\n",
      "Epoch 939/1000, Loss: 0.31070259218064344\n",
      "Epoch 940/1000, Loss: 0.3106974789448238\n",
      "Epoch 941/1000, Loss: 0.31070095682728827\n",
      "Epoch 942/1000, Loss: 0.3106940687970277\n",
      "Epoch 943/1000, Loss: 0.3106905416408634\n",
      "Epoch 944/1000, Loss: 0.3106877225126623\n",
      "Epoch 945/1000, Loss: 0.3106940475953849\n",
      "Epoch 946/1000, Loss: 0.31068622132726553\n",
      "Epoch 947/1000, Loss: 0.3106806163331177\n",
      "Epoch 948/1000, Loss: 0.31067358934046585\n",
      "Epoch 949/1000, Loss: 0.31067557587015343\n",
      "Epoch 950/1000, Loss: 0.31066266794980646\n",
      "Epoch 951/1000, Loss: 0.31065271017219037\n",
      "Epoch 952/1000, Loss: 0.3106427092167712\n",
      "Epoch 953/1000, Loss: 0.310634920664129\n",
      "Epoch 954/1000, Loss: 0.31063047328147697\n",
      "Epoch 955/1000, Loss: 0.31061678183629754\n",
      "Epoch 956/1000, Loss: 0.31060500119093465\n",
      "Epoch 957/1000, Loss: 0.31059315043487135\n",
      "Epoch 958/1000, Loss: 0.3105891758401796\n",
      "Epoch 959/1000, Loss: 0.31057127394201606\n",
      "Epoch 960/1000, Loss: 0.3105565119360517\n",
      "Epoch 961/1000, Loss: 0.3105439615801412\n",
      "Epoch 962/1000, Loss: 0.3105428776054889\n",
      "Epoch 963/1000, Loss: 0.31052910179520415\n",
      "Epoch 964/1000, Loss: 0.3105169595828885\n",
      "Epoch 965/1000, Loss: 0.3105076098759702\n",
      "Epoch 966/1000, Loss: 0.31050141500567835\n",
      "Epoch 967/1000, Loss: 0.3104876537096786\n",
      "Epoch 968/1000, Loss: 0.31047364084986306\n",
      "Epoch 969/1000, Loss: 0.31046171874026135\n",
      "Epoch 970/1000, Loss: 0.3104580571354341\n",
      "Epoch 971/1000, Loss: 0.3104400838764258\n",
      "Epoch 972/1000, Loss: 0.3104251898985924\n",
      "Epoch 973/1000, Loss: 0.3104111259312446\n",
      "Epoch 974/1000, Loss: 0.31040541910161573\n",
      "Epoch 975/1000, Loss: 0.3103861390893808\n",
      "Epoch 976/1000, Loss: 0.3103703021907553\n",
      "Epoch 977/1000, Loss: 0.310356604128615\n",
      "Epoch 978/1000, Loss: 0.31034932738289195\n",
      "Epoch 979/1000, Loss: 0.31033063075685496\n",
      "Epoch 980/1000, Loss: 0.31031372926687023\n",
      "Epoch 981/1000, Loss: 0.3103017690487202\n",
      "Epoch 982/1000, Loss: 0.31029598486482934\n",
      "Epoch 983/1000, Loss: 0.310278871922364\n",
      "Epoch 984/1000, Loss: 0.3102636862613651\n",
      "Epoch 985/1000, Loss: 0.3102595181800333\n",
      "Epoch 986/1000, Loss: 0.31024205693799506\n",
      "Epoch 987/1000, Loss: 0.3102265975876958\n",
      "Epoch 988/1000, Loss: 0.3102160520504993\n",
      "Epoch 989/1000, Loss: 0.310215634566606\n",
      "Epoch 990/1000, Loss: 0.31020329913448064\n",
      "Epoch 991/1000, Loss: 0.31019729483625386\n",
      "Epoch 992/1000, Loss: 0.3101863314143943\n",
      "Epoch 993/1000, Loss: 0.310184535520807\n",
      "Epoch 994/1000, Loss: 0.31017259323632823\n",
      "Epoch 995/1000, Loss: 0.31016363009402675\n",
      "Epoch 996/1000, Loss: 0.31015906956102424\n",
      "Epoch 997/1000, Loss: 0.3101611088966527\n",
      "Epoch 998/1000, Loss: 0.31015144111770443\n",
      "Epoch 999/1000, Loss: 0.31014590881318554\n",
      "Epoch 1000/1000, Loss: 0.31014957570688273\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.reshape(205, 1).shape[1]\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "hidden_size = 10\n",
    "\n",
    "# Train the model\n",
    "weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output = train_model(X_train, y_train.reshape(205,1), hidden_size, learning_rate, epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c10198-f6ce-4e1b-9846-549d0744de1d",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b5772596-6d9d-4530-acfb-573681e0b38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4400.0           4016.018119\n",
      "1              4000.0           4263.212183\n",
      "2              3475.0           3786.690141\n",
      "3              5250.0           5566.392920\n",
      "4              3950.0           3544.259714\n",
      "..                ...                   ...\n",
      "63             2850.0           3316.883480\n",
      "64             4450.0           4397.198708\n",
      "65             5500.0           5506.837942\n",
      "66             4500.0           4149.974093\n",
      "67             4750.0           5262.644275\n",
      "\n",
      "[68 rows x 2 columns]\n",
      "Validation Loss: 0.3307755901942887\n"
     ]
    }
   ],
   "source": [
    "# Perform forward pass on validation set\n",
    "val_output, _, _ = forward_pass(X_val, weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output)\n",
    "\n",
    "# Calculate validation loss\n",
    "val_loss = mae(y_val, val_output)\n",
    "\n",
    "val_normal_actual = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
    "val_normal_predict = scaler_y.inverse_transform(val_output.reshape(-1, 1))\n",
    "\n",
    "results_val = np.concatenate((val_normal_actual, val_normal_predict), axis=1)\n",
    "\n",
    "results_val_df = pd.DataFrame(results_val, columns=['Actual Weight (g)', 'Predicted Weight (g)'])\n",
    "\n",
    "print(results_val_df)\n",
    "print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0dc7efed-d8a9-4a2b-83df-aabad7f6f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4900.0           4802.594474\n",
      "1              4750.0           4898.932983\n",
      "2              5750.0           5456.480092\n",
      "3              3700.0           3422.835213\n",
      "4              4300.0           4165.783903\n",
      "..                ...                   ...\n",
      "64             4725.0           5146.369768\n",
      "65             3800.0           3460.379900\n",
      "66             4250.0           3733.899614\n",
      "67             6000.0           5387.535797\n",
      "68             3200.0           3272.078729\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "Test Loss: 0.36488158494635015\n"
     ]
    }
   ],
   "source": [
    "test_output, _, _ = forward_pass(X_test, weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output)\n",
    "\n",
    "test_normal_actual =  scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "test_normal_predict = scaler_y.inverse_transform(test_output.reshape(-1, 1))\n",
    "\n",
    "# Calculate test loss\n",
    "test_loss = mae(y_test, test_output)\n",
    "\n",
    "results_test = np.concatenate((test_normal_actual, test_normal_predict), axis=1)\n",
    "\n",
    "results_test_df = pd.DataFrame(results_test, columns=['Actual Weight (g)', 'Predicted Weight (g)'])\n",
    "\n",
    "print(results_test_df)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0005454-60c9-4c62-a7d7-ebb126736071",
   "metadata": {},
   "source": [
    "# 3 Keras fully connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d573c7-bd7d-4f3c-b442-60b565a9873e",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2c4a2dbf-2f33-46e5-bd8f-3700103e0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(16,input_shape = (X_train.shape[1],),activation= 'relu'))\n",
    "model_1.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "23fac53d-e5f1-4d51-bbe4-0e258e36d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compile the model \n",
    "\n",
    "model_1.compile(loss = tf.keras.losses.MeanAbsoluteError() ,\n",
    "               optimizer= keras.optimizers.Adam(learning_rate=.001) , \n",
    "               metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "173097ea-2ab7-48eb-93a3-1da18996705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "7/7 [==============================] - 2s 74ms/step - loss: 0.8570 - mae: 0.8570 - val_loss: 0.8104 - val_mae: 0.8104\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.8180 - mae: 0.8180 - val_loss: 0.7752 - val_mae: 0.7752\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.7801 - mae: 0.7801 - val_loss: 0.7388 - val_mae: 0.7388\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.7422 - mae: 0.7422 - val_loss: 0.7026 - val_mae: 0.7026\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.7059 - mae: 0.7059 - val_loss: 0.6692 - val_mae: 0.6692\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.6682 - mae: 0.6682 - val_loss: 0.6379 - val_mae: 0.6379\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.6314 - mae: 0.6314 - val_loss: 0.6056 - val_mae: 0.6056\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.5934 - mae: 0.5934 - val_loss: 0.5729 - val_mae: 0.5729\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 0.5561 - mae: 0.5561 - val_loss: 0.5392 - val_mae: 0.5392\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5183 - mae: 0.5183 - val_loss: 0.5063 - val_mae: 0.5063\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.4836 - mae: 0.4836 - val_loss: 0.4738 - val_mae: 0.4738\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.4546 - mae: 0.4546 - val_loss: 0.4454 - val_mae: 0.4454\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4286 - mae: 0.4286 - val_loss: 0.4231 - val_mae: 0.4231\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.4060 - mae: 0.4060 - val_loss: 0.4036 - val_mae: 0.4036\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3863 - mae: 0.3863 - val_loss: 0.3875 - val_mae: 0.3875\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3718 - mae: 0.3718 - val_loss: 0.3739 - val_mae: 0.3739\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3631 - mae: 0.3631 - val_loss: 0.3625 - val_mae: 0.3625\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3558 - mae: 0.3558 - val_loss: 0.3548 - val_mae: 0.3548\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3528 - mae: 0.3528 - val_loss: 0.3478 - val_mae: 0.3478\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3502 - mae: 0.3502 - val_loss: 0.3437 - val_mae: 0.3437\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3489 - mae: 0.3489 - val_loss: 0.3428 - val_mae: 0.3428\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3479 - mae: 0.3479 - val_loss: 0.3427 - val_mae: 0.3427\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3464 - mae: 0.3464 - val_loss: 0.3420 - val_mae: 0.3420\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3455 - mae: 0.3455 - val_loss: 0.3410 - val_mae: 0.3410\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3446 - mae: 0.3446 - val_loss: 0.3407 - val_mae: 0.3407\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3439 - mae: 0.3439 - val_loss: 0.3411 - val_mae: 0.3411\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3428 - mae: 0.3428 - val_loss: 0.3411 - val_mae: 0.3411\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3421 - mae: 0.3421 - val_loss: 0.3412 - val_mae: 0.3412\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3417 - mae: 0.3417 - val_loss: 0.3416 - val_mae: 0.3416\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3404 - mae: 0.3404 - val_loss: 0.3410 - val_mae: 0.3410\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3397 - mae: 0.3397 - val_loss: 0.3410 - val_mae: 0.3410\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3388 - mae: 0.3388 - val_loss: 0.3408 - val_mae: 0.3408\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3384 - mae: 0.3384 - val_loss: 0.3403 - val_mae: 0.3403\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3376 - mae: 0.3376 - val_loss: 0.3402 - val_mae: 0.3402\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.3369 - mae: 0.3369 - val_loss: 0.3401 - val_mae: 0.3401\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3363 - mae: 0.3363 - val_loss: 0.3405 - val_mae: 0.3405\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3360 - mae: 0.3360 - val_loss: 0.3417 - val_mae: 0.3417\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3352 - mae: 0.3352 - val_loss: 0.3410 - val_mae: 0.3410\n",
      "Epoch 39/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3350 - mae: 0.3350 - val_loss: 0.3400 - val_mae: 0.3400\n",
      "Epoch 40/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3347 - mae: 0.3347 - val_loss: 0.3386 - val_mae: 0.3386\n",
      "Epoch 41/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3338 - mae: 0.3338 - val_loss: 0.3385 - val_mae: 0.3385\n",
      "Epoch 42/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3328 - mae: 0.3328 - val_loss: 0.3382 - val_mae: 0.3382\n",
      "Epoch 43/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3324 - mae: 0.3324 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 44/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3326 - mae: 0.3326 - val_loss: 0.3383 - val_mae: 0.3383\n",
      "Epoch 45/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3316 - mae: 0.3316 - val_loss: 0.3386 - val_mae: 0.3386\n",
      "Epoch 46/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3310 - mae: 0.3310 - val_loss: 0.3382 - val_mae: 0.3382\n",
      "Epoch 47/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3303 - mae: 0.3303 - val_loss: 0.3385 - val_mae: 0.3385\n",
      "Epoch 48/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3306 - mae: 0.3306 - val_loss: 0.3393 - val_mae: 0.3393\n",
      "Epoch 49/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3297 - mae: 0.3297 - val_loss: 0.3382 - val_mae: 0.3382\n",
      "Epoch 50/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3293 - mae: 0.3293 - val_loss: 0.3361 - val_mae: 0.3361\n",
      "Epoch 51/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3288 - mae: 0.3288 - val_loss: 0.3354 - val_mae: 0.3354\n",
      "Epoch 52/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3286 - mae: 0.3286 - val_loss: 0.3357 - val_mae: 0.3357\n",
      "Epoch 53/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3280 - mae: 0.3280 - val_loss: 0.3365 - val_mae: 0.3365\n",
      "Epoch 54/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3273 - mae: 0.3273 - val_loss: 0.3364 - val_mae: 0.3364\n",
      "Epoch 55/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3270 - mae: 0.3270 - val_loss: 0.3363 - val_mae: 0.3363\n",
      "Epoch 56/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3267 - mae: 0.3267 - val_loss: 0.3354 - val_mae: 0.3354\n",
      "Epoch 57/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3262 - mae: 0.3262 - val_loss: 0.3345 - val_mae: 0.3345\n",
      "Epoch 58/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3259 - mae: 0.3259 - val_loss: 0.3335 - val_mae: 0.3335\n",
      "Epoch 59/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3256 - mae: 0.3256 - val_loss: 0.3330 - val_mae: 0.3330\n",
      "Epoch 60/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3254 - mae: 0.3254 - val_loss: 0.3339 - val_mae: 0.3339\n",
      "Epoch 61/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3247 - mae: 0.3247 - val_loss: 0.3333 - val_mae: 0.3333\n",
      "Epoch 62/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3245 - mae: 0.3245 - val_loss: 0.3334 - val_mae: 0.3334\n",
      "Epoch 63/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3241 - mae: 0.3241 - val_loss: 0.3330 - val_mae: 0.3330\n",
      "Epoch 64/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3239 - mae: 0.3239 - val_loss: 0.3325 - val_mae: 0.3325\n",
      "Epoch 65/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3236 - mae: 0.3236 - val_loss: 0.3325 - val_mae: 0.3325\n",
      "Epoch 66/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3234 - mae: 0.3234 - val_loss: 0.3323 - val_mae: 0.3323\n",
      "Epoch 67/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3231 - mae: 0.3231 - val_loss: 0.3327 - val_mae: 0.3327\n",
      "Epoch 68/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3230 - mae: 0.3230 - val_loss: 0.3333 - val_mae: 0.3333\n",
      "Epoch 69/1000\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3226 - mae: 0.3226 - val_loss: 0.3329 - val_mae: 0.3329\n",
      "Epoch 70/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3226 - mae: 0.3226 - val_loss: 0.3324 - val_mae: 0.3324\n",
      "Epoch 71/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3223 - mae: 0.3223 - val_loss: 0.3317 - val_mae: 0.3317\n",
      "Epoch 72/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3222 - mae: 0.3222 - val_loss: 0.3318 - val_mae: 0.3318\n",
      "Epoch 73/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3220 - mae: 0.3220 - val_loss: 0.3318 - val_mae: 0.3318\n",
      "Epoch 74/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3221 - mae: 0.3221 - val_loss: 0.3318 - val_mae: 0.3318\n",
      "Epoch 75/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3216 - mae: 0.3216 - val_loss: 0.3309 - val_mae: 0.3309\n",
      "Epoch 76/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3216 - mae: 0.3216 - val_loss: 0.3305 - val_mae: 0.3305\n",
      "Epoch 77/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3212 - mae: 0.3212 - val_loss: 0.3309 - val_mae: 0.3309\n",
      "Epoch 78/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3210 - mae: 0.3210 - val_loss: 0.3315 - val_mae: 0.3315\n",
      "Epoch 79/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3215 - mae: 0.3215 - val_loss: 0.3334 - val_mae: 0.3334\n",
      "Epoch 80/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3205 - mae: 0.3205 - val_loss: 0.3325 - val_mae: 0.3325\n",
      "Epoch 81/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3203 - mae: 0.3203 - val_loss: 0.3327 - val_mae: 0.3327\n",
      "Epoch 82/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3203 - mae: 0.3203 - val_loss: 0.3340 - val_mae: 0.3340\n",
      "Epoch 83/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3198 - mae: 0.3198 - val_loss: 0.3331 - val_mae: 0.3331\n",
      "Epoch 84/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3197 - mae: 0.3197 - val_loss: 0.3323 - val_mae: 0.3323\n",
      "Epoch 85/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3196 - mae: 0.3196 - val_loss: 0.3319 - val_mae: 0.3319\n",
      "Epoch 86/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3194 - mae: 0.3194 - val_loss: 0.3325 - val_mae: 0.3325\n",
      "Epoch 87/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3192 - mae: 0.3192 - val_loss: 0.3318 - val_mae: 0.3318\n",
      "Epoch 88/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3194 - mae: 0.3194 - val_loss: 0.3328 - val_mae: 0.3328\n",
      "Epoch 89/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3191 - mae: 0.3191 - val_loss: 0.3341 - val_mae: 0.3341\n",
      "Epoch 90/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3186 - mae: 0.3186 - val_loss: 0.3334 - val_mae: 0.3334\n",
      "Epoch 91/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3184 - mae: 0.3184 - val_loss: 0.3315 - val_mae: 0.3315\n",
      "Epoch 92/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3182 - mae: 0.3182 - val_loss: 0.3311 - val_mae: 0.3311\n",
      "Epoch 93/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3180 - mae: 0.3180 - val_loss: 0.3318 - val_mae: 0.3318\n",
      "Epoch 94/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3179 - mae: 0.3179 - val_loss: 0.3315 - val_mae: 0.3315\n",
      "Epoch 95/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3177 - mae: 0.3177 - val_loss: 0.3312 - val_mae: 0.3312\n",
      "Epoch 96/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3174 - mae: 0.3174 - val_loss: 0.3320 - val_mae: 0.3320\n",
      "Epoch 97/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3171 - mae: 0.3171 - val_loss: 0.3329 - val_mae: 0.3329\n",
      "Epoch 98/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3173 - mae: 0.3173 - val_loss: 0.3336 - val_mae: 0.3336\n",
      "Epoch 99/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3168 - mae: 0.3168 - val_loss: 0.3328 - val_mae: 0.3328\n",
      "Epoch 100/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3165 - mae: 0.3165 - val_loss: 0.3319 - val_mae: 0.3319\n",
      "Epoch 101/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3165 - mae: 0.3165 - val_loss: 0.3316 - val_mae: 0.3316\n"
     ]
    }
   ],
   "source": [
    "## fit the model \n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 25)\n",
    "# Create a ModelCheckpoint callback\n",
    "checkpoint_1 = ModelCheckpoint(\"best_model_1.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "train_1 = model_1.fit(X_train,y_train, epochs =1000,verbose =1,validation_data=(X_val,y_val), batch_size= 32,callbacks=[callback,checkpoint_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5da19f93-b6d2-40c2-b2f5-2c3c1b71bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(train_1.history['mae'])\n",
    "plt.title(\"Model MAE\")\n",
    "plt.ylabel(\"mae\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d70c3-122a-411b-bc82-b266c70b84f9",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "497750dd-17a2-47d3-b5df-54f3f27df2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(64,input_shape = (X_train.shape[1],),activation= 'relu'))\n",
    "model_2.add(Dense(32,activation='relu'))\n",
    "model_2.add(Dense(1,activation='linear'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d95bab2d-04e3-4942-8eff-3f3fd980e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compile the model \n",
    "\n",
    "model_2.compile(loss = tf.keras.losses.MeanAbsoluteError() ,\n",
    "               optimizer= keras.optimizers.Adam(learning_rate=.001) , \n",
    "               metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0b08730c-d2e7-4fc3-8786-83018810c6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 0.7464 - mae: 0.7464 - val_loss: 0.6017 - val_mae: 0.6017\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5899 - mae: 0.5899 - val_loss: 0.4769 - val_mae: 0.4769\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.4659 - mae: 0.4659 - val_loss: 0.3813 - val_mae: 0.3813\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3632 - mae: 0.3632 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3320 - mae: 0.3320 - val_loss: 0.3407 - val_mae: 0.3407\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3307 - mae: 0.3307 - val_loss: 0.3502 - val_mae: 0.3502\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3295 - mae: 0.3295 - val_loss: 0.3433 - val_mae: 0.3433\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3228 - mae: 0.3228 - val_loss: 0.3366 - val_mae: 0.3366\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3222 - mae: 0.3222 - val_loss: 0.3356 - val_mae: 0.3356\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3209 - mae: 0.3209 - val_loss: 0.3363 - val_mae: 0.3363\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3185 - mae: 0.3185 - val_loss: 0.3359 - val_mae: 0.3359\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3175 - mae: 0.3175 - val_loss: 0.3381 - val_mae: 0.3381\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3174 - mae: 0.3174 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3154 - mae: 0.3154 - val_loss: 0.3342 - val_mae: 0.3342\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3145 - mae: 0.3145 - val_loss: 0.3331 - val_mae: 0.3331\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3137 - mae: 0.3137 - val_loss: 0.3340 - val_mae: 0.3340\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3154 - mae: 0.3154 - val_loss: 0.3369 - val_mae: 0.3369\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3122 - mae: 0.3122 - val_loss: 0.3343 - val_mae: 0.3343\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3153 - mae: 0.3153 - val_loss: 0.3323 - val_mae: 0.3323\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3133 - mae: 0.3133 - val_loss: 0.3339 - val_mae: 0.3339\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3123 - mae: 0.3123 - val_loss: 0.3369 - val_mae: 0.3369\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3112 - mae: 0.3112 - val_loss: 0.3320 - val_mae: 0.3320\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3102 - mae: 0.3102 - val_loss: 0.3354 - val_mae: 0.3354\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3108 - mae: 0.3108 - val_loss: 0.3352 - val_mae: 0.3352\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3087 - mae: 0.3087 - val_loss: 0.3317 - val_mae: 0.3317\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3102 - mae: 0.3102 - val_loss: 0.3311 - val_mae: 0.3311\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3089 - mae: 0.3089 - val_loss: 0.3337 - val_mae: 0.3337\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3081 - mae: 0.3081 - val_loss: 0.3318 - val_mae: 0.3318\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3085 - mae: 0.3085 - val_loss: 0.3350 - val_mae: 0.3350\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3077 - mae: 0.3077 - val_loss: 0.3340 - val_mae: 0.3340\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3059 - mae: 0.3059 - val_loss: 0.3341 - val_mae: 0.3341\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3058 - mae: 0.3058 - val_loss: 0.3350 - val_mae: 0.3350\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3064 - mae: 0.3064 - val_loss: 0.3314 - val_mae: 0.3314\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3067 - mae: 0.3067 - val_loss: 0.3325 - val_mae: 0.3325\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3051 - mae: 0.3051 - val_loss: 0.3338 - val_mae: 0.3338\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3049 - mae: 0.3049 - val_loss: 0.3379 - val_mae: 0.3379\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3046 - mae: 0.3046 - val_loss: 0.3353 - val_mae: 0.3353\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3041 - mae: 0.3041 - val_loss: 0.3365 - val_mae: 0.3365\n",
      "Epoch 39/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3046 - mae: 0.3046 - val_loss: 0.3345 - val_mae: 0.3345\n",
      "Epoch 40/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3064 - mae: 0.3064 - val_loss: 0.3347 - val_mae: 0.3347\n",
      "Epoch 41/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3044 - mae: 0.3044 - val_loss: 0.3391 - val_mae: 0.3391\n",
      "Epoch 42/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3053 - mae: 0.3053 - val_loss: 0.3404 - val_mae: 0.3404\n",
      "Epoch 43/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3048 - mae: 0.3048 - val_loss: 0.3391 - val_mae: 0.3391\n",
      "Epoch 44/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3027 - mae: 0.3027 - val_loss: 0.3385 - val_mae: 0.3385\n",
      "Epoch 45/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3019 - mae: 0.3019 - val_loss: 0.3380 - val_mae: 0.3380\n",
      "Epoch 46/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3011 - mae: 0.3011 - val_loss: 0.3367 - val_mae: 0.3367\n",
      "Epoch 47/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3035 - mae: 0.3035 - val_loss: 0.3349 - val_mae: 0.3349\n",
      "Epoch 48/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3014 - mae: 0.3014 - val_loss: 0.3388 - val_mae: 0.3388\n",
      "Epoch 49/1000\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.3026 - mae: 0.3026 - val_loss: 0.3415 - val_mae: 0.3415\n",
      "Epoch 50/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.2994 - mae: 0.2994 - val_loss: 0.3363 - val_mae: 0.3363\n",
      "Epoch 51/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3023 - mae: 0.3023 - val_loss: 0.3363 - val_mae: 0.3363\n"
     ]
    }
   ],
   "source": [
    "## fit the model \n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 25)\n",
    "# Create a ModelCheckpoint callback\n",
    "checkpoint_2 = ModelCheckpoint(\"best_model_2.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "train_2 = model_2.fit(X_train,y_train, epochs =1000,verbose =1,validation_data=(X_val,y_val), batch_size= 32,callbacks=[callback,checkpoint_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a4fc7eaf-1393-44bb-85ba-7ae36271d071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/jklEQVR4nO3deZxT5d3///dJJsns+8bAsImCoICCUMR+rXUst1qX3lWwLnDj0ruKVh27iFZw6c+x9dZS3KhWtK0LuGvFoojCXRWLgtwFK6MgqzAbs2/JTHJ+f2QSmDLALEnOJLyej0cekzk5J/nklHbevc7nXJdhmqYpAACAGGGzugAAAIBQItwAAICYQrgBAAAxhXADAABiCuEGAADEFMINAACIKYQbAAAQUwg3AAAgphBuAABATCHcAOi3DMPQnXfe2ePjtm/fLsMw9PTTT4e8JgD9H+EGwGE9/fTTMgxDhmHogw8+OOh10zRVWFgowzD0/e9/34IKe2/VqlXB7/bMM890uc/UqVNlGIZOOOGELl/3er0qKCiQYRj629/+1uU+d955Z/BzunqUlZWF7DsBkOKsLgBAdIiPj9dzzz2n0047rdP21atXa/fu3XK5XBZV1neB73b55Zd32r59+3Z99NFHio+PP+Sx7733nvbu3auhQ4fq2Wef1dlnn33IfR977DElJycftD09Pb3XtQM4GOEGQLecc845evHFF7Vw4ULFxe3/n47nnntOEyZMUFVVlYXV9c0555yjN954Q1VVVcrOzg5uf+6555SXl6djjz1WNTU1XR77zDPP6OSTT9asWbN02223qampSUlJSV3ue9FFF3V6fwDhwWUpAN3yox/9SPv27dOKFSuC2zwej1566SVdeumlXR7T1NSkW265RYWFhXK5XBo5cqT+53/+R6ZpdtrP7Xbr5ptvVk5OjlJSUnT++edr9+7dXb7nN998oyuvvFJ5eXlyuVwaM2aMFi9e3KfvdsEFF8jlcunFF1/stP25557T9OnTZbfbuzyupaVFr776qi655BJNnz5dLS0tev311/tUC4C+I9wA6JahQ4dqypQpev7554Pb/va3v6murk6XXHLJQfubpqnzzz9fv/vd7/Qf//EfevDBBzVy5Ej9/Oc/V3Fxcad9r776ai1YsEDf+973dN9998nhcOjcc8896D3Ly8v1rW99S++++66uv/56/f73v9eIESN01VVXacGCBb3+bomJibrgggs6fbf/+7//0+eff37I4CZJb7zxhhobG3XJJZcoPz9f3/nOd/Tss88ecv/q6mpVVVV1etTW1va6bgCHYALAYTz11FOmJPOTTz4xH374YTMlJcVsbm42TdM0L774YvOMM84wTdM0hwwZYp577rnB41577TVTkvnrX/+60/tddNFFpmEY5pYtW0zTNM0NGzaYkszrrruu036XXnqpKcmcP39+cNtVV11lDhgwwKyqquq07yWXXGKmpaUF69q2bZspyXzqqacO+93ef/99U5L54osvmm+++aZpGIa5c+dO0zRN8+c//7k5fPhw0zRN8/TTTzfHjBlz0PHf//73zalTpwZ/f/zxx824uDizoqKi037z5883JXX5GDly5GFrBNBzjNwA6LbApZc333xTDQ0NevPNNw85svHWW2/Jbrfrpz/9aaftt9xyi0zTDN5Z9NZbb0nSQfvddNNNnX43TVMvv/yyzjvvPJmm2Wn0Y9q0aaqrq9P69et7/d2+973vKTMzU0uWLJFpmlqyZIl+9KMfHXL/ffv26e233+60zw9/+EMZhqEXXnihy2NefvllrVixotPjqaee6nXNALpGQzGAbsvJyVFRUZGee+45NTc3y+v16qKLLupy3x07dqigoEApKSmdth9//PHB1wM/bTabjjnmmE77jRw5stPvlZWVqq2t1eOPP67HH3+8y8+sqKjo1feSJIfDoYsvvljPPfecJk2apF27dh32ktTSpUvV1tamk046SVu2bAlunzx5sp599lnNmTPnoGP+3//7fzQUAxFAuAHQI5deeqmuueYalZWV6eyzz47Ybcw+n0+SdPnll2vWrFld7jN27Ng+fcall16qRYsW6c4779S4ceM0evToQ+4b6K2ZOnVql69//fXXGj58eJ/qAdA7hBsAPfKDH/xA//3f/62PP/5YS5cuPeR+Q4YM0bvvvquGhoZOozebN28Ovh746fP5tHXr1k6jNaWlpZ3eL3AnldfrVVFRUSi/UtBpp52mwYMHa9WqVfrNb35zyP22bdumjz76SNdff71OP/30Tq/5fD5dccUVeu655/SrX/0qLHUCODx6bgD0SHJysh577DHdeeedOu+88w653znnnCOv16uHH3640/bf/e53MgwjONld4OfChQs77ffvdz/Z7Xb98Ic/1Msvv6xNmzYd9HmVlZW9+TqdGIahhQsXav78+briiisOuV9g1OYXv/iFLrrook6P6dOn6/TTTz/sXVMAwouRGwA9dqjLQgc677zzdMYZZ+j222/X9u3bNW7cOL3zzjt6/fXXddNNNwV7bMaPH68f/ehHevTRR1VXV6dTTz1VK1eu7NTHEnDffffp/fff1+TJk3XNNddo9OjRqq6u1vr16/Xuu++qurq6z9/tggsu0AUXXHDYfZ599lmNHz9ehYWFXb5+/vnn64YbbtD69et18sknB7e/9NJLXc5QfNZZZykvL69vhQMIItwACAubzaY33nhD8+bN09KlS/XUU09p6NChuv/++3XLLbd02nfx4sXKycnRs88+q9dee03f/e53tWzZsoPCQ15entauXau7775br7zyih599FFlZWVpzJgxh72MFErr16/X5s2bdccddxxyn/POO0833HBDcPbigGuvvbbL/d9//33CDRBChmn+21ShAAAAUYyeGwAAEFMINwAAIKYQbgAAQEwh3AAAgJhCuAEAADGFcAMAAGLKUTfPjc/n0549e5SSkiLDMKwuBwAAdINpmmpoaFBBQYFstsOPzRx14WbPnj2HnFUUAAD0b7t27dKgQYMOu89RF24CC/jt2rVLqampFlcDAAC6o76+XoWFhZ0W4j2Uoy7cBC5FpaamEm4AAIgy3WkpoaEYAADEFMINAACIKYQbAAAQUwg3AAAgphBuAABATCHcAACAmEK4AQAAMYVwAwAAYgrhBgAAxBTCDQAAiCmEGwAAEFMINwAAIKYQbkKk3etTRX2rdu5rtroUAACOaoSbEFm7rVqT7l2pq/70idWlAABwVCPchEh6olOSVNPssbgSAACOboSbEMlMCoSbNpmmaXE1AAAcvQg3IZKe6JAkeX2m6lvbLa4GAICjF+EmROIddiU67ZKkmiYuTQEAYBXCTQhl0HcDAIDlCDchtL/vhnADAIBVCDchFOi7qW5qs7gSAACOXoSbEAqM3NQycgMAgGUINyEU6LmppqEYAADLEG5CiIZiAACsR7gJocwkf89NDT03AABYhnATQoElGKoZuQEAwDKEmxCioRgAAOsRbkJof0Mxl6UAALAK4SaEMjp6bmqbPSyeCQCARQg3IRQYuWn3mWpws3gmAABWINyEULzDrgQHi2cCAGAlwk2IBZqKmcgPAABrEG5CbH/fDU3FAABYgXATYizBAACAtQg3IcYSDAAAWItwE2KBnhvCDQAA1iDchFh6or/nhon8AACwBuEmxFiCAQAAaxFuQoyGYgAArEW4CTEaigEAsBbhJsQC89zUMM8NAACWINyEWHDkponFMwEAsALhJsQOXDyzkcUzAQCIOMJNiCU4D1w8k0tTAABEWr8IN4888oiGDh2q+Ph4TZ48WWvXrj3kvt/5zndkGMZBj3PPPTeCFR9eRmCuG5qKAQCIOMvDzdKlS1VcXKz58+dr/fr1GjdunKZNm6aKioou93/llVe0d+/e4GPTpk2y2+26+OKLI1z5oWUwSzEAAJaxPNw8+OCDuuaaazR79myNHj1aixYtUmJiohYvXtzl/pmZmcrPzw8+VqxYocTExH4VboJLMDDXDQAAEWdpuPF4PFq3bp2KioqC22w2m4qKirRmzZpuvceTTz6pSy65RElJSeEqs8fSmcgPAADLxFn54VVVVfJ6vcrLy+u0PS8vT5s3bz7i8WvXrtWmTZv05JNPHnIft9stt9sd/L2+vr73BXdTZkfPTS1z3QAAEHGWX5bqiyeffFInnniiJk2adMh9SkpKlJaWFnwUFhaGva7gyA09NwAARJyl4SY7O1t2u13l5eWdtpeXlys/P/+wxzY1NWnJkiW66qqrDrvf3LlzVVdXF3zs2rWrz3UfCYtnAgBgHUvDjdPp1IQJE7Ry5crgNp/Pp5UrV2rKlCmHPfbFF1+U2+3W5Zdfftj9XC6XUlNTOz3CLXC3FD03AABEnqU9N5JUXFysWbNmaeLEiZo0aZIWLFigpqYmzZ49W5I0c+ZMDRw4UCUlJZ2Oe/LJJ3XhhRcqKyvLirIPKzDPDZP4AQAQeZaHmxkzZqiyslLz5s1TWVmZxo8fr+XLlwebjHfu3CmbrfMAU2lpqT744AO98847VpR8RKwMDgCAdQzzKFvdsb6+XmlpaaqrqwvbJao9tS069b735LAb+vLXZ8swjLB8DgAAR4ue/P2O6rul+qvAyE2b11STx2txNQAAHF0IN2GQ4LQr3uE/tcxSDABAZBFuwiSDWYoBALAE4SZMaCoGAMAahJswyWRlcAAALEG4CZP0jrluqpnrBgCAiCLchAlLMAAAYA3CTZjQUAwAgDUIN2ESWIKhtpnLUgAARBLhJkxYPBMAAGsQbsKEW8EBALAG4SZMuBUcAABrEG7CJHBZqqapTUfZ2qQAAFiKcBMmgYZij9fH4pkAAEQQ4SZMEhx2ueJYPBMAgEgj3ISJYRj03QAAYAHCTRilB++YYq4bAAAihXATRplJ/r4bLksBABA5hJswYgkGAAAij3ATRoFww+KZAABEDuEmjIJLMBBuAACIGMJNGAXmuqlpoqEYAIBIIdyEEbeCAwAQeYSbMKKhGACAyCPchNH+hmIuSwEAECmEmzDK6JjnprrZw+KZAABECOEmjAI9N552n5pZPBMAgIgg3IRRgsMuZ2DxTJqKAQCICMJNGBmGoczA+lLcDg4AQEQQbsIsPXF/3w0AAAg/wk2YBfpuWIIBAIDIINyEWXAJBua6AQAgIgg3YRZcgoG5bgAAiAjCTZjtbyhm5AYAgEgg3IQZK4MDABBZhJsw278EA+EGAIBIINyE2f6GYnpuAACIBMJNmAUbium5AQAgIgg3YRa4LFXD4pkAAEQE4SbMApP4udt9amlj8UwAAMKNcBNmiU67nPbA4pn03QAAEG6EmzAzDEMZSfTdAAAQKYSbCAj03bAEAwAA4Ue4iYADm4oBAEB4EW4iINBUzGUpAADCj3ATAekdc91U01AMAEDYEW4iIDBywxIMAACEH+EmAmgoBgAgcgg3ERC4FbyWy1IAAIQd4SYCGLkBACByCDcRELxbip4bAADCjnATAcxzAwBA5BBuIiCjY+Smtc2nFg+LZwIAEE6EmwhIctrlsBuSpGpGbwAACCvCTQQYhrH/0hRNxQAAhBXhJkJoKgYAIDIINxESWIKhhrluAAAIK8JNhLB4JgAAkUG4iRAm8gMAIDIINxESCDcsngkAQHgRbiIkMNdNNT03AACEFeEmQjKDi2cycgMAQDgRbiIknZ4bAAAignATIZlM4gcAQEQQbiJk/+KZ9NwAABBOloebRx55REOHDlV8fLwmT56stWvXHnb/2tpazZkzRwMGDJDL5dJxxx2nt956K0LV9l5GR89NS5uXxTMBAAijOCs/fOnSpSouLtaiRYs0efJkLViwQNOmTVNpaalyc3MP2t/j8eiss85Sbm6uXnrpJQ0cOFA7duxQenp65IvvoWRXnBx2Q21eUzXNHiU4E6wuCQCAmGRpuHnwwQd1zTXXaPbs2ZKkRYsWadmyZVq8eLFuvfXWg/ZfvHixqqur9dFHH8nh8I+EDB06NJIl95phGEpPdKqywa2aZo8K0gk3AACEg2WXpTwej9atW6eioqL9xdhsKioq0po1a7o85o033tCUKVM0Z84c5eXl6YQTTtC9994rr/fQl3ncbrfq6+s7Payyv6mYvhsAAMLFsnBTVVUlr9ervLy8Ttvz8vJUVlbW5TFff/21XnrpJXm9Xr311lu644479MADD+jXv/71IT+npKREaWlpwUdhYWFIv0dPBPpuWBkcAIDwsbyhuCd8Pp9yc3P1+OOPa8KECZoxY4Zuv/12LVq06JDHzJ07V3V1dcHHrl27IlhxZ/vvmCLcAAAQLpb13GRnZ8tut6u8vLzT9vLycuXn53d5zIABA+RwOGS324Pbjj/+eJWVlcnj8cjpdB50jMvlksvlCm3xvRRcgoG5bgAACBvLRm6cTqcmTJiglStXBrf5fD6tXLlSU6ZM6fKYqVOnasuWLfL5fMFtX375pQYMGNBlsOlvMhIDSzDQcwMAQLhYelmquLhYTzzxhP70pz/piy++0LXXXqumpqbg3VMzZ87U3Llzg/tfe+21qq6u1o033qgvv/xSy5Yt07333qs5c+ZY9RV6JIMlGAAACDtLbwWfMWOGKisrNW/ePJWVlWn8+PFavnx5sMl4586dstn256/CwkK9/fbbuvnmmzV27FgNHDhQN954o375y19a9RV6JDOJnhsAAMLNME3TtLqISKqvr1daWprq6uqUmpoa0c9+f3OFZj/9iU4YmKo3b/h2RD8bAIBo1pO/31F1t1S0CzYUNzJyAwBAuBBuIignxX/XVlWjR0fZgBkAABFDuImg7GT/yI3H6+OOKQAAwoRwE0GuOLvSO24Hr2x0W1wNAACxiXATYbkdl6Yq6gk3AACEA+EmwnJT4iVJFQ2tFlcCAEBsItxEWHDkpoGRGwAAwoFwE2E5XJYCACCsCDcRFgg3NBQDABAehJsIy03t6Lmpp+cGAIBwINxEWKDnppKeGwAAwoJwE2E5NBQDABBWhJsIC4zcNLrb1expt7gaAABiD+EmwpJdcUpw2CVxaQoAgHAg3ESYYRjKTeXSFAAA4UK4sQBLMAAAED6EGwvsbyrmdnAAAEKNcGOBwPpS9NwAABB6hBsLcDs4AADhQ7ixAItnAgAQPoQbC+xfPJOeGwAAQo1wY4FAz00Vi2cCABByhBsLBOa52dfkUbvXZ3E1AADEFsKNBTITnYqzGTJNqarRY3U5AADEFMKNBWw2Q9nJzHUDAEA4EG4sksMsxQAAhAXhxiKB28EraSoGACCkCDcWCS6eycgNAAAhRbixSE7H7eD03AAAEFqEG4swSzEAAOFBuLEI60sBABAehBuLBEZuqgg3AACEFOHGIrmp/p6byga3TNO0uBoAAGIH4cYi2clOSZLH61Ntc5vF1QAAEDsINxZxxdmVnuiQRN8NAAChRLixUHAiP8INAAAhQ7ixUC5z3QAAEHKEGwsx1w0AAKFHuLFQDkswAAAQcr0ON3/5y180depUFRQUaMeOHZKkBQsW6PXXXw9ZcbEuJzkwcsNlKQAAQqVX4eaxxx5TcXGxzjnnHNXW1srr9UqS0tPTtWDBglDWF9MOnOsGAACERq/CzUMPPaQnnnhCt99+u+x2e3D7xIkTtXHjxpAVF+u4WwoAgNDrVbjZtm2bTjrppIO2u1wuNTU19bmoowUNxQAAhF6vws2wYcO0YcOGg7YvX75cxx9/fF9rOmoEFs9sdLer2dNucTUAAMSGuN4cVFxcrDlz5qi1tVWmaWrt2rV6/vnnVVJSoj/+8Y+hrjFmJbvilOCwq6XNq8oGt4Zk9eo/DgAAcIBe/TW9+uqrlZCQoF/96ldqbm7WpZdeqoKCAv3+97/XJZdcEuoaY5ZhGMpNdWnHvmZVNLg1JCvJ6pIAAIh6vR4quOyyy3TZZZepublZjY2Nys3NDWVdR43clI5ww1w3AACERJ+vgyQmJioxMTEUtRyVWIIBAIDQ6nW4eemll/TCCy9o586d8ng8nV5bv359nws7WuRwxxQAACHVq7ulFi5cqNmzZysvL0+fffaZJk2apKysLH399dc6++yzQ11jTMthrhsAAEKqV+Hm0Ucf1eOPP66HHnpITqdTv/jFL7RixQr99Kc/VV1dXahrjGnMdQMAQGj1Ktzs3LlTp556qiQpISFBDQ0NkqQrrrhCzz//fOiqOwoElmCoqKfnBgCAUOhVuMnPz1d1dbUkafDgwfr4448l+WcuNk0zdNUdBQKLZ3JZCgCA0OhVuPnud7+rN954Q5I0e/Zs3XzzzTrrrLM0Y8YM/eAHPwhpgbEuN9UfbqqbPWrz+iyuBgCA6Neru6Uef/xx+Xz+P8Rz5sxRdna2PvzwQ51//vn6yU9+EtICY11molNxNkPtPlP7Gj3KT4u3uiQAAKJar0ZubDab2tvbtXbtWr355ptKSEhQUVGRhgwZouXLl4e6xphmsxnKTg40FdN3AwBAX/Vq5Gb58uW64oortG/fvoNeMwxDXq+3z4UdTXJTXSqrb2WWYgAAQqBXIzc33HCDpk+frr1798rn83V6EGx6LieZ28EBAAiVXoWb8vJyFRcXKy8vL9T1HJUCTcXcMQUAQN/1KtxcdNFFWrVqVYhLOXrlsL4UAAAh06uem4cfflgXX3yx/v73v+vEE0+Uw+Ho9PpPf/rTkBR3tGCWYgAAQqdX4eb555/XO++8o/j4eK1atUqGYQRfMwyDcNNDLJ4JAEDo9Crc3H777brrrrt06623ymbr1ZUtHCAwclNFuAEAoM96lUw8Ho9mzJhBsAmRwPpSlQ1ulq8AAKCPepVOZs2apaVLl4asiEceeURDhw5VfHy8Jk+erLVr1x5y36efflqGYXR6xMdH96y+gVvBPV6fapvbLK4GAIDo1qvLUl6vV7/97W/19ttva+zYsQc1FD/44IPdfq+lS5equLhYixYt0uTJk7VgwQJNmzZNpaWlys3N7fKY1NRUlZaWBn8/sOcnGjnjbMpIdKimuU0VDW5lJDmtLgkAgKjVq3CzceNGnXTSSZKkTZs2dXqtp0HjwQcf1DXXXKPZs2dLkhYtWqRly5Zp8eLFuvXWW7s8xjAM5efn96Ly/isnxdURblo1Mj/F6nIAAIhavQo377//fkg+3OPxaN26dZo7d25wm81mU1FRkdasWXPI4xobGzVkyBD5fD6dfPLJuvfeezVmzJgu93W73XK79zfq1tfXh6T2UMtNideX5Y1M5AcAQB9Z2hFcVVUlr9d70EzHeXl5Kisr6/KYkSNHavHixXr99df1zDPPyOfz6dRTT9Xu3bu73L+kpERpaWnBR2FhYci/Rygw1w0AAKERdbc7TZkyRTNnztT48eN1+umn65VXXlFOTo7+8Ic/dLn/3LlzVVdXF3zs2rUrwhV3T07HEgwsngkAQN/06rJUqGRnZ8tut6u8vLzT9vLy8m731DgcDp100knasmVLl6+7XC65XK4+1xpuuSzBAABASFg6cuN0OjVhwgStXLkyuM3n82nlypWaMmVKt97D6/Vq48aNGjBgQLjKjAhmKQYAIDQsHbmRpOLiYs2aNUsTJ07UpEmTtGDBAjU1NQXvnpo5c6YGDhyokpISSdLdd9+tb33rWxoxYoRqa2t1//33a8eOHbr66qut/Bp9xizFAACEhuXhZsaMGaqsrNS8efNUVlam8ePHa/ny5cEm4507d3aaCbmmpkbXXHONysrKlJGRoQkTJuijjz7S6NGjrfoKIUFDMQAAoWGYR9l8//X19UpLS1NdXZ1SU1OtLieo0d2uE+a/LUn6193TlOi0PHcCANBv9OTvd9TdLRWrkpx2JTjskrhjCgCAviDc9BOGYSi343bwykbCDQAAvUW46UeCfTeM3AAA0GuEm36EuW4AAOg7wk0/wlw3AAD0HeGmH8nhshQAAH1GuOlHAj03NBQDANB7hJt+JDe1o+emnp4bAAB6i3DTjwRHbui5AQCg1wg3/Uig52Zfk0dtXp/F1QAAEJ0IN/1IZqJTcTZDkrSv0WNxNQAARCfCTT9isxnKTg7cDk7fDQAAvUG46WcCSzBwOzgAAL1DuOlncpnIDwCAPiHc9DP7ZynmshQAAL1BuOlncjrWl+J2cAAAeodw089wWQoAgL4h3PQzhBsAAPqGcNPP7F88k54bAAB6g3DTzxSkJ0jyj9x4fabF1QAAEH0IN/1MTrJLDrshr8/kjikAAHqBcNPP2GyG8tP8d0ztqW2xuBoAAKIP4aYfKkjzX5r6ppaRGwAAeopw0w8N7Oi7YeQGAICeI9z0QwPSuSwFAEBvEW76oQJGbgAA6DXCTT8UCDf03AAA0HOEm36InhsAAHqPcNMPDei4FbyupU2N7naLqwEAILoQbvqhlHiHUuPjJEl7Gb0BAKBHCDf91P6+G8INAAA9QbjppwJ9N3vraCoGAKAnCDf9FLeDAwDQO4SbfiowkR+XpQAA6BnCTT/F7eAAAPQO4aaf2n9Zip4bAAB6gnDTTxUEG4pb5POZFlcDAED0INz0U3kpLtkMqc1rqqrRbXU5AABEDcJNPxVntyk/laZiAAB6inDTj9F3AwBAzxFu+jHmugEAoOcIN/1YMNzUEW4AAOguwk0/NrBjIj9GbgAA6D7CTT82II2eGwAAeopw04/RcwMAQM8RbvqxwBIM+5o8am3zWlwNAADRgXDTj6UmxCnJaZfE6A0AAN1FuOnHDMNgrhsAAHqIcNPP0XcDAEDPEG76uUC4YQkGAAC6h3DTzzHXDQAAPUO46ecCIzd76+i5AQCgOwg3/dz+ifwYuQEAoDsIN/3cwAN6bkzTtLgaAAD6P8JNP5eX5pJhSO52n6qbPFaXAwBAv0e46edccXblJLskMdcNAADdQbiJAtwODgBA9xFuosBAJvIDAKDbCDdRoIC5bgAA6DbCTRQILsFQR7gBAOBICDdRYH/PDQ3FAAAcCeEmCgR6bvZyWQoAgCMi3ESBAWn+npuKBrfc7V6LqwEAoH8j3ESBzCSnXHH+/6jK69wWVwMAQP/WL8LNI488oqFDhyo+Pl6TJ0/W2rVru3XckiVLZBiGLrzwwvAWaDHDMDotwwAAAA7N8nCzdOlSFRcXa/78+Vq/fr3GjRunadOmqaKi4rDHbd++XT/72c/07W9/O0KVWquAuW4AAOgWy8PNgw8+qGuuuUazZ8/W6NGjtWjRIiUmJmrx4sWHPMbr9eqyyy7TXXfdpeHDh0ewWusw1w0AAN1jabjxeDxat26dioqKgttsNpuKioq0Zs2aQx539913Kzc3V1ddddURP8Ptdqu+vr7TIxox1w0AAN1jabipqqqS1+tVXl5ep+15eXkqKyvr8pgPPvhATz75pJ544olufUZJSYnS0tKCj8LCwj7XbQXmugEAoHssvyzVEw0NDbriiiv0xBNPKDs7u1vHzJ07V3V1dcHHrl27wlxleLC+FAAA3RNn5YdnZ2fLbrervLy80/by8nLl5+cftP/WrVu1fft2nXfeecFtPp9PkhQXF6fS0lIdc8wxnY5xuVxyuVxhqD6yDmwoNk1ThmFYXBEAAP2TpSM3TqdTEyZM0MqVK4PbfD6fVq5cqSlTphy0/6hRo7Rx40Zt2LAh+Dj//PN1xhlnaMOGDVF7yak7AhP5NXu8qm9pt7gaAAD6L0tHbiSpuLhYs2bN0sSJEzVp0iQtWLBATU1Nmj17tiRp5syZGjhwoEpKShQfH68TTjih0/Hp6emSdND2WBPvsCsryal9TR59U9uitESH1SUBANAvWR5uZsyYocrKSs2bN09lZWUaP368li9fHmwy3rlzp2y2qGoNCpuC9ATta/JoT22LRhekWl0OAAD9kmGapml1EZFUX1+vtLQ01dXVKTU1ugLCf//lU739ebnuvmCMZk4ZanU5AABETE/+fjMkEkUKWIIBAIAjItxEkf23gzPXDQAAh0K4iSKsLwUAwJERbqII4QYAgCMj3ESRwOKZ5fWtavP6LK4GAID+iXATRbKTXHLabfKZ/oADAAAORriJIjabofyOmYppKgYAoGuEmygTuDS1t46+GwAAukK4iTLMdQMAwOERbqLMQO6YAgDgsAg3UaaAifwAADgswk2UYa4bAAAOj3ATZQZ2NBTTcwMAQNcIN1FmQJp/5KahtV31rW0WVwMAQP9DuIkySa44pSc6JEl76bsBAOAghJsoFBi9oe8GAICDEW6iEH03AAAcGuEmCgXumGKWYgAADka4iULMdQMAwKERbqIQSzAAAHBohJsoFOi52VXdLNM0La4GAID+hXAThY7NS5Ezzqa9da3a9E291eUAANCvEG6iUGq8Q9PG5EuSXvh0l8XVAADQvxBuotSMiYWSpNc2fKPWNq/F1QAA0H8QbqLUqcdkaWB6ghpa2/X252VWlwMAQL9BuIlSNpuhiycOkiQt/YRLUwAABBBuothFEwbJMKSPtu7Trupmq8sBAKBfINxEsUEZiTptRLYk6UUaiwEAkES4iXoXdzQWv7Rut7w+5rwBAIBwE+W+NzpPaQkO7alr1QdbqqwuBwAAyxFuoly8w64LxxdIYs4bAAAkwk1MCFyaWvF5uWqaPBZXAwCAtQg3MeCEgWkaU5Aqj9en1zZ8Y3U5AABYinATI6Z3jN4s/WQXi2kCAI5qhJsYccH4AjnjbNpc1sBimgCAoxrhJkakJzpZTBMAABFuYgqLaQIAQLiJKSymCQAA4SamsJgmAACEm5jDYpoAgKMd4SbGDMpI1NRjWEwTAHD0ItzEoOmnsJgmAODoRbiJQSymCQA4msVZXQBCL7CY5p/W7ND/vF2qf+2p14C0eOWlxmtAWrzy0+IV77BbXSYAAGFBuIlR008p1J/W7NDGb+q08Zu6g15PT3QoP9UfdPJT45Wb6v+Zl+pSXqo/CGUlOWWzGRZUDwBA7xFuYtSYgjT9ceZErd9Zo7K6VpXVt6qsrlV761rV0uZVbXObapvbtLms4ZDvEWczlJviUm5qvCYNy9S1px+jjCRnBL8FAAA9Z5hH2SqL9fX1SktLU11dnVJTU60uJ+JM01R9a7vK6/1Bp6yuReX1bpXXt3Y8/M8rG936938ZqfFxuuG7x2rmqUPkiuOyFgAgcnry95twgy61e32qavSovL5V2/c1adHqr/XFXv+CnIWZCfrlf4zSuScOkGFw2QoAEH6Em8Mg3PSO12fqlfW79T/vlKq83i1JOmlwun517vGaMCTT4uoAALGOcHMYhJu+afa064n/3aY//O9WNXv8i3Oee+IA/eI/RmpIVpLF1QEAYhXh5jAIN6FRUd+qB1d8qRc+3SWfKTnshk4/LkepCQ4lu+KU7IpTkitOKfFxSnLuf56R6FRBerzSEhxc0gIAdBvh5jAIN6G1uaxe/9+yL/T3r3o2WWCCwx6cc2dAWoIK0v3PC9ISNCQrUUOzkrgNHQAQRLg5DMJNeHyyvVqlZQ1qcrerMfBobVeTp10Nre3B7VWNHlU3eY74fklOu0YXpGpMQZpOGJimMQWpGpGbLIedSbUB4GhEuDkMwo31Wtu8wTl39ta17P9Z26o9da36urJR7nbfQcc542walZ+iMQVpOiYnSTkpLmUnBx5OZSQy6SAAxKqe/P1mEj9EXLzDrqHZSRqa3XUDcrvXp62VTfp8T502fVOvz/fU6V976tXgbtc/d9fpn7sPnnFZkmyGlJnkDzo5KS4lu+JkMwwZhmQzDNk6fhoHPE9LdGhETrJG5CVrRG6yUuMd4fzqAIAIYOQGUcHnM7WrplmbvqnXpj112lXdrH2NHlU1ulXV6FZNc1tIPicv1aURuck6NjdFx+Qm69jcZA3OTFRGolMJzvBNXGiaZvByXVayq0/v1e71qc1rhrVeAIg0LksdBuEmNrV5fapu8qiywd0ReDxq9rTLNCWfacpn+gNE4LnPNOXzmapq9GhLRaO2VDSqrL71sJ/hirMpI9Gp9ESHMhKdykhyKD3RqYyO39MS/L+nJzqUnuBQWqJDaQmOTrM517e2aXtVk7Yd8Nhe1aSvq5rU0NouSRqZl6KpI7J12rFZmjQsS8muww+w+nymSssb9NHWffpoS5X+sa1aje52FWYmaGReikbmp2hkfqpG5qVoeE4SfUsAohLh5jAINziU+ta2YNAJPL6qaNDe2la1+3r/X5MEh13piQ552n3ad4RmasNQp2Uv4myGThqcrtNG5Oi0Y7M0dlC64myGtu9r1kdbq/TRln1a8/W+bjVpS/5b9o/JSdZxHUEnO9mlrCSnMpOcykp2KivJpbQEB71LAPodws1hEG7QU6ZpqtHdrtrmNtU0e1TT3KbaZo9qmjyqbWkLbq/reO7/6f+9q0yUk+LSsKwkDevoOxrW8RiSlagWj1drvt6nv39VpQ+3VGlndXOnYwNzCP37KFOCw65JwzJ16jFZmjoiWwPS4vVVRaNKyxq0uaxBX5Y3qLSsQY3u9iN+X7vNUEaiQ1lJLmUmOZWd4u9jyk52KSfZpayO54HtrDMGIBIIN4dBuEGk+HymGtztqmtuU22LRzbD0JCsRKX0oGl5575mfbi1Sh98VaUPt1aptqO3yGm36aTB6Tr1mGxNHeEf0XHGHf5yk2ma+qa2RV+W+wNPoG+pusmjfU0e7Wt0q771yOHn36W44pSZ3DH60zEKlBF87h8ZSnDag9MBNLm9anS3qdHtVZPbP01Ag7tdhqRjc1M0akCKRuWnqDAjsUcjSF6fqfL6VjW0tis3xaX0RCaKBGIJ4eYwCDeIVj6fqX/trVdDa7vGF6aHpWHY0+5TTbPngNDj7uhj2t+8XdXoVlWD/7U2b/j+5yPRadfIfH/QGZWfqpH5KcpLjdfeuhZ9U9Oi3TUt+qa2RbtrmvVNbctBlw9dcbbgRJH5qfHKT0sI/p6b4r/8lhLvUGpCHKNPQBQg3BwG4QYIDdM0VdfSFpyYcf/DreqmNlU3ubWvY1tLm9e/JMcBS3Ekuez+5x1LdXjafSotb9DmvQ3aUtEoj/fguY6OxGE3lOSKC45wdZczzqbUeIdS4+OUkuD/meCwq83rk8frU1u7KbfXJ0+7T552b3CbzzSV2rF/WoK/gTw18DPe/zMjyamhWYkanJVIiAL6gHluAISdYRgdd4c5Q/7e7V6ftlU1aXNZQ0ffUL02lzWoqtGtgrQEDcxI0KCMBA3KSNTAdP/zgRkJyk2Jl91myN3uVUW9OzhBZGDSyLK6Vu2tb1Vlx+Wrho4eJE+7Lzgq1VMVDd07xmZIAzMSNDw7WcOykzQ8Z3+/VUFaQrcuwZmmKXe7r+Nyntd/mc+zf0bwFo9Xbq9Pbe2BUOb/6ekIZm1en0xTKkjfv8zJ4KzEqJ7fqbbZo6+rmrS3tlUDMxJ0bG6yko5whyFiHyM3AI5aXp+/WbyhtU0Nre2qb+n42dqmljavHHabXHE2Oew2Oe02OQPP4/zbJf9ddvUt/mPrWtpU3+r/GXhUNbq1var5sM3cNkOKs9lkGDpg0kn/BJSGJJvNkM9nqtnj7dOde4eSkejQkCx/U/uQzEQNykiUM84mm80/4aXdMGSzGbIbhuw2f11xNv95cMb5z43L0fEzLnB+7HLG2WQPwZ137navduxr1teVTfq6qlHbKv3TJ2yrauryTsFBGQk6Li9Fx+Yla2Reio7LS9GI3GTFO/wjZz6fqYbWdlU3+0cWa5o8qu64SaCupU0Ou02JTrsSnXYlOOM6ftqV6LAr0ekfdQycI0RO1F2WeuSRR3T//ferrKxM48aN00MPPaRJkyZ1ue8rr7yie++9V1u2bFFbW5uOPfZY3XLLLbriiiu69VmEGwCRZpqmKhvd2la5f36jwB/nHfuaetW7lOCwKznef/dcksuupI4/wq44uxxxgTBmyGn3B7LANp9pandNi3bsa9LO6mZVNXZvGoHecsUFgkJcMDAEn7viFB9nU2u7T83udjV7vGpu8waft7R51expV2vb4S9R5qfGa0B6vHbXtKjyECNphiEVpCXI3e5VTXObvH0MiXE2/7QKI/P3N8GPyk/VgLT4iDayt3l92rGvWTnJLqUlRu8IXHdEVbhZunSpZs6cqUWLFmny5MlasGCBXnzxRZWWlio3N/eg/VetWqWamhqNGjVKTqdTb775pm655RYtW7ZM06ZNO+LnEW4A9CftXv/8R16fKVP+UQXTlEztn3Ay8L/SgT6lJGdcSEZEJKnR3e4POvuataO6WTv2NWlvXavavaa8vsDEl/7nXtNfX+B3T7tP7o5LX+42fy+Su92ncPxVSXbFBS/lDc9O1rCcJA3vuKx34GWomiaPvixv6Hg0qrTjeVd9WElOuzI67vDLTHIqM9Gp1ASH2rw+tXi8avZ41eRpDz4PhK36lna1tHm7rDMlPk6j8v2TZ2Ynu/xN6/FxnX8m+H+mxMf1aFJNr8/U1spG/XN3nTburtX/7a7Tv/bWy9OxFl9eqqtjws7k4MSdx+btH7E6kKfdp8pGtyrqW1XR4P+5r8mjYdlJmjoiW9m9nCm9tc2rDbtqZUiaPDyrV+9xKFEVbiZPnqxTTjlFDz/8sCTJ5/OpsLBQN9xwg2699dZuvcfJJ5+sc889V/fcc88R9yXcAED4mKap9gOCT3NHOGjy7B+RaTpgW2ubV/EO+8GjOgc8T3LFKaMPt/YHRs527mtWojNOmUn+mcS7+qPf3ffbU9eqzXv9vWD+3rB6fV3Z1OPLhklOe3Bm84xEp9ISHcpIdCg9wb8t0RmnrZWN2ri7Tpv21KnZc3CoSnDYDxm2bIY0JCtJI3KT5W73BcPMkSb+HJWfom8fm62pI7I1aVimEp1d9zHVtbRp3Y5qrd1Wo0+2V2vj7jp5vD6dekyWnrvmWz06F0cSNQ3FHo9H69at09y5c4PbbDabioqKtGbNmiMeb5qm3nvvPZWWluo3v/lNl/u43W653fuHKevr6/teOACgS4ZhyGE35LDblOSSMpNC33Dem5pyU+KVmxIfsvcbmJ6ggekJOvP4vOB2d7tXWyuaVFper6/KG1XT3Bbs52pobVN96/7+rkBIafJ41eTxT2vQHYlOu04oSNPYQWk6cVCaxg5K15DMRDV52v0jVR2Tdm4uq1dpWYNqmtuCl0L/ncNuKCfZpZzUeOV1zA218Zt6fXFAaHvi79vksBs6eXCGvn1stiYPz1JZXas+2V6ttduqVVrecNBIXU6KSwXpCb0/wSFgabipqqqS1+tVXl5ep+15eXnavHnzIY+rq6vTwIED5Xa7Zbfb9eijj+qss87qct+SkhLdddddIa0bAIB/54qza3RBqkYXHPmqQJvXp4bWdtW1dMxw3jHZZ01TW8fM5x7VdoSjwZmJGjsoXWMHpWl4TnKXlyRT4h2aMCRDE4ZkBLcFRqy+LGvU1spGJTjtyk1xKS/VP9dTRqKzy7v0qhrd+mjrPn34VZU+2FKlb2pb9I9t1frHtuouv8vQrESdMjRTpwzL1KShmRqSlWj5BJpReb9cSkqKNmzYoMbGRq1cuVLFxcUaPny4vvOd7xy079y5c1VcXBz8vb6+XoWFhRGsFgCAzhx2W7DXZ5iSwvIZB45YnXZsdrePy0526fxxBTp/XIFM09T2fc36YEuVPvyqSp/uqFFuikuThmX6A83QDOWmhmZELJQsDTfZ2dmy2+0qLy/vtL28vFz5+fmHPM5ms2nEiBGSpPHjx+uLL75QSUlJl+HG5XLJ5epdYxQAAEczwzCC8zFd8a0hVpfTbZbepO90OjVhwgStXLkyuM3n82nlypWaMmVKt9/H5/N16qsBAABHL8svSxUXF2vWrFmaOHGiJk2apAULFqipqUmzZ8+WJM2cOVMDBw5USUmJJH8PzcSJE3XMMcfI7Xbrrbfe0l/+8hc99thjVn4NAADQT1gebmbMmKHKykrNmzdPZWVlGj9+vJYvXx5sMt65c6dstv0DTE1NTbruuuu0e/duJSQkaNSoUXrmmWc0Y8YMq74CAADoRyyf5ybSmOcGAIDo05O/3yyMAQAAYgrhBgAAxBTCDQAAiCmEGwAAEFMINwAAIKYQbgAAQEwh3AAAgJhCuAEAADGFcAMAAGIK4QYAAMQUy9eWirTAahP19fUWVwIAALor8He7O6tGHXXhpqGhQZJUWFhocSUAAKCnGhoalJaWdth9jrqFM30+n/bs2aOUlBQZhhHS966vr1dhYaF27drFopxhxHmODM5zZHCeI4dzHRnhOs+maaqhoUEFBQWy2Q7fVXPUjdzYbDYNGjQorJ+RmprKf3EigPMcGZznyOA8Rw7nOjLCcZ6PNGITQEMxAACIKYQbAAAQUwg3IeRyuTR//ny5XC6rS4lpnOfI4DxHBuc5cjjXkdEfzvNR11AMAABiGyM3AAAgphBuAABATCHcAACAmEK4AQAAMYVwEyKPPPKIhg4dqvj4eE2ePFlr1661uqSo97//+78677zzVFBQIMMw9Nprr3V63TRNzZs3TwMGDFBCQoKKior01VdfWVNslCopKdEpp5yilJQU5ebm6sILL1RpaWmnfVpbWzVnzhxlZWUpOTlZP/zhD1VeXm5RxdHrscce09ixY4MTm02ZMkV/+9vfgq9znkPvvvvuk2EYuummm4LbOM+hceedd8owjE6PUaNGBV+3+jwTbkJg6dKlKi4u1vz587V+/XqNGzdO06ZNU0VFhdWlRbWmpiaNGzdOjzzySJev//a3v9XChQu1aNEi/eMf/1BSUpKmTZum1tbWCFcavVavXq05c+bo448/1ooVK9TW1qbvfe97ampqCu5z8803669//atefPFFrV69Wnv27NF//ud/Wlh1dBo0aJDuu+8+rVu3Tp9++qm++93v6oILLtDnn38uifMcap988on+8Ic/aOzYsZ22c55DZ8yYMdq7d2/w8cEHHwRfs/w8m+izSZMmmXPmzAn+7vV6zYKCArOkpMTCqmKLJPPVV18N/u7z+cz8/Hzz/vvvD26rra01XS6X+fzzz1tQYWyoqKgwJZmrV682TdN/Th0Oh/niiy8G9/niiy9MSeaaNWusKjNmZGRkmH/84x85zyHW0NBgHnvsseaKFSvM008/3bzxxhtN0+TfcyjNnz/fHDduXJev9YfzzMhNH3k8Hq1bt05FRUXBbTabTUVFRVqzZo2FlcW2bdu2qaysrNN5T0tL0+TJkznvfVBXVydJyszMlCStW7dObW1tnc7zqFGjNHjwYM5zH3i9Xi1ZskRNTU2aMmUK5znE5syZo3PPPbfT+ZT49xxqX331lQoKCjR8+HBddtll2rlzp6T+cZ6PuoUzQ62qqkper1d5eXmdtufl5Wnz5s0WVRX7ysrKJKnL8x54DT3j8/l00003aerUqTrhhBMk+c+z0+lUenp6p305z72zceNGTZkyRa2trUpOTtarr76q0aNHa8OGDZznEFmyZInWr1+vTz755KDX+PccOpMnT9bTTz+tkSNHau/evbrrrrv07W9/W5s2beoX55lwA0CS///tbtq0qdN1c4TWyJEjtWHDBtXV1emll17SrFmztHr1aqvLihm7du3SjTfeqBUrVig+Pt7qcmLa2WefHXw+duxYTZ48WUOGDNELL7yghIQECyvz47JUH2VnZ8tutx/UBV5eXq78/HyLqop9gXPLeQ+N66+/Xm+++abef/99DRo0KLg9Pz9fHo9HtbW1nfbnPPeO0+nUiBEjNGHCBJWUlGjcuHH6/e9/z3kOkXXr1qmiokInn3yy4uLiFBcXp9WrV2vhwoWKi4tTXl4e5zlM0tPTddxxx2nLli394t8z4aaPnE6nJkyYoJUrVwa3+Xw+rVy5UlOmTLGwstg2bNgw5efndzrv9fX1+sc//sF57wHTNHX99dfr1Vdf1Xvvvadhw4Z1en3ChAlyOBydznNpaal27tzJeQ4Bn88nt9vNeQ6RM888Uxs3btSGDRuCj4kTJ+qyyy4LPuc8h0djY6O2bt2qAQMG9I9/zxFpW45xS5YsMV0ul/n000+b//rXv8wf//jHZnp6ullWVmZ1aVGtoaHB/Oyzz8zPPvvMlGQ++OCD5meffWbu2LHDNE3TvO+++8z09HTz9ddfN//5z3+aF1xwgTls2DCzpaXF4sqjx7XXXmumpaWZq1atMvfu3Rt8NDc3B/f5yU9+Yg4ePNh87733zE8//dScMmWKOWXKFAurjk633nqruXr1anPbtm3mP//5T/PWW281DcMw33nnHdM0Oc/hcuDdUqbJeQ6VW265xVy1apW5bds288MPPzSLiorM7Oxss6KiwjRN688z4SZEHnroIXPw4MGm0+k0J02aZH788cdWlxT13n//fVPSQY9Zs2aZpum/HfyOO+4w8/LyTJfLZZ555plmaWmptUVHma7OryTzqaeeCu7T0tJiXnfddWZGRoaZmJho/uAHPzD37t1rXdFR6sorrzSHDBliOp1OMycnxzzzzDODwcY0Oc/h8u/hhvMcGjNmzDAHDBhgOp1Oc+DAgeaMGTPMLVu2BF+3+jwbpmmakRkjAgAACD96bgAAQEwh3AAAgJhCuAEAADGFcAMAAGIK4QYAAMQUwg0AAIgphBsAABBTCDcAjnqrVq2SYRgHrYUDIDoRbgAAQEwh3AAAgJhCuAFgOZ/Pp5KSEg0bNkwJCQkaN26cXnrpJUn7LxktW7ZMY8eOVXx8vL71rW9p06ZNnd7j5Zdf1pgxY+RyuTR06FA98MADnV53u9365S9/qcLCQrlcLo0YMUJPPvlkp33WrVuniRMnKjExUaeeeqpKS0vD+8UBhAXhBoDlSkpK9Oc//1mLFi3S559/rptvvlmXX365Vq9eHdzn5z//uR544AF98sknysnJ0Xnnnae2tjZJ/lAyffp0XXLJJdq4caPuvPNO3XHHHXr66aeDx8+cOVPPP/+8Fi5cqC+++EJ/+MMflJyc3KmO22+/XQ888IA+/fRTxcXF6corr4zI9wcQWiycCcBSbrdbmZmZevfddzVlypTg9quvvlrNzc368Y9/rDPOOENLlizRjBkzJEnV1dUaNGiQnn76aU2fPl2XXXaZKisr9c477wSP/8UvfqFly5bp888/15dffqmRI0dqxYoVKioqOqiGVatW6YwzztC7776rM888U5L01ltv6dxzz1VLS4vi4+PDfBYAhBIjNwAstWXLFjU3N+uss85ScnJy8PHnP/9ZW7duDe53YPDJzMzUyJEj9cUXX0iSvvjiC02dOrXT+06dOlVfffWVvF6vNmzYILvdrtNPP/2wtYwdOzb4fMCAAZKkioqKPn9HAJEVZ3UBAI5ujY2NkqRly5Zp4MCBnV5zuVydAk5vJSQkdGs/h8MRfG4YhiR/PxCA6MLIDQBLjR49Wi6XSzt37tSIESM6PQoLC4P7ffzxx8HnNTU1+vLLL3X88cdLko4//nh9+OGHnd73ww8/1HHHHSe73a4TTzxRPp+vUw8PgNjFyA0AS6WkpOhnP/uZbr75Zvl8Pp122mmqq6vThx9+qNTUVA0ZMkSSdPfddysrK0t5eXm6/fbblZ2drQsvvFCSdMstt+iUU07RPffcoxkzZmjNmjV6+OGH9eijj0qShg4dqlmzZunKK6/UwoULNW7cOO3YsUMVFRWaPn26VV8dQJgQbgBY7p577lFOTo5KSkr09ddfKz09XSeffLJuu+224GWh++67TzfeeKO++uorjR8/Xn/961/ldDolSSeffLJeeOEFzZs3T/fcc48GDBigu+++W//1X/8V/IzHHntMt912m6677jrt27dPgwcP1m233WbF1wUQZtwtBaBfC9zJVFNTo/T0dKvLARAF6LkBAAAxhXADAABiCpelAABATGHkBgAAxBTCDQAAiCmEGwAAEFMINwAAIKYQbgAAQEwh3AAAgJhCuAEAADGFcAMAAGIK4QYAAMSU/x/0AUETgPxpOAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_2.history['mae'])\n",
    "plt.title(\"Model MAE\")\n",
    "plt.ylabel(\"mae\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407445c8-203d-4fd0-8088-3e24896b6615",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "06bb54ab-0617-4ff2-aab2-f07a7ae5c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Dense(128,input_shape = (X_train.shape[1],),activation= 'relu'))\n",
    "model_3.add(Dense(64,activation='relu'))\n",
    "model_3.add(Dense(32,activation = 'relu'))\n",
    "model_3.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b8a81957-357d-4f15-af0e-81f444956f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compile the model \n",
    "\n",
    "\n",
    "model_3.compile(loss = tf.keras.losses.MeanAbsoluteError() ,\n",
    "               optimizer= keras.optimizers.Adam(learning_rate=.001) , \n",
    "               metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "74579828-fb3d-4227-b800-18a1ef8b4960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "7/7 [==============================] - 1s 71ms/step - loss: 0.7220 - mae: 0.7220 - val_loss: 0.5503 - val_mae: 0.5503\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.5395 - mae: 0.5395 - val_loss: 0.4456 - val_mae: 0.4456\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.4297 - mae: 0.4297 - val_loss: 0.3709 - val_mae: 0.3709\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3398 - mae: 0.3398 - val_loss: 0.3443 - val_mae: 0.3443\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3516 - mae: 0.3516 - val_loss: 0.3566 - val_mae: 0.3566\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3379 - mae: 0.3379 - val_loss: 0.3477 - val_mae: 0.3477\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3258 - mae: 0.3258 - val_loss: 0.3376 - val_mae: 0.3376\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.3229 - mae: 0.3229 - val_loss: 0.3334 - val_mae: 0.3334\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 0s 49ms/step - loss: 0.3176 - mae: 0.3176 - val_loss: 0.3325 - val_mae: 0.3325\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3133 - mae: 0.3133 - val_loss: 0.3463 - val_mae: 0.3463\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.3206 - mae: 0.3206 - val_loss: 0.3415 - val_mae: 0.3415\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3145 - mae: 0.3145 - val_loss: 0.3323 - val_mae: 0.3323\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3193 - mae: 0.3193 - val_loss: 0.3344 - val_mae: 0.3344\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3113 - mae: 0.3113 - val_loss: 0.3337 - val_mae: 0.3337\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3106 - mae: 0.3106 - val_loss: 0.3350 - val_mae: 0.3350\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3091 - mae: 0.3091 - val_loss: 0.3347 - val_mae: 0.3347\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3069 - mae: 0.3069 - val_loss: 0.3352 - val_mae: 0.3352\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3079 - mae: 0.3079 - val_loss: 0.3373 - val_mae: 0.3373\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3078 - mae: 0.3078 - val_loss: 0.3376 - val_mae: 0.3376\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3050 - mae: 0.3050 - val_loss: 0.3391 - val_mae: 0.3391\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3058 - mae: 0.3058 - val_loss: 0.3395 - val_mae: 0.3395\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3075 - mae: 0.3075 - val_loss: 0.3394 - val_mae: 0.3394\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3078 - mae: 0.3078 - val_loss: 0.3386 - val_mae: 0.3386\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3041 - mae: 0.3041 - val_loss: 0.3399 - val_mae: 0.3399\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3076 - mae: 0.3076 - val_loss: 0.3409 - val_mae: 0.3409\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3043 - mae: 0.3043 - val_loss: 0.3377 - val_mae: 0.3377\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3087 - mae: 0.3087 - val_loss: 0.3425 - val_mae: 0.3425\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3046 - mae: 0.3046 - val_loss: 0.3359 - val_mae: 0.3359\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3065 - mae: 0.3065 - val_loss: 0.3381 - val_mae: 0.3381\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3028 - mae: 0.3028 - val_loss: 0.3380 - val_mae: 0.3380\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3088 - mae: 0.3088 - val_loss: 0.3375 - val_mae: 0.3375\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3039 - mae: 0.3039 - val_loss: 0.3394 - val_mae: 0.3394\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3014 - mae: 0.3014 - val_loss: 0.3472 - val_mae: 0.3472\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3047 - mae: 0.3047 - val_loss: 0.3425 - val_mae: 0.3425\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3039 - mae: 0.3039 - val_loss: 0.3432 - val_mae: 0.3432\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2999 - mae: 0.2999 - val_loss: 0.3480 - val_mae: 0.3480\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3029 - mae: 0.3029 - val_loss: 0.3420 - val_mae: 0.3420\n"
     ]
    }
   ],
   "source": [
    "## fit the model \n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 25)\n",
    "# Create a ModelCheckpoint callback\n",
    "checkpoint_3 = ModelCheckpoint(\"best_model_3.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "train_3 = model_3.fit(X_train,y_train, epochs =1000,verbose =1,validation_data=(X_val,y_val), batch_size= 32,callbacks=[callback,checkpoint_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9c581ee2-68ae-4059-acd1-b0d4686eae52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_3.history['mae'])\n",
    "plt.title(\"Model MAE\")\n",
    "plt.ylabel(\"mae\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e52df9-d2de-4041-a268-d62cfec4f726",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b1be7d48-ea97-4e62-9ff5-c3ecc7713b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the best weights from training \n",
    "model_1.load_weights(\"best_model_1.h5\")\n",
    "model_2.load_weights(\"best_model_2.h5\")\n",
    "model_3.load_weights(\"best_model_3.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "11234398-1175-4a55-8b5b-cccf142388ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step\n",
      "Keras Sequential Model 1 MAE: 0.3544202585426504\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Keras Sequential Model 2 MAE: 0.358362437573057\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "Keras Sequential Model 3 MAE: 0.3711611722152703\n"
     ]
    }
   ],
   "source": [
    "##Model 1 predictions \n",
    "predictions_keras_1_standard= model_1.predict(X_test)\n",
    "predictions_keras_1 = scaler_y.inverse_transform(predictions_keras_1_standard)\n",
    "predictions_keras_1 = np.round(predictions_keras_1,3)\n",
    "\n",
    "keras_model_mae_1 = mae(y_test,predictions_keras_1_standard)\n",
    "print(f\"Keras Sequential Model 1 MAE: {keras_model_mae_1}\")\n",
    "\n",
    "\n",
    "#Model 2 predictions\n",
    "predictions_keras_2_standard = model_2.predict(X_test)\n",
    "predictions_keras_2 = scaler_y.inverse_transform(predictions_keras_2_standard)\n",
    "predictions_keras_2 = np.round(predictions_keras_2,3)\n",
    "\n",
    "keras_model_mae_2 = mae(y_test,predictions_keras_2_standard)\n",
    "print(f\"Keras Sequential Model 2 MAE: {keras_model_mae_2}\")\n",
    "\n",
    "#3Model 3 Predictions \n",
    "predictions_keras_3_standard = model_3.predict(X_test)\n",
    "predictions_keras_3 = scaler_y.inverse_transform(predictions_keras_3_standard)\n",
    "predictions_keras_3 = np.round(predictions_keras_3,3)\n",
    "\n",
    "keras_model_mae_3 = mae(y_test,predictions_keras_3_standard)\n",
    "print(f\"Keras Sequential Model 3 MAE: {keras_model_mae_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b0bb6-7581-41b4-a682-2871d2345546",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e671b07e-fae0-4738-a830-3fdbb84ae862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4900.0           4897.892090\n",
      "1              4750.0           4898.956055\n",
      "2              5750.0           5424.321777\n",
      "3              3700.0           3440.545898\n",
      "4              4300.0           4192.275879\n",
      "..                ...                   ...\n",
      "64             4725.0           5052.609863\n",
      "65             3800.0           3459.864990\n",
      "66             4250.0           3658.979980\n",
      "67             6000.0           5373.580078\n",
      "68             3200.0           3315.195068\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4900.0           4821.904785\n",
      "1              4750.0           4876.089844\n",
      "2              5750.0           5438.666016\n",
      "3              3700.0           3394.862061\n",
      "4              4300.0           4112.057129\n",
      "..                ...                   ...\n",
      "64             4725.0           5077.814941\n",
      "65             3800.0           3458.858887\n",
      "66             4250.0           3713.464111\n",
      "67             6000.0           5361.133789\n",
      "68             3200.0           3363.314941\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4900.0           4900.098145\n",
      "1              4750.0           4934.062012\n",
      "2              5750.0           5422.586914\n",
      "3              3700.0           3361.419922\n",
      "4              4300.0           4076.850098\n",
      "..                ...                   ...\n",
      "64             4725.0           5117.436035\n",
      "65             3800.0           3436.406006\n",
      "66             4250.0           3690.520020\n",
      "67             6000.0           5348.192871\n",
      "68             3200.0           3323.819092\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4900.0           4802.594474\n",
      "1              4750.0           4898.932983\n",
      "2              5750.0           5456.480092\n",
      "3              3700.0           3422.835213\n",
      "4              4300.0           4165.783903\n",
      "..                ...                   ...\n",
      "64             4725.0           5146.369768\n",
      "65             3800.0           3460.379900\n",
      "66             4250.0           3733.899614\n",
      "67             6000.0           5387.535797\n",
      "68             3200.0           3272.078729\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "Keras1 Loss: 0.3544202585426504\n",
      "Keras2 Loss: 0.358362437573057\n",
      "Keras3 Loss: 0.3711611722152703\n",
      "Custom Loss: 0.36488158494635015\n"
     ]
    }
   ],
   "source": [
    "results_keras1 = np.concatenate((test_normal_actual, predictions_keras_1), axis=1)\n",
    "results_keras2 = np.concatenate((test_normal_actual, predictions_keras_2), axis=1)\n",
    "results_keras3 = np.concatenate((test_normal_actual, predictions_keras_3), axis=1)\n",
    "\n",
    "results_keras1_df = pd.DataFrame(results_keras1, columns=['Actual Weight (g)', 'Predicted Weight (g)'])\n",
    "results_keras2_df = pd.DataFrame(results_keras2, columns=['Actual Weight (g)', 'Predicted Weight (g)'])\n",
    "results_keras3_df = pd.DataFrame(results_keras3, columns=['Actual Weight (g)', 'Predicted Weight (g)'])\n",
    "\n",
    "print(results_keras1_df)\n",
    "print(results_keras2_df)\n",
    "print(results_keras3_df)\n",
    "print(results_test_df)\n",
    "\n",
    "print(f\"Keras1 Loss: {keras_model_mae_1}\")\n",
    "print(f\"Keras2 Loss: {keras_model_mae_2}\")\n",
    "print(f\"Keras3 Loss: {keras_model_mae_3}\")\n",
    "print(f\"Custom Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281bb0db-6994-4ff0-a985-2efef13a6654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47469ff6-f318-4388-8105-7dc6a67a3be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
