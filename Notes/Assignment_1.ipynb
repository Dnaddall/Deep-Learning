{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "949dae75-7cc1-4e51-ae83-20e89b9f7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import os, shutil\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6dd64620-f05a-4e11-b576-a0e27e1b6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\david\\\\OneDrive\\\\Inholland\\\\DeepLearning\\\\Assignment 1\\\\palmerpenguins_original.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fb98f-c73f-4f7e-9141-83a88dc26038",
   "metadata": {},
   "source": [
    "# 1 Preprocessing \n",
    "Firstly the data must be analysed and cleaned.\n",
    "Thereafter, to decide what variables to use as predictors we must employ descriptive statistics to analyse what variables are relevant.\n",
    "# 1.1 Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "94e24a9c-97ac-4a1c-b513-69a2dcdea479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts for each column:\n",
      "species               0\n",
      "island                0\n",
      "bill_length_mm        2\n",
      "bill_depth_mm         2\n",
      "flipper_length_mm     2\n",
      "body_mass_g           2\n",
      "sex                  11\n",
      "year                  0\n",
      "dtype: int64\n",
      "Rows with null values:\n",
      "    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "3    Adelie  Torgersen             NaN            NaN                NaN   \n",
      "8    Adelie  Torgersen            34.1           18.1              193.0   \n",
      "9    Adelie  Torgersen            42.0           20.2              190.0   \n",
      "10   Adelie  Torgersen            37.8           17.1              186.0   \n",
      "11   Adelie  Torgersen            37.8           17.3              180.0   \n",
      "47   Adelie      Dream            37.5           18.9              179.0   \n",
      "178  Gentoo     Biscoe            44.5           14.3              216.0   \n",
      "218  Gentoo     Biscoe            46.2           14.4              214.0   \n",
      "256  Gentoo     Biscoe            47.3           13.8              216.0   \n",
      "268  Gentoo     Biscoe            44.5           15.7              217.0   \n",
      "271  Gentoo     Biscoe             NaN            NaN                NaN   \n",
      "\n",
      "     body_mass_g  sex  year  \n",
      "3            NaN  NaN  2007  \n",
      "8         3475.0  NaN  2007  \n",
      "9         4250.0  NaN  2007  \n",
      "10        3300.0  NaN  2007  \n",
      "11        3700.0  NaN  2007  \n",
      "47        2975.0  NaN  2007  \n",
      "178       4100.0  NaN  2007  \n",
      "218       4650.0  NaN  2008  \n",
      "256       4725.0  NaN  2009  \n",
      "268       4875.0  NaN  2009  \n",
      "271          NaN  NaN  2009  \n",
      "Shape of the DataFrame:\n",
      "(344, 8)\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column and sum them up\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Print the count of null values for each column\n",
    "print(\"Null value counts for each column:\")\n",
    "print(null_counts)\n",
    "\n",
    "# Find rows with null values\n",
    "rows_with_null = df[df.isnull().any(axis=1)]\n",
    "\n",
    "# Display rows with null values\n",
    "print(\"Rows with null values:\")\n",
    "print(rows_with_null)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(\"Shape of the DataFrame:\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ee429-b853-46dd-896c-fb18e55ce798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d948793-19cb-49ca-990b-273a54b5aca3",
   "metadata": {},
   "source": [
    "# 1.2 Data Cleaning and encoding\n",
    "Given the results of the null values the decision I decided was to drop the null rows which contain no body_mass as that is the target variable. Furthermore, I decided to add sex to the data based on the mode to compensate for the null values.\n",
    "To do certain statistics on the categorical data the data must be encoded. With Pandas get_dummies uses \"One-Hot encoding\" to encode the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9fdd11e2-b44c-40c8-bbb3-25c310118c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species              0\n",
      "island               0\n",
      "bill_length_mm       0\n",
      "bill_depth_mm        0\n",
      "flipper_length_mm    0\n",
      "body_mass_g          0\n",
      "sex                  0\n",
      "year                 0\n",
      "dtype: int64\n",
      "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0              39.1           18.7              181.0       3750.0   \n",
      "1              39.5           17.4              186.0       3800.0   \n",
      "2              40.3           18.0              195.0       3250.0   \n",
      "4              36.7           19.3              193.0       3450.0   \n",
      "5              39.3           20.6              190.0       3650.0   \n",
      "..              ...            ...                ...          ...   \n",
      "339            55.8           19.8              207.0       4000.0   \n",
      "340            43.5           18.1              202.0       3400.0   \n",
      "341            49.6           18.2              193.0       3775.0   \n",
      "342            50.8           19.0              210.0       4100.0   \n",
      "343            50.2           18.7              198.0       3775.0   \n",
      "\n",
      "     species_Chinstrap  species_Gentoo  island_Dream  island_Torgersen  \\\n",
      "0                    0               0             0                 1   \n",
      "1                    0               0             0                 1   \n",
      "2                    0               0             0                 1   \n",
      "4                    0               0             0                 1   \n",
      "5                    0               0             0                 1   \n",
      "..                 ...             ...           ...               ...   \n",
      "339                  1               0             1                 0   \n",
      "340                  1               0             1                 0   \n",
      "341                  1               0             1                 0   \n",
      "342                  1               0             1                 0   \n",
      "343                  1               0             1                 0   \n",
      "\n",
      "     sex_male  year_2008  year_2009  \n",
      "0           1          0          0  \n",
      "1           0          0          0  \n",
      "2           0          0          0  \n",
      "4           0          0          0  \n",
      "5           1          0          0  \n",
      "..        ...        ...        ...  \n",
      "339         1          0          1  \n",
      "340         0          0          1  \n",
      "341         1          0          1  \n",
      "342         1          0          1  \n",
      "343         0          0          1  \n",
      "\n",
      "[342 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=[\"body_mass_g\"], inplace=True)\n",
    "df[\"sex\"].fillna(df[\"sex\"].mode()[0], inplace=True)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "encoded_values = pd.get_dummies(df, columns=[\"species\", \"island\",\"sex\",\"year\"], drop_first=True)\n",
    "\n",
    "print(encoded_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c1ef7-f218-4789-954d-8db9464abe9f",
   "metadata": {},
   "source": [
    "# 1.3 Data Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "14814fd1-79fb-4a58-ab3f-b043ecd934e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    342.000000\n",
      "mean      43.921930\n",
      "std        5.459584\n",
      "min       32.100000\n",
      "25%       39.225000\n",
      "50%       44.450000\n",
      "75%       48.500000\n",
      "max       59.600000\n",
      "Name: bill_length_mm, dtype: float64 2\n",
      "count    342.000000\n",
      "mean      17.151170\n",
      "std        1.974793\n",
      "min       13.100000\n",
      "25%       15.600000\n",
      "50%       17.300000\n",
      "75%       18.700000\n",
      "max       21.500000\n",
      "Name: bill_depth_mm, dtype: float64 2\n",
      "count    342.000000\n",
      "mean     200.915205\n",
      "std       14.061714\n",
      "min      172.000000\n",
      "25%      190.000000\n",
      "50%      197.000000\n",
      "75%      213.000000\n",
      "max      231.000000\n",
      "Name: flipper_length_mm, dtype: float64 2\n",
      "count     342.000000\n",
      "mean     4201.754386\n",
      "std       801.954536\n",
      "min      2700.000000\n",
      "25%      3550.000000\n",
      "50%      4050.000000\n",
      "75%      4750.000000\n",
      "max      6300.000000\n",
      "Name: body_mass_g, dtype: float64 2\n",
      "Adelie       151\n",
      "Gentoo       123\n",
      "Chinstrap     68\n",
      "Name: species, dtype: int64\n",
      "Biscoe       167\n",
      "Dream        124\n",
      "Torgersen     51\n",
      "Name: island, dtype: int64\n",
      "male      177\n",
      "female    165\n",
      "Name: sex, dtype: int64\n",
      "2009    119\n",
      "2008    114\n",
      "2007    109\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stats_bill_length = df['bill_length_mm'].describe()\n",
    "stats_bill_depth = df['bill_depth_mm'].describe()\n",
    "stats_flipper_length = df['flipper_length_mm'].describe()\n",
    "\n",
    "#target variable \n",
    "stats_body_mass = df['body_mass_g'].describe()\n",
    "\n",
    "print(stats_bill_length,2)\n",
    "print(stats_bill_depth,2)\n",
    "print(stats_flipper_length,2)\n",
    "\n",
    "print(stats_body_mass,2)\n",
    "\n",
    "species_count = df['species'].value_counts()\n",
    "island_count = df['island'].value_counts()\n",
    "sex_count = df['sex'].value_counts()\n",
    "year_count = df['year'].value_counts()\n",
    "\n",
    "print(species_count)\n",
    "print(island_count)\n",
    "print(sex_count)\n",
    "print(year_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "63158220-d4ab-4d13-a8e8-96322da4cff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficients between target variable and predictor variables:\n",
      "bill_length_mm       0.595110\n",
      "bill_depth_mm       -0.471916\n",
      "flipper_length_mm    0.871202\n",
      "body_mass_g          1.000000\n",
      "species_Chinstrap   -0.291561\n",
      "species_Gentoo       0.818198\n",
      "island_Dream        -0.460411\n",
      "island_Torgersen    -0.258979\n",
      "sex_male             0.409315\n",
      "year_2008            0.057319\n",
      "year_2009            0.007790\n",
      "Name: body_mass_g, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlation coefficients between target variable and predictor variables\n",
    "correlation_matrix = encoded_values.corr()\n",
    "\n",
    "# Extract correlation coefficients for the target variable\n",
    "target_correlation = correlation_matrix['body_mass_g']\n",
    "\n",
    "# Print correlation coefficients\n",
    "print(\"Correlation coefficients between target variable and predictor variables:\")\n",
    "print(target_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d791fd-98b5-4f35-8279-2c81f7fbd3b6",
   "metadata": {},
   "source": [
    "Given these results it is concluded that flipper length,bill_length and the gentoo species, has a high positive correlation. Meanwhile, the bill_depth and dream island pinguins give a negative correlation. it is also important to note that the sex has a slight correlation to the weight. Because of this the predictor variable that will be selected is flipper length, bill length and species. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745bb9d-0dbf-44de-82b7-9e9cc30f92f1",
   "metadata": {},
   "source": [
    "# 1.4 Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "09c80e86-0804-48a4-8864-8373978e9b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor variables (X):\n",
      "   bill_length_mm  flipper_length_mm  species_Chinstrap  species_Gentoo\n",
      "0            39.1              181.0                  0               0\n",
      "1            39.5              186.0                  0               0\n",
      "2            40.3              195.0                  0               0\n",
      "4            36.7              193.0                  0               0\n",
      "5            39.3              190.0                  0               0\n",
      "Gentoo type: uint8\n",
      "Chinstrap type: uint8\n",
      "bill_length type: float64\n",
      "flipper_legth: float64\n",
      "\n",
      "Target variable (y):\n",
      "0    3750.0\n",
      "1    3800.0\n",
      "2    3250.0\n",
      "4    3450.0\n",
      "5    3650.0\n",
      "Name: body_mass_g, dtype: float64\n",
      "Training set - Predictor variables: (205, 4)\n",
      "Training set - Target variable: (205,)\n",
      "Testing set - Predictor variables: (69, 4)\n",
      "Testing set - Target variable: (69,)\n",
      "Validation set - Predictor variables: (68, 4)\n",
      "Validation set - Target variable: (68,)\n"
     ]
    }
   ],
   "source": [
    "# Selecting predictor variables ('species', 'bill_length_mm', 'flipper_length_mm') and target variable ('body_mass_g')\n",
    "X = df[['species', 'bill_length_mm', 'flipper_length_mm']]\n",
    "y = df['body_mass_g']\n",
    "\n",
    "# One-hot encode categorical variable 'species'\n",
    "X_encoded = pd.get_dummies(X, columns=['species'], drop_first=True)\n",
    "\n",
    "print(\"Predictor variables (X):\")\n",
    "print(X_encoded.head())\n",
    "print(f\"Gentoo type: {X_encoded['species_Gentoo'].dtype}\\n\"\n",
    "      f\"Chinstrap type: {X_encoded['species_Chinstrap'].dtype}\\n\"\n",
    "      f\"bill_length type: {X_encoded['bill_length_mm'].dtype}\\n\"\n",
    "      f\"flipper_legth: {X_encoded['flipper_length_mm'].dtype}\")\n",
    "\n",
    "print(\"\\nTarget variable (y):\")\n",
    "print(y.head())\n",
    "\n",
    "\n",
    "# Split the dataset into 60% training 20% validation and 20% testing\n",
    "X_train, X_split, y_train, y_split = train_test_split(X_encoded, y, test_size=0.4, random_state=42)\n",
    "X_validation,X_test,y_validation,y_test = train_test_split(X_split,y_split,test_size=.5)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training set - Predictor variables:\", X_train.shape)\n",
    "print(\"Training set - Target variable:\", y_train.shape)\n",
    "print(\"Testing set - Predictor variables:\", X_test.shape)\n",
    "print(\"Testing set - Target variable:\", y_test.shape)\n",
    "print(\"Validation set - Predictor variables:\", X_validation.shape)\n",
    "print(\"Validation set - Target variable:\", y_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5e245-7e90-48cd-89aa-a6811ca0ac89",
   "metadata": {},
   "source": [
    "# 2 A custom neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d8d15-d449-45cd-a9d3-62e6b2acd499",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9f9d3a41-1efe-4233-8b39-7c95ae48d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def dydxrelu(x):\n",
    "    dx = np.where(x > 0, 1, 0)\n",
    "    return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9307b35-f8f7-46d7-a585-b2b951d08a02",
   "metadata": {},
   "source": [
    "# Loss\n",
    "y= true value                                                 \n",
    "y_hat = predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2b2b9bf6-13ab-4a68-97de-641b7ab74864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(y, y_hat):\n",
    "        return np.mean((y - y_hat) ** 2)\n",
    "\n",
    "def square_loss_derivative(y, y_hat):\n",
    "    return 2 * (y_hat - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a0659-5355-4fb7-97d0-f193f563675c",
   "metadata": {},
   "source": [
    "# Network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e1b2d445-a437-4cd3-9d2e-5e2531fb5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Initialization\n",
    "def initialize_weights(layers):\n",
    "    network = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        weights = np.random.uniform(-1, 1, size=(layers[i], layers[i+1]))\n",
    "        biases = np.random.uniform(-1, 1, size=layers[i+1])\n",
    "        layer = {'weights': weights, 'biases': biases}\n",
    "        network.append(layer)\n",
    "    return network\n",
    "\n",
    "# Forward Pass\n",
    "def forward_pass(network, inputs):\n",
    "    activations = [inputs]\n",
    "    weighted_sums = []\n",
    "    for layer in network:\n",
    "        weighted_sum = np.dot(inputs, layer['weights']) + layer['biases']\n",
    "        activation = relu(weighted_sum)\n",
    "        weighted_sums.append(weighted_sum)\n",
    "        activations.append(activation)\n",
    "        inputs = activation\n",
    "    return activations, weighted_sums\n",
    "\n",
    "# Backpropagation\n",
    "def backpropagation(network, activations, weighted_sums, targets, learning_rate):\n",
    "    output_gradient = square_loss_derivative(targets, activations[-1])\n",
    "    for i in range(len(network) - 1, -1, -1):\n",
    "        if i == len(network) - 1:\n",
    "            network[i]['weights'] += learning_rate * np.outer(activations[i], output_gradient)\n",
    "            network[i]['biases'] += learning_rate * output_gradient.reshape(-1,)  # Reshape to match bias shape\n",
    "        else:\n",
    "            hidden_error = np.dot(network[i + 1]['weights'], output_gradient)\n",
    "            hidden_gradient = hidden_error * dydxrelu(weighted_sums[i])\n",
    "            network[i]['weights'] += learning_rate * np.outer(activations[i], hidden_gradient)\n",
    "            network[i]['biases'] += learning_rate * hidden_gradient\n",
    "            output_gradient = hidden_gradient\n",
    "\n",
    "# Training the Model\n",
    "def train_model(network, X_train, y_train, learning_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, target in zip(X_train.values, y_train.values):\n",
    "            inputs = inputs.reshape(1, -1)\n",
    "            target = target.reshape(1, -1)\n",
    "            activations, weighted_sums = forward_pass(network, inputs)\n",
    "            predicted_output = activations[-1]\n",
    "            loss = square_loss(target, predicted_output)\n",
    "            total_loss += loss\n",
    "            backpropagation(network, activations, weighted_sums, target, learning_rate)\n",
    "        avg_loss = total_loss / len(X_train)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss}\")\n",
    "    return network\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(network, X, y):\n",
    "    predictions = []\n",
    "    for inputs, target in zip(X.values, y.values):\n",
    "        inputs = inputs.reshape(1, -1)\n",
    "        activations, _ = forward_pass(network, inputs)\n",
    "        predictions.append(activations[-1])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cd964-e681-43e2-b244-44a54238e837",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ad12e410-c516-432a-9ed7-d940d05ffe5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Loss: 18402981.707317073\n",
      "Epoch 2/100, Average Loss: 18402981.707317073\n",
      "Epoch 3/100, Average Loss: 18402981.707317073\n",
      "Epoch 4/100, Average Loss: 18402981.707317073\n",
      "Epoch 5/100, Average Loss: 18402981.707317073\n",
      "Epoch 6/100, Average Loss: 18402981.707317073\n",
      "Epoch 7/100, Average Loss: 18402981.707317073\n",
      "Epoch 8/100, Average Loss: 18402981.707317073\n",
      "Epoch 9/100, Average Loss: 18402981.707317073\n",
      "Epoch 10/100, Average Loss: 18402981.707317073\n",
      "Epoch 11/100, Average Loss: 18402981.707317073\n",
      "Epoch 12/100, Average Loss: 18402981.707317073\n",
      "Epoch 13/100, Average Loss: 18402981.707317073\n",
      "Epoch 14/100, Average Loss: 18402981.707317073\n",
      "Epoch 15/100, Average Loss: 18402981.707317073\n",
      "Epoch 16/100, Average Loss: 18402981.707317073\n",
      "Epoch 17/100, Average Loss: 18402981.707317073\n",
      "Epoch 18/100, Average Loss: 18402981.707317073\n",
      "Epoch 19/100, Average Loss: 18402981.707317073\n",
      "Epoch 20/100, Average Loss: 18402981.707317073\n",
      "Epoch 21/100, Average Loss: 18402981.707317073\n",
      "Epoch 22/100, Average Loss: 18402981.707317073\n",
      "Epoch 23/100, Average Loss: 18402981.707317073\n",
      "Epoch 24/100, Average Loss: 18402981.707317073\n",
      "Epoch 25/100, Average Loss: 18402981.707317073\n",
      "Epoch 26/100, Average Loss: 18402981.707317073\n",
      "Epoch 27/100, Average Loss: 18402981.707317073\n",
      "Epoch 28/100, Average Loss: 18402981.707317073\n",
      "Epoch 29/100, Average Loss: 18402981.707317073\n",
      "Epoch 30/100, Average Loss: 18402981.707317073\n",
      "Epoch 31/100, Average Loss: 18402981.707317073\n",
      "Epoch 32/100, Average Loss: 18402981.707317073\n",
      "Epoch 33/100, Average Loss: 18402981.707317073\n",
      "Epoch 34/100, Average Loss: 18402981.707317073\n",
      "Epoch 35/100, Average Loss: 18402981.707317073\n",
      "Epoch 36/100, Average Loss: 18402981.707317073\n",
      "Epoch 37/100, Average Loss: 18402981.707317073\n",
      "Epoch 38/100, Average Loss: 18402981.707317073\n",
      "Epoch 39/100, Average Loss: 18402981.707317073\n",
      "Epoch 40/100, Average Loss: 18402981.707317073\n",
      "Epoch 41/100, Average Loss: 18402981.707317073\n",
      "Epoch 42/100, Average Loss: 18402981.707317073\n",
      "Epoch 43/100, Average Loss: 18402981.707317073\n",
      "Epoch 44/100, Average Loss: 18402981.707317073\n",
      "Epoch 45/100, Average Loss: 18402981.707317073\n",
      "Epoch 46/100, Average Loss: 18402981.707317073\n",
      "Epoch 47/100, Average Loss: 18402981.707317073\n",
      "Epoch 48/100, Average Loss: 18402981.707317073\n",
      "Epoch 49/100, Average Loss: 18402981.707317073\n",
      "Epoch 50/100, Average Loss: 18402981.707317073\n",
      "Epoch 51/100, Average Loss: 18402981.707317073\n",
      "Epoch 52/100, Average Loss: 18402981.707317073\n",
      "Epoch 53/100, Average Loss: 18402981.707317073\n",
      "Epoch 54/100, Average Loss: 18402981.707317073\n",
      "Epoch 55/100, Average Loss: 18402981.707317073\n",
      "Epoch 56/100, Average Loss: 18402981.707317073\n",
      "Epoch 57/100, Average Loss: 18402981.707317073\n",
      "Epoch 58/100, Average Loss: 18402981.707317073\n",
      "Epoch 59/100, Average Loss: 18402981.707317073\n",
      "Epoch 60/100, Average Loss: 18402981.707317073\n",
      "Epoch 61/100, Average Loss: 18402981.707317073\n",
      "Epoch 62/100, Average Loss: 18402981.707317073\n",
      "Epoch 63/100, Average Loss: 18402981.707317073\n",
      "Epoch 64/100, Average Loss: 18402981.707317073\n",
      "Epoch 65/100, Average Loss: 18402981.707317073\n",
      "Epoch 66/100, Average Loss: 18402981.707317073\n",
      "Epoch 67/100, Average Loss: 18402981.707317073\n",
      "Epoch 68/100, Average Loss: 18402981.707317073\n",
      "Epoch 69/100, Average Loss: 18402981.707317073\n",
      "Epoch 70/100, Average Loss: 18402981.707317073\n",
      "Epoch 71/100, Average Loss: 18402981.707317073\n",
      "Epoch 72/100, Average Loss: 18402981.707317073\n",
      "Epoch 73/100, Average Loss: 18402981.707317073\n",
      "Epoch 74/100, Average Loss: 18402981.707317073\n",
      "Epoch 75/100, Average Loss: 18402981.707317073\n",
      "Epoch 76/100, Average Loss: 18402981.707317073\n",
      "Epoch 77/100, Average Loss: 18402981.707317073\n",
      "Epoch 78/100, Average Loss: 18402981.707317073\n",
      "Epoch 79/100, Average Loss: 18402981.707317073\n",
      "Epoch 80/100, Average Loss: 18402981.707317073\n",
      "Epoch 81/100, Average Loss: 18402981.707317073\n",
      "Epoch 82/100, Average Loss: 18402981.707317073\n",
      "Epoch 83/100, Average Loss: 18402981.707317073\n",
      "Epoch 84/100, Average Loss: 18402981.707317073\n",
      "Epoch 85/100, Average Loss: 18402981.707317073\n",
      "Epoch 86/100, Average Loss: 18402981.707317073\n",
      "Epoch 87/100, Average Loss: 18402981.707317073\n",
      "Epoch 88/100, Average Loss: 18402981.707317073\n",
      "Epoch 89/100, Average Loss: 18402981.707317073\n",
      "Epoch 90/100, Average Loss: 18402981.707317073\n",
      "Epoch 91/100, Average Loss: 18402981.707317073\n",
      "Epoch 92/100, Average Loss: 18402981.707317073\n",
      "Epoch 93/100, Average Loss: 18402981.707317073\n",
      "Epoch 94/100, Average Loss: 18402981.707317073\n",
      "Epoch 95/100, Average Loss: 18402981.707317073\n",
      "Epoch 96/100, Average Loss: 18402981.707317073\n",
      "Epoch 97/100, Average Loss: 18402981.707317073\n",
      "Epoch 98/100, Average Loss: 18402981.707317073\n",
      "Epoch 99/100, Average Loss: 18402981.707317073\n",
      "Epoch 100/100, Average Loss: 18402981.707317073\n",
      "Validation Set Loss: 18125928.30882353\n",
      "Test Set Loss: 18145715.579710145\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Neural Network\n",
    "network = initialize_weights([X_encoded.shape[1], 1])  # Output layer with 1 neuron\n",
    "\n",
    "# Train the Model\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "network = train_model(network, X_train, y_train, learning_rate, epochs)\n",
    "\n",
    "# Validation Set Evaluation\n",
    "predictions_validation = evaluate(network, X_validation, y_validation)\n",
    "loss_validation = square_loss(y_validation.values.reshape(-1, 1), np.array(predictions_validation).reshape(-1, 1))\n",
    "print(\"Validation Set Loss:\", loss_validation)\n",
    "\n",
    "# Test Set Evaluation\n",
    "predictions_test = evaluate(network, X_test, y_test)\n",
    "loss_test = square_loss(y_test.values.reshape(-1, 1), np.array(predictions_test).reshape(-1, 1))\n",
    "print(\"Test Set Loss:\", loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63385f2d-f422-4424-a3ba-06e56e03a9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
