{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "949dae75-7cc1-4e51-ae83-20e89b9f7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import layers\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import os, shutil\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd64620-f05a-4e11-b576-a0e27e1b6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\david\\\\OneDrive\\\\Inholland\\\\DeepLearning\\\\Assignment 1\\\\palmerpenguins_original.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6fb98f-c73f-4f7e-9141-83a88dc26038",
   "metadata": {},
   "source": [
    "# 1 Preprocessing \n",
    "Firstly the data must be analysed and cleaned.\n",
    "Thereafter, to decide what variables to use as predictors we must employ descriptive statistics to analyse what variables are relevant.\n",
    "# 1.1 Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e24a9c-97ac-4a1c-b513-69a2dcdea479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts for each column:\n",
      "species               0\n",
      "island                0\n",
      "bill_length_mm        2\n",
      "bill_depth_mm         2\n",
      "flipper_length_mm     2\n",
      "body_mass_g           2\n",
      "sex                  11\n",
      "year                  0\n",
      "dtype: int64\n",
      "Rows with null values:\n",
      "    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "3    Adelie  Torgersen             NaN            NaN                NaN   \n",
      "8    Adelie  Torgersen            34.1           18.1              193.0   \n",
      "9    Adelie  Torgersen            42.0           20.2              190.0   \n",
      "10   Adelie  Torgersen            37.8           17.1              186.0   \n",
      "11   Adelie  Torgersen            37.8           17.3              180.0   \n",
      "47   Adelie      Dream            37.5           18.9              179.0   \n",
      "178  Gentoo     Biscoe            44.5           14.3              216.0   \n",
      "218  Gentoo     Biscoe            46.2           14.4              214.0   \n",
      "256  Gentoo     Biscoe            47.3           13.8              216.0   \n",
      "268  Gentoo     Biscoe            44.5           15.7              217.0   \n",
      "271  Gentoo     Biscoe             NaN            NaN                NaN   \n",
      "\n",
      "     body_mass_g  sex  year  \n",
      "3            NaN  NaN  2007  \n",
      "8         3475.0  NaN  2007  \n",
      "9         4250.0  NaN  2007  \n",
      "10        3300.0  NaN  2007  \n",
      "11        3700.0  NaN  2007  \n",
      "47        2975.0  NaN  2007  \n",
      "178       4100.0  NaN  2007  \n",
      "218       4650.0  NaN  2008  \n",
      "256       4725.0  NaN  2009  \n",
      "268       4875.0  NaN  2009  \n",
      "271          NaN  NaN  2009  \n",
      "Shape of the DataFrame:\n",
      "(344, 8)\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in each column and sum them up\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Print the count of null values for each column\n",
    "print(\"Null value counts for each column:\")\n",
    "print(null_counts)\n",
    "\n",
    "# Find rows with null values\n",
    "rows_with_null = df[df.isnull().any(axis=1)]\n",
    "\n",
    "# Display rows with null values\n",
    "print(\"Rows with null values:\")\n",
    "print(rows_with_null)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(\"Shape of the DataFrame:\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d948793-19cb-49ca-990b-273a54b5aca3",
   "metadata": {},
   "source": [
    "# 1.2 Data Cleaning and encoding\n",
    "Given the results of the null values the decision I decided was to drop the null rows which contain no body_mass as that is the target variable. Furthermore, I decided to add sex to the data based on the mode to compensate for the null values.\n",
    "To do certain statistics on the categorical data the data must be encoded. With Pandas get_dummies uses \"One-Hot encoding\" to encode the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fdd11e2-b44c-40c8-bbb3-25c310118c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species              0\n",
      "island               0\n",
      "bill_length_mm       0\n",
      "bill_depth_mm        0\n",
      "flipper_length_mm    0\n",
      "body_mass_g          0\n",
      "sex                  0\n",
      "year                 0\n",
      "dtype: int64\n",
      "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
      "0              39.1           18.7              181.0       3750.0   \n",
      "1              39.5           17.4              186.0       3800.0   \n",
      "2              40.3           18.0              195.0       3250.0   \n",
      "4              36.7           19.3              193.0       3450.0   \n",
      "5              39.3           20.6              190.0       3650.0   \n",
      "..              ...            ...                ...          ...   \n",
      "339            55.8           19.8              207.0       4000.0   \n",
      "340            43.5           18.1              202.0       3400.0   \n",
      "341            49.6           18.2              193.0       3775.0   \n",
      "342            50.8           19.0              210.0       4100.0   \n",
      "343            50.2           18.7              198.0       3775.0   \n",
      "\n",
      "     species_Chinstrap  species_Gentoo  island_Dream  island_Torgersen  \\\n",
      "0                    0               0             0                 1   \n",
      "1                    0               0             0                 1   \n",
      "2                    0               0             0                 1   \n",
      "4                    0               0             0                 1   \n",
      "5                    0               0             0                 1   \n",
      "..                 ...             ...           ...               ...   \n",
      "339                  1               0             1                 0   \n",
      "340                  1               0             1                 0   \n",
      "341                  1               0             1                 0   \n",
      "342                  1               0             1                 0   \n",
      "343                  1               0             1                 0   \n",
      "\n",
      "     sex_male  year_2008  year_2009  \n",
      "0           1          0          0  \n",
      "1           0          0          0  \n",
      "2           0          0          0  \n",
      "4           0          0          0  \n",
      "5           1          0          0  \n",
      "..        ...        ...        ...  \n",
      "339         1          0          1  \n",
      "340         0          0          1  \n",
      "341         1          0          1  \n",
      "342         1          0          1  \n",
      "343         0          0          1  \n",
      "\n",
      "[342 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=[\"body_mass_g\"], inplace=True)\n",
    "df[\"sex\"].fillna(df[\"sex\"].mode()[0], inplace=True)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "encoded_values = pd.get_dummies(df, columns=[\"species\", \"island\",\"sex\",\"year\"], drop_first=True)\n",
    "\n",
    "print(encoded_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c1ef7-f218-4789-954d-8db9464abe9f",
   "metadata": {},
   "source": [
    "# 1.3 Data Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14814fd1-79fb-4a58-ab3f-b043ecd934e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    342.000000\n",
      "mean      43.921930\n",
      "std        5.459584\n",
      "min       32.100000\n",
      "25%       39.225000\n",
      "50%       44.450000\n",
      "75%       48.500000\n",
      "max       59.600000\n",
      "Name: bill_length_mm, dtype: float64 2\n",
      "count    342.000000\n",
      "mean      17.151170\n",
      "std        1.974793\n",
      "min       13.100000\n",
      "25%       15.600000\n",
      "50%       17.300000\n",
      "75%       18.700000\n",
      "max       21.500000\n",
      "Name: bill_depth_mm, dtype: float64 2\n",
      "count    342.000000\n",
      "mean     200.915205\n",
      "std       14.061714\n",
      "min      172.000000\n",
      "25%      190.000000\n",
      "50%      197.000000\n",
      "75%      213.000000\n",
      "max      231.000000\n",
      "Name: flipper_length_mm, dtype: float64 2\n",
      "count     342.000000\n",
      "mean     4201.754386\n",
      "std       801.954536\n",
      "min      2700.000000\n",
      "25%      3550.000000\n",
      "50%      4050.000000\n",
      "75%      4750.000000\n",
      "max      6300.000000\n",
      "Name: body_mass_g, dtype: float64 2\n",
      "Adelie       151\n",
      "Gentoo       123\n",
      "Chinstrap     68\n",
      "Name: species, dtype: int64\n",
      "Biscoe       167\n",
      "Dream        124\n",
      "Torgersen     51\n",
      "Name: island, dtype: int64\n",
      "male      177\n",
      "female    165\n",
      "Name: sex, dtype: int64\n",
      "2009    119\n",
      "2008    114\n",
      "2007    109\n",
      "Name: year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stats_bill_length = df['bill_length_mm'].describe()\n",
    "stats_bill_depth = df['bill_depth_mm'].describe()\n",
    "stats_flipper_length = df['flipper_length_mm'].describe()\n",
    "\n",
    "#target variable \n",
    "stats_body_mass = df['body_mass_g'].describe()\n",
    "\n",
    "print(stats_bill_length,2)\n",
    "print(stats_bill_depth,2)\n",
    "print(stats_flipper_length,2)\n",
    "\n",
    "print(stats_body_mass,2)\n",
    "\n",
    "species_count = df['species'].value_counts()\n",
    "island_count = df['island'].value_counts()\n",
    "sex_count = df['sex'].value_counts()\n",
    "year_count = df['year'].value_counts()\n",
    "\n",
    "print(species_count)\n",
    "print(island_count)\n",
    "print(sex_count)\n",
    "print(year_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63158220-d4ab-4d13-a8e8-96322da4cff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficients between target variable and predictor variables:\n",
      "bill_length_mm       0.595110\n",
      "bill_depth_mm       -0.471916\n",
      "flipper_length_mm    0.871202\n",
      "body_mass_g          1.000000\n",
      "species_Chinstrap   -0.291561\n",
      "species_Gentoo       0.818198\n",
      "island_Dream        -0.460411\n",
      "island_Torgersen    -0.258979\n",
      "sex_male             0.409315\n",
      "year_2008            0.057319\n",
      "year_2009            0.007790\n",
      "Name: body_mass_g, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlation coefficients between target variable and predictor variables\n",
    "correlation_matrix = encoded_values.corr()\n",
    "\n",
    "# Extract correlation coefficients for the target variable\n",
    "target_correlation = correlation_matrix['body_mass_g']\n",
    "\n",
    "# Print correlation coefficients\n",
    "print(\"Correlation coefficients between target variable and predictor variables:\")\n",
    "print(target_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d791fd-98b5-4f35-8279-2c81f7fbd3b6",
   "metadata": {},
   "source": [
    "Given these results it is concluded that flipper length,bill_length and the gentoo species, has a high positive correlation. Meanwhile, the bill_depth and dream island pinguins give a negative correlation. it is also important to note that the sex has a slight correlation to the weight. Because of this the predictor variable that will be selected is flipper length, bill length and species. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745bb9d-0dbf-44de-82b7-9e9cc30f92f1",
   "metadata": {},
   "source": [
    "# 1.4 Splitting Dataset\n",
    "\n",
    "Here the data is split in validation and testing sets to evaluate the model performance during training and after training. The validation set is used to monitor the model's performance during training. The testing set is used to evaluate the final model's performance on unseen data. Furthermore, before splitting the data, Normalization techniques were applied to scale the features to a similar range, usually between 0 and 1 or with a mean of 0 and a standard deviation of 1. Species is not normalized because it already consists of binary values (0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09c80e86-0804-48a4-8864-8373978e9b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor variables (X):\n",
      "   bill_length_mm  flipper_length_mm  species_Chinstrap  species_Gentoo\n",
      "0            39.1              181.0                  0               0\n",
      "1            39.5              186.0                  0               0\n",
      "2            40.3              195.0                  0               0\n",
      "4            36.7              193.0                  0               0\n",
      "5            39.3              190.0                  0               0\n",
      "Gentoo type: uint8\n",
      "Chinstrap type: uint8\n",
      "bill_length type: float64\n",
      "flipper_legth: float64\n",
      "\n",
      "Target variable (y):\n",
      "0    3750.0\n",
      "1    3800.0\n",
      "2    3250.0\n",
      "4    3450.0\n",
      "5    3650.0\n",
      "Name: body_mass_g, dtype: float64\n",
      "(342,)\n",
      "Normalized Predictor variables (X):\n",
      "(342, 4)\n",
      "Normalized target variables (y):\n",
      "(342,)\n",
      "Training set - Predictor variables: (205, 4)\n",
      "Training set - Target variable: (205, 1)\n",
      "Testing set - Predictor variables: (69, 4)\n",
      "Testing set - Target variable: (69, 1)\n",
      "Validation set - Predictor variables: (68, 4)\n",
      "Validation set - Target variable: (68, 1)\n"
     ]
    }
   ],
   "source": [
    "# Selecting predictor variables ('species', 'bill_length_mm', 'flipper_length_mm') and target variable ('body_mass_g')\n",
    "X = df[['species', 'bill_length_mm', 'flipper_length_mm']]\n",
    "y = df['body_mass_g']\n",
    "\n",
    "# One-hot encode categorical variable 'species'\n",
    "X_encoded = pd.get_dummies(X, columns=['species'], drop_first=True)\n",
    "\n",
    "print(\"Predictor variables (X):\")\n",
    "print(X_encoded .head())\n",
    "print(f\"Gentoo type: {X_encoded['species_Gentoo'].dtype}\\n\"\n",
    "      f\"Chinstrap type: {X_encoded['species_Chinstrap'].dtype}\\n\"\n",
    "      f\"bill_length type: {X_encoded['bill_length_mm'].dtype}\\n\"\n",
    "      f\"flipper_legth: {X_encoded['flipper_length_mm'].dtype}\")\n",
    "\n",
    "print(\"\\nTarget variable (y):\")\n",
    "print(y.head())\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical columns of X_encoded\n",
    "numerical_cols = X_encoded.columns.drop(['species_Gentoo', 'species_Chinstrap'])\n",
    "X_encoded[numerical_cols] = scaler.fit_transform(X_encoded[numerical_cols])\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_normalized = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "inputs = np.array(X_encoded)\n",
    "targets = np.array(y_normalized)\n",
    "\n",
    "print(targets.shape)\n",
    "\n",
    "\n",
    "# Print the normalized predictor variables\n",
    "print(\"Normalized Predictor variables (X):\")\n",
    "print(inputs.shape)\n",
    "\n",
    "print(\"Normalized target variables (y):\")\n",
    "print(targets.shape)\n",
    "\n",
    "\n",
    "# Split the dataset into 60% training 20% validation and 20% testing\n",
    "X_train, X_split, y_train, y_split = train_test_split(inputs, targets.reshape(342, 1), test_size=0.4, random_state=42)\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_split,y_split,test_size=.5)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training set - Predictor variables:\", X_train.shape)\n",
    "print(\"Training set - Target variable:\", y_train.shape)\n",
    "print(\"Testing set - Predictor variables:\", X_test.shape)\n",
    "print(\"Testing set - Target variable:\", y_test.shape)\n",
    "print(\"Validation set - Predictor variables:\", X_val.shape)\n",
    "print(\"Validation set - Target variable:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5e245-7e90-48cd-89aa-a6811ca0ac89",
   "metadata": {},
   "source": [
    "# 2 A custom neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d8d15-d449-45cd-a9d3-62e6b2acd499",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9d3a41-1efe-4233-8b39-7c95ae48d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def dydxrelu(x):\n",
    "    dx = np.where(x > 0, 1, 0)\n",
    "    return dx\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dydxsig(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent activation function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (numpy.ndarray): Input values.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Hyperbolic tangent values.\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of the hyperbolic tangent activation function.\n",
    "    \n",
    "    Parameters:\n",
    "    x (numpy.ndarray): Input values.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Derivative values.\n",
    "    \"\"\"\n",
    "    return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9307b35-f8f7-46d7-a585-b2b951d08a02",
   "metadata": {},
   "source": [
    "# Loss\n",
    "y= true value                                                 \n",
    "y_hat = predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b2b9bf6-13ab-4a68-97de-641b7ab74864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, y_hat):\n",
    "        return np.mean((y - y_hat) ** 2)\n",
    "\n",
    "def mse_derivative(y, y_hat):\n",
    "    return 2 * (y_hat - y)\n",
    "\n",
    "def mae(y, y_hat):\n",
    "    return np.mean(np.abs(y - y_hat))\n",
    "\n",
    "def mae_derivative(y, y_hat):\n",
    "    diff = y_hat - y\n",
    "    derivative = np.where(diff > 0, 1, -1)\n",
    "    return derivative / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a0659-5355-4fb7-97d0-f193f563675c",
   "metadata": {},
   "source": [
    "# Network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e1b2d445-a437-4cd3-9d2e-5e2531fb5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "    biases_input_hidden = np.zeros((1, hidden_size))\n",
    "    weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "    biases_hidden_output = np.zeros((1, output_size))\n",
    "    return weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output\n",
    "\n",
    "def forward_pass(X, weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output):\n",
    "    hidden_layer_input = np.dot(X, weights_input_hidden) + biases_input_hidden\n",
    "    hidden_layer_output = relu(hidden_layer_input)\n",
    "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + biases_hidden_output\n",
    "\n",
    "    return output_layer_input, hidden_layer_input, hidden_layer_output\n",
    "\n",
    "def backpropagation(X, output, output_delta, hidden_layer_input, hidden_layer_output, weights_hidden_output, biases_hidden_output, weights_input_hidden, biases_input_hidden, learning_rate):\n",
    "    hidden_error = np.dot(output_delta, weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * dydxrelu(hidden_layer_input)\n",
    "    weights_hidden_output -= learning_rate * np.dot(hidden_layer_output.T, output_delta)\n",
    "    biases_hidden_output -= learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n",
    "    weights_input_hidden -= learning_rate * np.dot(X.T, hidden_delta)\n",
    "    biases_input_hidden -= learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n",
    "\n",
    "def train_model(X_train, y_train, hidden_size=10, learning_rate=0.0001, epochs=1000):\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = y_train.shape[1]\n",
    "    weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output = initialize_weights(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        output_layer_input, hidden_layer_input, hidden_layer_output = forward_pass(X_train, weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = mae(y_train, output_layer_input)\n",
    "    \n",
    "        # Backpropagation\n",
    "        output_error = mae_derivative(y_train, output_layer_input)\n",
    "        backpropagation(X_train, output_layer_input, output_error, hidden_layer_input, hidden_layer_output, weights_hidden_output, biases_hidden_output, weights_input_hidden, biases_input_hidden, learning_rate)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")\n",
    "\n",
    "    return weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cd964-e681-43e2-b244-44a54238e837",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ad12e410-c516-432a-9ed7-d940d05ffe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 2.1929696100093117\n",
      "Epoch 2/1000, Loss: 2.192251566105538\n",
      "Epoch 3/1000, Loss: 2.191533560254283\n",
      "Epoch 4/1000, Loss: 2.1908155924334065\n",
      "Epoch 5/1000, Loss: 2.1900976626207687\n",
      "Epoch 6/1000, Loss: 2.18937977079423\n",
      "Epoch 7/1000, Loss: 2.188661916931653\n",
      "Epoch 8/1000, Loss: 2.187944101010901\n",
      "Epoch 9/1000, Loss: 2.187226323009838\n",
      "Epoch 10/1000, Loss: 2.1865085829063307\n",
      "Epoch 11/1000, Loss: 2.185790880678245\n",
      "Epoch 12/1000, Loss: 2.1850732163034485\n",
      "Epoch 13/1000, Loss: 2.1843555897598113\n",
      "Epoch 14/1000, Loss: 2.1836380010252032\n",
      "Epoch 15/1000, Loss: 2.1829204500774955\n",
      "Epoch 16/1000, Loss: 2.1822029368945612\n",
      "Epoch 17/1000, Loss: 2.1814854614542734\n",
      "Epoch 18/1000, Loss: 2.180768023734506\n",
      "Epoch 19/1000, Loss: 2.180050623713136\n",
      "Epoch 20/1000, Loss: 2.17933326136804\n",
      "Epoch 21/1000, Loss: 2.1786159366770965\n",
      "Epoch 22/1000, Loss: 2.177898649618185\n",
      "Epoch 23/1000, Loss: 2.1771814001691845\n",
      "Epoch 24/1000, Loss: 2.176464188307978\n",
      "Epoch 25/1000, Loss: 2.175747014012447\n",
      "Epoch 26/1000, Loss: 2.175029877260476\n",
      "Epoch 27/1000, Loss: 2.17431277802995\n",
      "Epoch 28/1000, Loss: 2.1735957162987543\n",
      "Epoch 29/1000, Loss: 2.1728786920447765\n",
      "Epoch 30/1000, Loss: 2.1721617052459052\n",
      "Epoch 31/1000, Loss: 2.171444755880029\n",
      "Epoch 32/1000, Loss: 2.170727843925039\n",
      "Epoch 33/1000, Loss: 2.170010969358827\n",
      "Epoch 34/1000, Loss: 2.1692941321592847\n",
      "Epoch 35/1000, Loss: 2.1685773323043067\n",
      "Epoch 36/1000, Loss: 2.1678605697717885\n",
      "Epoch 37/1000, Loss: 2.167143844539625\n",
      "Epoch 38/1000, Loss: 2.1664271565857147\n",
      "Epoch 39/1000, Loss: 2.1657105058879544\n",
      "Epoch 40/1000, Loss: 2.1649938924242456\n",
      "Epoch 41/1000, Loss: 2.1642773161724875\n",
      "Epoch 42/1000, Loss: 2.163560777110581\n",
      "Epoch 43/1000, Loss: 2.1628442752164307\n",
      "Epoch 44/1000, Loss: 2.1621278104679393\n",
      "Epoch 45/1000, Loss: 2.161411382843013\n",
      "Epoch 46/1000, Loss: 2.1606949923195566\n",
      "Epoch 47/1000, Loss: 2.159978638875478\n",
      "Epoch 48/1000, Loss: 2.1592623224886855\n",
      "Epoch 49/1000, Loss: 2.158546043137089\n",
      "Epoch 50/1000, Loss: 2.1578298007985977\n",
      "Epoch 51/1000, Loss: 2.1571135954511247\n",
      "Epoch 52/1000, Loss: 2.1563974270725823\n",
      "Epoch 53/1000, Loss: 2.155681295640884\n",
      "Epoch 54/1000, Loss: 2.1549652011339457\n",
      "Epoch 55/1000, Loss: 2.154249143529683\n",
      "Epoch 56/1000, Loss: 2.1535331228060124\n",
      "Epoch 57/1000, Loss: 2.1528171389408537\n",
      "Epoch 58/1000, Loss: 2.152101191912125\n",
      "Epoch 59/1000, Loss: 2.1513852816977472\n",
      "Epoch 60/1000, Loss: 2.150669408275642\n",
      "Epoch 61/1000, Loss: 2.1499535716237324\n",
      "Epoch 62/1000, Loss: 2.1492377717199416\n",
      "Epoch 63/1000, Loss: 2.1485220085421948\n",
      "Epoch 64/1000, Loss: 2.14780588077693\n",
      "Epoch 65/1000, Loss: 2.1470878586840314\n",
      "Epoch 66/1000, Loss: 2.1463698733425045\n",
      "Epoch 67/1000, Loss: 2.145651924730085\n",
      "Epoch 68/1000, Loss: 2.1449340128245127\n",
      "Epoch 69/1000, Loss: 2.1442161376035274\n",
      "Epoch 70/1000, Loss: 2.1434982990448694\n",
      "Epoch 71/1000, Loss: 2.1427804971262807\n",
      "Epoch 72/1000, Loss: 2.1420627318255043\n",
      "Epoch 73/1000, Loss: 2.141345003120284\n",
      "Epoch 74/1000, Loss: 2.1406273109883647\n",
      "Epoch 75/1000, Loss: 2.1399096554074934\n",
      "Epoch 76/1000, Loss: 2.1391920363554173\n",
      "Epoch 77/1000, Loss: 2.138474453809884\n",
      "Epoch 78/1000, Loss: 2.137756907748644\n",
      "Epoch 79/1000, Loss: 2.137039398149447\n",
      "Epoch 80/1000, Loss: 2.1363219249900456\n",
      "Epoch 81/1000, Loss: 2.135604488248192\n",
      "Epoch 82/1000, Loss: 2.1348870879016406\n",
      "Epoch 83/1000, Loss: 2.134169723928146\n",
      "Epoch 84/1000, Loss: 2.1334523963054646\n",
      "Epoch 85/1000, Loss: 2.1327351050113523\n",
      "Epoch 86/1000, Loss: 2.132017850023569\n",
      "Epoch 87/1000, Loss: 2.1313006313198737\n",
      "Epoch 88/1000, Loss: 2.130583448878027\n",
      "Epoch 89/1000, Loss: 2.1298663026757882\n",
      "Epoch 90/1000, Loss: 2.129149192690923\n",
      "Epoch 91/1000, Loss: 2.128432118901193\n",
      "Epoch 92/1000, Loss: 2.127715081284364\n",
      "Epoch 93/1000, Loss: 2.1269980798182018\n",
      "Epoch 94/1000, Loss: 2.1262811144804727\n",
      "Epoch 95/1000, Loss: 2.125564185248945\n",
      "Epoch 96/1000, Loss: 2.124847292101388\n",
      "Epoch 97/1000, Loss: 2.1241304350155716\n",
      "Epoch 98/1000, Loss: 2.123413613969267\n",
      "Epoch 99/1000, Loss: 2.122696828940247\n",
      "Epoch 100/1000, Loss: 2.1219800799062845\n",
      "Epoch 101/1000, Loss: 2.1212633668451546\n",
      "Epoch 102/1000, Loss: 2.1205466897346317\n",
      "Epoch 103/1000, Loss: 2.1198300485524935\n",
      "Epoch 104/1000, Loss: 2.119113443276517\n",
      "Epoch 105/1000, Loss: 2.1183968738844814\n",
      "Epoch 106/1000, Loss: 2.1176803403541666\n",
      "Epoch 107/1000, Loss: 2.1169638426633535\n",
      "Epoch 108/1000, Loss: 2.1162473807898237\n",
      "Epoch 109/1000, Loss: 2.1155309547113608\n",
      "Epoch 110/1000, Loss: 2.1148145644057488\n",
      "Epoch 111/1000, Loss: 2.114098209850772\n",
      "Epoch 112/1000, Loss: 2.1133818910242184\n",
      "Epoch 113/1000, Loss: 2.1126656079038737\n",
      "Epoch 114/1000, Loss: 2.1119493604675275\n",
      "Epoch 115/1000, Loss: 2.1112331486929685\n",
      "Epoch 116/1000, Loss: 2.1105169725579875\n",
      "Epoch 117/1000, Loss: 2.109800832040376\n",
      "Epoch 118/1000, Loss: 2.109084727117927\n",
      "Epoch 119/1000, Loss: 2.1083686577684335\n",
      "Epoch 120/1000, Loss: 2.1076526239696913\n",
      "Epoch 121/1000, Loss: 2.106934823776366\n",
      "Epoch 122/1000, Loss: 2.10621416559105\n",
      "Epoch 123/1000, Loss: 2.1054935430632353\n",
      "Epoch 124/1000, Loss: 2.1047729561703328\n",
      "Epoch 125/1000, Loss: 2.1040524048897513\n",
      "Epoch 126/1000, Loss: 2.1033318891989023\n",
      "Epoch 127/1000, Loss: 2.10261139407426\n",
      "Epoch 128/1000, Loss: 2.1018908876728783\n",
      "Epoch 129/1000, Loss: 2.1011704167849756\n",
      "Epoch 130/1000, Loss: 2.100449981387967\n",
      "Epoch 131/1000, Loss: 2.099729581459267\n",
      "Epoch 132/1000, Loss: 2.09900921697629\n",
      "Epoch 133/1000, Loss: 2.098288887916453\n",
      "Epoch 134/1000, Loss: 2.097568594257175\n",
      "Epoch 135/1000, Loss: 2.0968483359758734\n",
      "Epoch 136/1000, Loss: 2.0961280964419866\n",
      "Epoch 137/1000, Loss: 2.0954076106624764\n",
      "Epoch 138/1000, Loss: 2.0946871601783004\n",
      "Epoch 139/1000, Loss: 2.0939667449668566\n",
      "Epoch 140/1000, Loss: 2.0932463650055446\n",
      "Epoch 141/1000, Loss: 2.092526020271764\n",
      "Epoch 142/1000, Loss: 2.091805710742917\n",
      "Epoch 143/1000, Loss: 2.0910854363964053\n",
      "Epoch 144/1000, Loss: 2.090365197209632\n",
      "Epoch 145/1000, Loss: 2.089644993160001\n",
      "Epoch 146/1000, Loss: 2.088924824224919\n",
      "Epoch 147/1000, Loss: 2.088204690381792\n",
      "Epoch 148/1000, Loss: 2.0874845916080282\n",
      "Epoch 149/1000, Loss: 2.0867645278810354\n",
      "Epoch 150/1000, Loss: 2.0860444991782234\n",
      "Epoch 151/1000, Loss: 2.085324505477003\n",
      "Epoch 152/1000, Loss: 2.0846045467547856\n",
      "Epoch 153/1000, Loss: 2.083884622988985\n",
      "Epoch 154/1000, Loss: 2.0831647341570143\n",
      "Epoch 155/1000, Loss: 2.082444880236289\n",
      "Epoch 156/1000, Loss: 2.081725061204224\n",
      "Epoch 157/1000, Loss: 2.0810052770382375\n",
      "Epoch 158/1000, Loss: 2.080285527715747\n",
      "Epoch 159/1000, Loss: 2.079565813214172\n",
      "Epoch 160/1000, Loss: 2.078846133510932\n",
      "Epoch 161/1000, Loss: 2.0781264885834485\n",
      "Epoch 162/1000, Loss: 2.077406878409144\n",
      "Epoch 163/1000, Loss: 2.0766873029654414\n",
      "Epoch 164/1000, Loss: 2.0759677622297654\n",
      "Epoch 165/1000, Loss: 2.0752482561795405\n",
      "Epoch 166/1000, Loss: 2.074528784792194\n",
      "Epoch 167/1000, Loss: 2.0738093480451534\n",
      "Epoch 168/1000, Loss: 2.0730899459158465\n",
      "Epoch 169/1000, Loss: 2.072368393978088\n",
      "Epoch 170/1000, Loss: 2.071644649768725\n",
      "Epoch 171/1000, Loss: 2.0709209402634814\n",
      "Epoch 172/1000, Loss: 2.070197265439419\n",
      "Epoch 173/1000, Loss: 2.0694736252736\n",
      "Epoch 174/1000, Loss: 2.068750019743088\n",
      "Epoch 175/1000, Loss: 2.0680264488249485\n",
      "Epoch 176/1000, Loss: 2.067302912496245\n",
      "Epoch 177/1000, Loss: 2.0665794107340463\n",
      "Epoch 178/1000, Loss: 2.0658559435154187\n",
      "Epoch 179/1000, Loss: 2.0651325108174317\n",
      "Epoch 180/1000, Loss: 2.0644091260830364\n",
      "Epoch 181/1000, Loss: 2.0636858292493376\n",
      "Epoch 182/1000, Loss: 2.062962566887127\n",
      "Epoch 183/1000, Loss: 2.062239338973483\n",
      "Epoch 184/1000, Loss: 2.0615161454854825\n",
      "Epoch 185/1000, Loss: 2.060792986400204\n",
      "Epoch 186/1000, Loss: 2.060069861694729\n",
      "Epoch 187/1000, Loss: 2.059346771346137\n",
      "Epoch 188/1000, Loss: 2.0586237153315112\n",
      "Epoch 189/1000, Loss: 2.0579006936279347\n",
      "Epoch 190/1000, Loss: 2.057177706212492\n",
      "Epoch 191/1000, Loss: 2.056454753062268\n",
      "Epoch 192/1000, Loss: 2.0557318341543493\n",
      "Epoch 193/1000, Loss: 2.055008949465823\n",
      "Epoch 194/1000, Loss: 2.054286098973777\n",
      "Epoch 195/1000, Loss: 2.0535632826553014\n",
      "Epoch 196/1000, Loss: 2.0528405004874863\n",
      "Epoch 197/1000, Loss: 2.0521177524474234\n",
      "Epoch 198/1000, Loss: 2.0513950385122053\n",
      "Epoch 199/1000, Loss: 2.050672358658925\n",
      "Epoch 200/1000, Loss: 2.049949712864677\n",
      "Epoch 201/1000, Loss: 2.0492271011065575\n",
      "Epoch 202/1000, Loss: 2.0485045233616623\n",
      "Epoch 203/1000, Loss: 2.0477819796070893\n",
      "Epoch 204/1000, Loss: 2.0470594698199376\n",
      "Epoch 205/1000, Loss: 2.0463369939773064\n",
      "Epoch 206/1000, Loss: 2.0456145520562963\n",
      "Epoch 207/1000, Loss: 2.044892144034009\n",
      "Epoch 208/1000, Loss: 2.044169769887547\n",
      "Epoch 209/1000, Loss: 2.043447429594015\n",
      "Epoch 210/1000, Loss: 2.042725123130517\n",
      "Epoch 211/1000, Loss: 2.042002879742525\n",
      "Epoch 212/1000, Loss: 2.041289722979534\n",
      "Epoch 213/1000, Loss: 2.040576599712436\n",
      "Epoch 214/1000, Loss: 2.0398635099187747\n",
      "Epoch 215/1000, Loss: 2.039150453576097\n",
      "Epoch 216/1000, Loss: 2.0384374306619493\n",
      "Epoch 217/1000, Loss: 2.0377244411538786\n",
      "Epoch 218/1000, Loss: 2.037011485029434\n",
      "Epoch 219/1000, Loss: 2.036298562266166\n",
      "Epoch 220/1000, Loss: 2.0355856728416244\n",
      "Epoch 221/1000, Loss: 2.0348728167333614\n",
      "Epoch 222/1000, Loss: 2.0341599939189288\n",
      "Epoch 223/1000, Loss: 2.0334472043758804\n",
      "Epoch 224/1000, Loss: 2.032734448081772\n",
      "Epoch 225/1000, Loss: 2.0320217250141583\n",
      "Epoch 226/1000, Loss: 2.031309035150596\n",
      "Epoch 227/1000, Loss: 2.0305963784686423\n",
      "Epoch 228/1000, Loss: 2.029883754945857\n",
      "Epoch 229/1000, Loss: 2.0291711645597985\n",
      "Epoch 230/1000, Loss: 2.028458607288028\n",
      "Epoch 231/1000, Loss: 2.0277460831081062\n",
      "Epoch 232/1000, Loss: 2.0270335919975966\n",
      "Epoch 233/1000, Loss: 2.026321133934062\n",
      "Epoch 234/1000, Loss: 2.0256085165891853\n",
      "Epoch 235/1000, Loss: 2.024893686135239\n",
      "Epoch 236/1000, Loss: 2.024178888738507\n",
      "Epoch 237/1000, Loss: 2.0234641243763525\n",
      "Epoch 238/1000, Loss: 2.0227493930261384\n",
      "Epoch 239/1000, Loss: 2.022034694665229\n",
      "Epoch 240/1000, Loss: 2.02132002927099\n",
      "Epoch 241/1000, Loss: 2.0206053968207875\n",
      "Epoch 242/1000, Loss: 2.019890797291989\n",
      "Epoch 243/1000, Loss: 2.0191762306619627\n",
      "Epoch 244/1000, Loss: 2.0184616969080778\n",
      "Epoch 245/1000, Loss: 2.017747196007705\n",
      "Epoch 246/1000, Loss: 2.0170327279382154\n",
      "Epoch 247/1000, Loss: 2.016318292676981\n",
      "Epoch 248/1000, Loss: 2.015603890201375\n",
      "Epoch 249/1000, Loss: 2.0148895204887722\n",
      "Epoch 250/1000, Loss: 2.0141751835165467\n",
      "Epoch 251/1000, Loss: 2.013460879262076\n",
      "Epoch 252/1000, Loss: 2.0127466077027356\n",
      "Epoch 253/1000, Loss: 2.0120323688159054\n",
      "Epoch 254/1000, Loss: 2.0113181625789633\n",
      "Epoch 255/1000, Loss: 2.0106039889692893\n",
      "Epoch 256/1000, Loss: 2.0098898479642644\n",
      "Epoch 257/1000, Loss: 2.0091757395412704\n",
      "Epoch 258/1000, Loss: 2.0084616304424627\n",
      "Epoch 259/1000, Loss: 2.007746811086316\n",
      "Epoch 260/1000, Loss: 2.007032024268253\n",
      "Epoch 261/1000, Loss: 2.006317269965593\n",
      "Epoch 262/1000, Loss: 2.0056025481556587\n",
      "Epoch 263/1000, Loss: 2.0048878588157715\n",
      "Epoch 264/1000, Loss: 2.0041732019232548\n",
      "Epoch 265/1000, Loss: 2.0034585774554325\n",
      "Epoch 266/1000, Loss: 2.00274398538963\n",
      "Epoch 267/1000, Loss: 2.002029425703173\n",
      "Epoch 268/1000, Loss: 2.0013148983733893\n",
      "Epoch 269/1000, Loss: 2.0006004033776064\n",
      "Epoch 270/1000, Loss: 1.9998859406931533\n",
      "Epoch 271/1000, Loss: 1.9991715102973597\n",
      "Epoch 272/1000, Loss: 1.9984571121675572\n",
      "Epoch 273/1000, Loss: 1.9977427462810766\n",
      "Epoch 274/1000, Loss: 1.9970284126152518\n",
      "Epoch 275/1000, Loss: 1.9963141111474156\n",
      "Epoch 276/1000, Loss: 1.995599841854903\n",
      "Epoch 277/1000, Loss: 1.99488560471505\n",
      "Epoch 278/1000, Loss: 1.9941713997051929\n",
      "Epoch 279/1000, Loss: 1.9934572268026691\n",
      "Epoch 280/1000, Loss: 1.9927430859848172\n",
      "Epoch 281/1000, Loss: 1.9920289772289774\n",
      "Epoch 282/1000, Loss: 1.9913149005124893\n",
      "Epoch 283/1000, Loss: 1.9906008558126946\n",
      "Epoch 284/1000, Loss: 1.9898868431069354\n",
      "Epoch 285/1000, Loss: 1.989172862372555\n",
      "Epoch 286/1000, Loss: 1.9884589135868973\n",
      "Epoch 287/1000, Loss: 1.987744996727309\n",
      "Epoch 288/1000, Loss: 1.9870311117711337\n",
      "Epoch 289/1000, Loss: 1.9863171257975931\n",
      "Epoch 290/1000, Loss: 1.985599542906939\n",
      "Epoch 291/1000, Loss: 1.9848819919627099\n",
      "Epoch 292/1000, Loss: 1.9841644729419348\n",
      "Epoch 293/1000, Loss: 1.9834469858216446\n",
      "Epoch 294/1000, Loss: 1.9827295305788728\n",
      "Epoch 295/1000, Loss: 1.9820121071906518\n",
      "Epoch 296/1000, Loss: 1.9812947156340162\n",
      "Epoch 297/1000, Loss: 1.9805773558859998\n",
      "Epoch 298/1000, Loss: 1.9798600279236402\n",
      "Epoch 299/1000, Loss: 1.9791427317239725\n",
      "Epoch 300/1000, Loss: 1.9784254672640362\n",
      "Epoch 301/1000, Loss: 1.9777082345208694\n",
      "Epoch 302/1000, Loss: 1.976991033471511\n",
      "Epoch 303/1000, Loss: 1.9762738640930033\n",
      "Epoch 304/1000, Loss: 1.975556726362387\n",
      "Epoch 305/1000, Loss: 1.9748396202567042\n",
      "Epoch 306/1000, Loss: 1.9741225457529996\n",
      "Epoch 307/1000, Loss: 1.9734055028283164\n",
      "Epoch 308/1000, Loss: 1.9726884914597007\n",
      "Epoch 309/1000, Loss: 1.9719715116241987\n",
      "Epoch 310/1000, Loss: 1.9712545632988578\n",
      "Epoch 311/1000, Loss: 1.9705376464607258\n",
      "Epoch 312/1000, Loss: 1.969820761086852\n",
      "Epoch 313/1000, Loss: 1.969103072759664\n",
      "Epoch 314/1000, Loss: 1.968382844744084\n",
      "Epoch 315/1000, Loss: 1.9676626482208772\n",
      "Epoch 316/1000, Loss: 1.9669424831668043\n",
      "Epoch 317/1000, Loss: 1.9662223495586275\n",
      "Epoch 318/1000, Loss: 1.9655022473731085\n",
      "Epoch 319/1000, Loss: 1.964782182825523\n",
      "Epoch 320/1000, Loss: 1.964062167524478\n",
      "Epoch 321/1000, Loss: 1.9633421835795084\n",
      "Epoch 322/1000, Loss: 1.9626222309673804\n",
      "Epoch 323/1000, Loss: 1.961902309664863\n",
      "Epoch 324/1000, Loss: 1.9611824196487238\n",
      "Epoch 325/1000, Loss: 1.9604625608957333\n",
      "Epoch 326/1000, Loss: 1.959742733382662\n",
      "Epoch 327/1000, Loss: 1.959022937086281\n",
      "Epoch 328/1000, Loss: 1.9583031719833637\n",
      "Epoch 329/1000, Loss: 1.9575834380506831\n",
      "Epoch 330/1000, Loss: 1.9568637352650136\n",
      "Epoch 331/1000, Loss: 1.9561440636031309\n",
      "Epoch 332/1000, Loss: 1.955423533232636\n",
      "Epoch 333/1000, Loss: 1.9546978519181855\n",
      "Epoch 334/1000, Loss: 1.9539722018258123\n",
      "Epoch 335/1000, Loss: 1.9532465829317682\n",
      "Epoch 336/1000, Loss: 1.952520995212304\n",
      "Epoch 337/1000, Loss: 1.9517954386436733\n",
      "Epoch 338/1000, Loss: 1.9510699132021292\n",
      "Epoch 339/1000, Loss: 1.9503444188639267\n",
      "Epoch 340/1000, Loss: 1.9496189556053218\n",
      "Epoch 341/1000, Loss: 1.9488935234025706\n",
      "Epoch 342/1000, Loss: 1.9481681222319311\n",
      "Epoch 343/1000, Loss: 1.9474427520696613\n",
      "Epoch 344/1000, Loss: 1.946717412892021\n",
      "Epoch 345/1000, Loss: 1.9459921046752702\n",
      "Epoch 346/1000, Loss: 1.9452668273956706\n",
      "Epoch 347/1000, Loss: 1.944541581029484\n",
      "Epoch 348/1000, Loss: 1.9438163655529728\n",
      "Epoch 349/1000, Loss: 1.9430911809424025\n",
      "Epoch 350/1000, Loss: 1.9423660271740373\n",
      "Epoch 351/1000, Loss: 1.9416409042241431\n",
      "Epoch 352/1000, Loss: 1.9409158120689862\n",
      "Epoch 353/1000, Loss: 1.9401907506848353\n",
      "Epoch 354/1000, Loss: 1.9394657200479581\n",
      "Epoch 355/1000, Loss: 1.938740720134625\n",
      "Epoch 356/1000, Loss: 1.9380157509211065\n",
      "Epoch 357/1000, Loss: 1.937290812383673\n",
      "Epoch 358/1000, Loss: 1.9365659044985977\n",
      "Epoch 359/1000, Loss: 1.9358410272421536\n",
      "Epoch 360/1000, Loss: 1.9351161805906143\n",
      "Epoch 361/1000, Loss: 1.9343913645202557\n",
      "Epoch 362/1000, Loss: 1.933666579007354\n",
      "Epoch 363/1000, Loss: 1.9329418240281855\n",
      "Epoch 364/1000, Loss: 1.932217099559028\n",
      "Epoch 365/1000, Loss: 1.93149240557616\n",
      "Epoch 366/1000, Loss: 1.930767742055862\n",
      "Epoch 367/1000, Loss: 1.9300431089744141\n",
      "Epoch 368/1000, Loss: 1.929318506308097\n",
      "Epoch 369/1000, Loss: 1.9285939340331946\n",
      "Epoch 370/1000, Loss: 1.9278693921259884\n",
      "Epoch 371/1000, Loss: 1.9271448805627642\n",
      "Epoch 372/1000, Loss: 1.9264203993198064\n",
      "Epoch 373/1000, Loss: 1.9256959483734006\n",
      "Epoch 374/1000, Loss: 1.9249715276998345\n",
      "Epoch 375/1000, Loss: 1.9242471372753953\n",
      "Epoch 376/1000, Loss: 1.9235227770763716\n",
      "Epoch 377/1000, Loss: 1.9227984470790536\n",
      "Epoch 378/1000, Loss: 1.9220741472597316\n",
      "Epoch 379/1000, Loss: 1.9213498775946969\n",
      "Epoch 380/1000, Loss: 1.9206252110497462\n",
      "Epoch 381/1000, Loss: 1.9198983175050635\n",
      "Epoch 382/1000, Loss: 1.9191714541121783\n",
      "Epoch 383/1000, Loss: 1.9184446208471486\n",
      "Epoch 384/1000, Loss: 1.9177178176860352\n",
      "Epoch 385/1000, Loss: 1.9169910446048974\n",
      "Epoch 386/1000, Loss: 1.9162643015797978\n",
      "Epoch 387/1000, Loss: 1.9155375885867978\n",
      "Epoch 388/1000, Loss: 1.9148109056019613\n",
      "Epoch 389/1000, Loss: 1.9140842526013524\n",
      "Epoch 390/1000, Loss: 1.913357629561036\n",
      "Epoch 391/1000, Loss: 1.912631036457078\n",
      "Epoch 392/1000, Loss: 1.9119044732655452\n",
      "Epoch 393/1000, Loss: 1.911177939962506\n",
      "Epoch 394/1000, Loss: 1.9104514365240282\n",
      "Epoch 395/1000, Loss: 1.9097249629261819\n",
      "Epoch 396/1000, Loss: 1.9089985191450374\n",
      "Epoch 397/1000, Loss: 1.9082721051566662\n",
      "Epoch 398/1000, Loss: 1.9075457209371403\n",
      "Epoch 399/1000, Loss: 1.9068193664625324\n",
      "Epoch 400/1000, Loss: 1.9060930417089172\n",
      "Epoch 401/1000, Loss: 1.9053667466523692\n",
      "Epoch 402/1000, Loss: 1.904640481268964\n",
      "Epoch 403/1000, Loss: 1.9039142455347784\n",
      "Epoch 404/1000, Loss: 1.90318803942589\n",
      "Epoch 405/1000, Loss: 1.9024618629183776\n",
      "Epoch 406/1000, Loss: 1.90173571598832\n",
      "Epoch 407/1000, Loss: 1.9010095986117972\n",
      "Epoch 408/1000, Loss: 1.9002835107648908\n",
      "Epoch 409/1000, Loss: 1.8995574524236822\n",
      "Epoch 410/1000, Loss: 1.8988314235642547\n",
      "Epoch 411/1000, Loss: 1.8981054241626913\n",
      "Epoch 412/1000, Loss: 1.897379454195077\n",
      "Epoch 413/1000, Loss: 1.8966535136374976\n",
      "Epoch 414/1000, Loss: 1.8959276024660394\n",
      "Epoch 415/1000, Loss: 1.8952017206567884\n",
      "Epoch 416/1000, Loss: 1.8944758681858342\n",
      "Epoch 417/1000, Loss: 1.8937500450292646\n",
      "Epoch 418/1000, Loss: 1.8930242511631699\n",
      "Epoch 419/1000, Loss: 1.8922984865636407\n",
      "Epoch 420/1000, Loss: 1.8915727512067688\n",
      "Epoch 421/1000, Loss: 1.8908470450686465\n",
      "Epoch 422/1000, Loss: 1.8901213681253666\n",
      "Epoch 423/1000, Loss: 1.8893957203530236\n",
      "Epoch 424/1000, Loss: 1.8886701017277123\n",
      "Epoch 425/1000, Loss: 1.8879445122255292\n",
      "Epoch 426/1000, Loss: 1.8872189518225702\n",
      "Epoch 427/1000, Loss: 1.886493420494934\n",
      "Epoch 428/1000, Loss: 1.8857679182187181\n",
      "Epoch 429/1000, Loss: 1.8850424449700218\n",
      "Epoch 430/1000, Loss: 1.8843170007249461\n",
      "Epoch 431/1000, Loss: 1.8835915854595913\n",
      "Epoch 432/1000, Loss: 1.8828661991500597\n",
      "Epoch 433/1000, Loss: 1.8821408417724539\n",
      "Epoch 434/1000, Loss: 1.8814155133028778\n",
      "Epoch 435/1000, Loss: 1.8806902137174355\n",
      "Epoch 436/1000, Loss: 1.8799649429922327\n",
      "Epoch 437/1000, Loss: 1.8792397011033755\n",
      "Epoch 438/1000, Loss: 1.8785144880269702\n",
      "Epoch 439/1000, Loss: 1.8777893037391256\n",
      "Epoch 440/1000, Loss: 1.87706414821595\n",
      "Epoch 441/1000, Loss: 1.8763390214335536\n",
      "Epoch 442/1000, Loss: 1.875613923368046\n",
      "Epoch 443/1000, Loss: 1.8748888539955393\n",
      "Epoch 444/1000, Loss: 1.8741638132921452\n",
      "Epoch 445/1000, Loss: 1.8734388012339764\n",
      "Epoch 446/1000, Loss: 1.8727138177971474\n",
      "Epoch 447/1000, Loss: 1.8719888629577723\n",
      "Epoch 448/1000, Loss: 1.8712639366919674\n",
      "Epoch 449/1000, Loss: 1.870539038975848\n",
      "Epoch 450/1000, Loss: 1.8698141697855317\n",
      "Epoch 451/1000, Loss: 1.8690893290971375\n",
      "Epoch 452/1000, Loss: 1.868364516886783\n",
      "Epoch 453/1000, Loss: 1.867639733130589\n",
      "Epoch 454/1000, Loss: 1.866914977804675\n",
      "Epoch 455/1000, Loss: 1.8661902508851627\n",
      "Epoch 456/1000, Loss: 1.8654655523481751\n",
      "Epoch 457/1000, Loss: 1.864740882169835\n",
      "Epoch 458/1000, Loss: 1.864016240326266\n",
      "Epoch 459/1000, Loss: 1.863291626793593\n",
      "Epoch 460/1000, Loss: 1.8625670415479412\n",
      "Epoch 461/1000, Loss: 1.8618424845654378\n",
      "Epoch 462/1000, Loss: 1.8611179558222095\n",
      "Epoch 463/1000, Loss: 1.8603934552943844\n",
      "Epoch 464/1000, Loss: 1.8596689829580921\n",
      "Epoch 465/1000, Loss: 1.8589445387894612\n",
      "Epoch 466/1000, Loss: 1.8582201227646236\n",
      "Epoch 467/1000, Loss: 1.8574958150541117\n",
      "Epoch 468/1000, Loss: 1.8567717157299728\n",
      "Epoch 469/1000, Loss: 1.8560476443678997\n",
      "Epoch 470/1000, Loss: 1.8553236009440504\n",
      "Epoch 471/1000, Loss: 1.8545995854345838\n",
      "Epoch 472/1000, Loss: 1.8538755978156602\n",
      "Epoch 473/1000, Loss: 1.8531516380634405\n",
      "Epoch 474/1000, Loss: 1.852427706154086\n",
      "Epoch 475/1000, Loss: 1.8517038020637584\n",
      "Epoch 476/1000, Loss: 1.8509799257686215\n",
      "Epoch 477/1000, Loss: 1.8502545822896799\n",
      "Epoch 478/1000, Loss: 1.8495261236881821\n",
      "Epoch 479/1000, Loss: 1.8487976929091474\n",
      "Epoch 480/1000, Loss: 1.8480692899283295\n",
      "Epoch 481/1000, Loss: 1.847340914721484\n",
      "Epoch 482/1000, Loss: 1.8466125672643667\n",
      "Epoch 483/1000, Loss: 1.845884247532734\n",
      "Epoch 484/1000, Loss: 1.8451559555023445\n",
      "Epoch 485/1000, Loss: 1.8444276911489557\n",
      "Epoch 486/1000, Loss: 1.843699454448327\n",
      "Epoch 487/1000, Loss: 1.842971245376218\n",
      "Epoch 488/1000, Loss: 1.8422430639083909\n",
      "Epoch 489/1000, Loss: 1.841514910020606\n",
      "Epoch 490/1000, Loss: 1.8407867836886267\n",
      "Epoch 491/1000, Loss: 1.8400586836482606\n",
      "Epoch 492/1000, Loss: 1.8393305935906727\n",
      "Epoch 493/1000, Loss: 1.8386025310307448\n",
      "Epoch 494/1000, Loss: 1.8378744959442395\n",
      "Epoch 495/1000, Loss: 1.8371464883069202\n",
      "Epoch 496/1000, Loss: 1.8364185080945499\n",
      "Epoch 497/1000, Loss: 1.8356905552828937\n",
      "Epoch 498/1000, Loss: 1.8349626298477166\n",
      "Epoch 499/1000, Loss: 1.8342347317647858\n",
      "Epoch 500/1000, Loss: 1.833506861009867\n",
      "Epoch 501/1000, Loss: 1.8327790175587297\n",
      "Epoch 502/1000, Loss: 1.8320512013871413\n",
      "Epoch 503/1000, Loss: 1.8313234124708715\n",
      "Epoch 504/1000, Loss: 1.830595650785691\n",
      "Epoch 505/1000, Loss: 1.8298679163073701\n",
      "Epoch 506/1000, Loss: 1.8291402090116813\n",
      "Epoch 507/1000, Loss: 1.8284125288743964\n",
      "Epoch 508/1000, Loss: 1.8276848758712898\n",
      "Epoch 509/1000, Loss: 1.826957249978135\n",
      "Epoch 510/1000, Loss: 1.826229651170708\n",
      "Epoch 511/1000, Loss: 1.825502079424784\n",
      "Epoch 512/1000, Loss: 1.8247745347161388\n",
      "Epoch 513/1000, Loss: 1.8240470335768115\n",
      "Epoch 514/1000, Loss: 1.8233195798768753\n",
      "Epoch 515/1000, Loss: 1.8225921531258342\n",
      "Epoch 516/1000, Loss: 1.8218647532994714\n",
      "Epoch 517/1000, Loss: 1.8211373803735695\n",
      "Epoch 518/1000, Loss: 1.820410034323912\n",
      "Epoch 519/1000, Loss: 1.819682715126284\n",
      "Epoch 520/1000, Loss: 1.8189554227564702\n",
      "Epoch 521/1000, Loss: 1.8182281571902583\n",
      "Epoch 522/1000, Loss: 1.8175009184034334\n",
      "Epoch 523/1000, Loss: 1.816773706371785\n",
      "Epoch 524/1000, Loss: 1.816046521071101\n",
      "Epoch 525/1000, Loss: 1.81531936247717\n",
      "Epoch 526/1000, Loss: 1.8145922305657833\n",
      "Epoch 527/1000, Loss: 1.8138651253127314\n",
      "Epoch 528/1000, Loss: 1.8131380466938058\n",
      "Epoch 529/1000, Loss: 1.8124109946847988\n",
      "Epoch 530/1000, Loss: 1.8116839692615039\n",
      "Epoch 531/1000, Loss: 1.810956970399715\n",
      "Epoch 532/1000, Loss: 1.8102299980752272\n",
      "Epoch 533/1000, Loss: 1.8095030522638358\n",
      "Epoch 534/1000, Loss: 1.808776132941337\n",
      "Epoch 535/1000, Loss: 1.808049240083527\n",
      "Epoch 536/1000, Loss: 1.8073223736662054\n",
      "Epoch 537/1000, Loss: 1.8065955336651702\n",
      "Epoch 538/1000, Loss: 1.80586872005622\n",
      "Epoch 539/1000, Loss: 1.8051419328151563\n",
      "Epoch 540/1000, Loss: 1.8044151719177781\n",
      "Epoch 541/1000, Loss: 1.803688437339889\n",
      "Epoch 542/1000, Loss: 1.8029617290572904\n",
      "Epoch 543/1000, Loss: 1.8022350470457862\n",
      "Epoch 544/1000, Loss: 1.8015083912811793\n",
      "Epoch 545/1000, Loss: 1.8007817617392752\n",
      "Epoch 546/1000, Loss: 1.8000551583958795\n",
      "Epoch 547/1000, Loss: 1.799328581226798\n",
      "Epoch 548/1000, Loss: 1.7986020302078376\n",
      "Epoch 549/1000, Loss: 1.7978755053148066\n",
      "Epoch 550/1000, Loss: 1.7971490065235125\n",
      "Epoch 551/1000, Loss: 1.796422533809766\n",
      "Epoch 552/1000, Loss: 1.7956960871493761\n",
      "Epoch 553/1000, Loss: 1.794969666518154\n",
      "Epoch 554/1000, Loss: 1.7942432718919108\n",
      "Epoch 555/1000, Loss: 1.7935169032464595\n",
      "Epoch 556/1000, Loss: 1.7927905605576127\n",
      "Epoch 557/1000, Loss: 1.7920642438011831\n",
      "Epoch 558/1000, Loss: 1.7913379529529874\n",
      "Epoch 559/1000, Loss: 1.7906116879888392\n",
      "Epoch 560/1000, Loss: 1.789885448884555\n",
      "Epoch 561/1000, Loss: 1.7891592356159518\n",
      "Epoch 562/1000, Loss: 1.7884330481588466\n",
      "Epoch 563/1000, Loss: 1.787706886489058\n",
      "Epoch 564/1000, Loss: 1.7869807505824051\n",
      "Epoch 565/1000, Loss: 1.7862546404147075\n",
      "Epoch 566/1000, Loss: 1.7855285559617857\n",
      "Epoch 567/1000, Loss: 1.784802497199461\n",
      "Epoch 568/1000, Loss: 1.784076464103555\n",
      "Epoch 569/1000, Loss: 1.7833504566498903\n",
      "Epoch 570/1000, Loss: 1.7826244748142908\n",
      "Epoch 571/1000, Loss: 1.7818985185725804\n",
      "Epoch 572/1000, Loss: 1.781172587900584\n",
      "Epoch 573/1000, Loss: 1.7804466827741277\n",
      "Epoch 574/1000, Loss: 1.7797208031690377\n",
      "Epoch 575/1000, Loss: 1.7789949490611403\n",
      "Epoch 576/1000, Loss: 1.7782691204262642\n",
      "Epoch 577/1000, Loss: 1.7775433172402373\n",
      "Epoch 578/1000, Loss: 1.7768175394788897\n",
      "Epoch 579/1000, Loss: 1.7760917871180508\n",
      "Epoch 580/1000, Loss: 1.775366060133552\n",
      "Epoch 581/1000, Loss: 1.774640358501224\n",
      "Epoch 582/1000, Loss: 1.7739146821968994\n",
      "Epoch 583/1000, Loss: 1.7731890311964114\n",
      "Epoch 584/1000, Loss: 1.7724634054755928\n",
      "Epoch 585/1000, Loss: 1.7717378050102786\n",
      "Epoch 586/1000, Loss: 1.771012229776304\n",
      "Epoch 587/1000, Loss: 1.7702866797495045\n",
      "Epoch 588/1000, Loss: 1.769561154905717\n",
      "Epoch 589/1000, Loss: 1.7688356552207785\n",
      "Epoch 590/1000, Loss: 1.7681101806705268\n",
      "Epoch 591/1000, Loss: 1.7673847312308006\n",
      "Epoch 592/1000, Loss: 1.76665930687744\n",
      "Epoch 593/1000, Loss: 1.7659339075862848\n",
      "Epoch 594/1000, Loss: 1.7652115103561157\n",
      "Epoch 595/1000, Loss: 1.7644947910493585\n",
      "Epoch 596/1000, Loss: 1.7637780962949736\n",
      "Epoch 597/1000, Loss: 1.763061426069449\n",
      "Epoch 598/1000, Loss: 1.7623447803492718\n",
      "Epoch 599/1000, Loss: 1.7616281591109317\n",
      "Epoch 600/1000, Loss: 1.760911562330918\n",
      "Epoch 601/1000, Loss: 1.7601949899857219\n",
      "Epoch 602/1000, Loss: 1.7594784420518332\n",
      "Epoch 603/1000, Loss: 1.7587619185057448\n",
      "Epoch 604/1000, Loss: 1.758045419323948\n",
      "Epoch 605/1000, Loss: 1.7573289444829365\n",
      "Epoch 606/1000, Loss: 1.7566124939592036\n",
      "Epoch 607/1000, Loss: 1.755896067729244\n",
      "Epoch 608/1000, Loss: 1.7551796657695526\n",
      "Epoch 609/1000, Loss: 1.7544632880566258\n",
      "Epoch 610/1000, Loss: 1.7537469345669594\n",
      "Epoch 611/1000, Loss: 1.7530306052770512\n",
      "Epoch 612/1000, Loss: 1.752314300163398\n",
      "Epoch 613/1000, Loss: 1.7515980192024996\n",
      "Epoch 614/1000, Loss: 1.7508817623708537\n",
      "Epoch 615/1000, Loss: 1.7501655296449612\n",
      "Epoch 616/1000, Loss: 1.7494493210013227\n",
      "Epoch 617/1000, Loss: 1.7487331364164393\n",
      "Epoch 618/1000, Loss: 1.748016975866812\n",
      "Epoch 619/1000, Loss: 1.7473008393289449\n",
      "Epoch 620/1000, Loss: 1.7465847267793397\n",
      "Epoch 621/1000, Loss: 1.7458686381945014\n",
      "Epoch 622/1000, Loss: 1.745152573550934\n",
      "Epoch 623/1000, Loss: 1.744436532825143\n",
      "Epoch 624/1000, Loss: 1.743721757096125\n",
      "Epoch 625/1000, Loss: 1.743008282369991\n",
      "Epoch 626/1000, Loss: 1.7422948308809645\n",
      "Epoch 627/1000, Loss: 1.7415814026056953\n",
      "Epoch 628/1000, Loss: 1.7408679975208314\n",
      "Epoch 629/1000, Loss: 1.7401546156030219\n",
      "Epoch 630/1000, Loss: 1.739441256828918\n",
      "Epoch 631/1000, Loss: 1.7387279211751696\n",
      "Epoch 632/1000, Loss: 1.7380146086184285\n",
      "Epoch 633/1000, Loss: 1.7373013191353466\n",
      "Epoch 634/1000, Loss: 1.736588052702576\n",
      "Epoch 635/1000, Loss: 1.735874809296771\n",
      "Epoch 636/1000, Loss: 1.7351615888945855\n",
      "Epoch 637/1000, Loss: 1.7344483914726736\n",
      "Epoch 638/1000, Loss: 1.7337352170076905\n",
      "Epoch 639/1000, Loss: 1.733022065476292\n",
      "Epoch 640/1000, Loss: 1.7323089368551348\n",
      "Epoch 641/1000, Loss: 1.7315958311208761\n",
      "Epoch 642/1000, Loss: 1.7308827482501723\n",
      "Epoch 643/1000, Loss: 1.7301696882196838\n",
      "Epoch 644/1000, Loss: 1.7294566510060683\n",
      "Epoch 645/1000, Loss: 1.7287436365859863\n",
      "Epoch 646/1000, Loss: 1.7280306449360967\n",
      "Epoch 647/1000, Loss: 1.7273176760330615\n",
      "Epoch 648/1000, Loss: 1.7266047298535419\n",
      "Epoch 649/1000, Loss: 1.7258918063741995\n",
      "Epoch 650/1000, Loss: 1.725178905571697\n",
      "Epoch 651/1000, Loss: 1.7244660274226988\n",
      "Epoch 652/1000, Loss: 1.7237531719038677\n",
      "Epoch 653/1000, Loss: 1.7230403389918687\n",
      "Epoch 654/1000, Loss: 1.7223275286633668\n",
      "Epoch 655/1000, Loss: 1.7216147408950278\n",
      "Epoch 656/1000, Loss: 1.7209019756635189\n",
      "Epoch 657/1000, Loss: 1.720189232945506\n",
      "Epoch 658/1000, Loss: 1.7194765127176574\n",
      "Epoch 659/1000, Loss: 1.7187638149566415\n",
      "Epoch 660/1000, Loss: 1.718051139639127\n",
      "Epoch 661/1000, Loss: 1.7173384867417825\n",
      "Epoch 662/1000, Loss: 1.7166258562412795\n",
      "Epoch 663/1000, Loss: 1.7159132481142871\n",
      "Epoch 664/1000, Loss: 1.7152006623374787\n",
      "Epoch 665/1000, Loss: 1.7144880988875246\n",
      "Epoch 666/1000, Loss: 1.7137755577410976\n",
      "Epoch 667/1000, Loss: 1.7130630388748709\n",
      "Epoch 668/1000, Loss: 1.7123505422655187\n",
      "Epoch 669/1000, Loss: 1.7116380678897147\n",
      "Epoch 670/1000, Loss: 1.710925615724134\n",
      "Epoch 671/1000, Loss: 1.7102128061376376\n",
      "Epoch 672/1000, Loss: 1.7094989311840392\n",
      "Epoch 673/1000, Loss: 1.7087850783906735\n",
      "Epoch 674/1000, Loss: 1.7080712477340878\n",
      "Epoch 675/1000, Loss: 1.7073574391908302\n",
      "Epoch 676/1000, Loss: 1.7066436527374502\n",
      "Epoch 677/1000, Loss: 1.705929888350497\n",
      "Epoch 678/1000, Loss: 1.7052161460065212\n",
      "Epoch 679/1000, Loss: 1.7045024256820724\n",
      "Epoch 680/1000, Loss: 1.7037887273537031\n",
      "Epoch 681/1000, Loss: 1.7030750509979642\n",
      "Epoch 682/1000, Loss: 1.702361396591409\n",
      "Epoch 683/1000, Loss: 1.7016477641105898\n",
      "Epoch 684/1000, Loss: 1.7009341535320606\n",
      "Epoch 685/1000, Loss: 1.7002205648323763\n",
      "Epoch 686/1000, Loss: 1.6995069979880904\n",
      "Epoch 687/1000, Loss: 1.6987934529757598\n",
      "Epoch 688/1000, Loss: 1.6980799297719396\n",
      "Epoch 689/1000, Loss: 1.6973664283531864\n",
      "Epoch 690/1000, Loss: 1.6966529486960575\n",
      "Epoch 691/1000, Loss: 1.6959394907771108\n",
      "Epoch 692/1000, Loss: 1.6952260545729052\n",
      "Epoch 693/1000, Loss: 1.6945126400599988\n",
      "Epoch 694/1000, Loss: 1.6937992472149517\n",
      "Epoch 695/1000, Loss: 1.6930858760143237\n",
      "Epoch 696/1000, Loss: 1.6923725264346758\n",
      "Epoch 697/1000, Loss: 1.6916591984525688\n",
      "Epoch 698/1000, Loss: 1.6909458920445652\n",
      "Epoch 699/1000, Loss: 1.6902326152972555\n",
      "Epoch 700/1000, Loss: 1.6895194113417342\n",
      "Epoch 701/1000, Loss: 1.6888062289070984\n",
      "Epoch 702/1000, Loss: 1.6880930679699155\n",
      "Epoch 703/1000, Loss: 1.6873799285067534\n",
      "Epoch 704/1000, Loss: 1.686666810494182\n",
      "Epoch 705/1000, Loss: 1.6859537139087695\n",
      "Epoch 706/1000, Loss: 1.685240638727087\n",
      "Epoch 707/1000, Loss: 1.684527584925705\n",
      "Epoch 708/1000, Loss: 1.683814552481194\n",
      "Epoch 709/1000, Loss: 1.6831015413701262\n",
      "Epoch 710/1000, Loss: 1.682388551569074\n",
      "Epoch 711/1000, Loss: 1.6816755830546097\n",
      "Epoch 712/1000, Loss: 1.680962635803307\n",
      "Epoch 713/1000, Loss: 1.68024970979174\n",
      "Epoch 714/1000, Loss: 1.6795368049964838\n",
      "Epoch 715/1000, Loss: 1.6788239213941127\n",
      "Epoch 716/1000, Loss: 1.6781110589612032\n",
      "Epoch 717/1000, Loss: 1.6773982176743307\n",
      "Epoch 718/1000, Loss: 1.676685397510073\n",
      "Epoch 719/1000, Loss: 1.6759725984450065\n",
      "Epoch 720/1000, Loss: 1.67525982045571\n",
      "Epoch 721/1000, Loss: 1.6745470635187616\n",
      "Epoch 722/1000, Loss: 1.6738343276107404\n",
      "Epoch 723/1000, Loss: 1.6731216127082267\n",
      "Epoch 724/1000, Loss: 1.6724089187878\n",
      "Epoch 725/1000, Loss: 1.6716962458260407\n",
      "Epoch 726/1000, Loss: 1.670983593799531\n",
      "Epoch 727/1000, Loss: 1.6702709626848524\n",
      "Epoch 728/1000, Loss: 1.669558352458587\n",
      "Epoch 729/1000, Loss: 1.668845763097319\n",
      "Epoch 730/1000, Loss: 1.66813319457763\n",
      "Epoch 731/1000, Loss: 1.6674206468761057\n",
      "Epoch 732/1000, Loss: 1.6667081199693303\n",
      "Epoch 733/1000, Loss: 1.6659956138338887\n",
      "Epoch 734/1000, Loss: 1.6652831284463674\n",
      "Epoch 735/1000, Loss: 1.6645706637833517\n",
      "Epoch 736/1000, Loss: 1.6638582198214287\n",
      "Epoch 737/1000, Loss: 1.6631457965371859\n",
      "Epoch 738/1000, Loss: 1.6624333939072113\n",
      "Epoch 739/1000, Loss: 1.6617210119080936\n",
      "Epoch 740/1000, Loss: 1.6610086505164214\n",
      "Epoch 741/1000, Loss: 1.660296309708785\n",
      "Epoch 742/1000, Loss: 1.6595839894617734\n",
      "Epoch 743/1000, Loss: 1.6588716897519784\n",
      "Epoch 744/1000, Loss: 1.6581594105559898\n",
      "Epoch 745/1000, Loss: 1.6574471518504015\n",
      "Epoch 746/1000, Loss: 1.6567349136118033\n",
      "Epoch 747/1000, Loss: 1.6560226958167892\n",
      "Epoch 748/1000, Loss: 1.6553104984419533\n",
      "Epoch 749/1000, Loss: 1.6545983214638877\n",
      "Epoch 750/1000, Loss: 1.6538861648591885\n",
      "Epoch 751/1000, Loss: 1.65317402860445\n",
      "Epoch 752/1000, Loss: 1.6524619126762672\n",
      "Epoch 753/1000, Loss: 1.651749817051237\n",
      "Epoch 754/1000, Loss: 1.6510377417059552\n",
      "Epoch 755/1000, Loss: 1.6503256866170193\n",
      "Epoch 756/1000, Loss: 1.6496136517610271\n",
      "Epoch 757/1000, Loss: 1.6489016371145764\n",
      "Epoch 758/1000, Loss: 1.6481896426542657\n",
      "Epoch 759/1000, Loss: 1.6474776683566945\n",
      "Epoch 760/1000, Loss: 1.646768623668798\n",
      "Epoch 761/1000, Loss: 1.6460817009068813\n",
      "Epoch 762/1000, Loss: 1.6453947971682679\n",
      "Epoch 763/1000, Loss: 1.644707912431259\n",
      "Epoch 764/1000, Loss: 1.6440210466741583\n",
      "Epoch 765/1000, Loss: 1.6433341998752684\n",
      "Epoch 766/1000, Loss: 1.6426473720128931\n",
      "Epoch 767/1000, Loss: 1.6419605630653367\n",
      "Epoch 768/1000, Loss: 1.641273773010904\n",
      "Epoch 769/1000, Loss: 1.6405870018278994\n",
      "Epoch 770/1000, Loss: 1.639900249494629\n",
      "Epoch 771/1000, Loss: 1.6392135159893995\n",
      "Epoch 772/1000, Loss: 1.6385268012905154\n",
      "Epoch 773/1000, Loss: 1.6378401053762857\n",
      "Epoch 774/1000, Loss: 1.6371534282250169\n",
      "Epoch 775/1000, Loss: 1.6364667698150175\n",
      "Epoch 776/1000, Loss: 1.6357801301245956\n",
      "Epoch 777/1000, Loss: 1.6350935091320602\n",
      "Epoch 778/1000, Loss: 1.63440690681572\n",
      "Epoch 779/1000, Loss: 1.633720323153886\n",
      "Epoch 780/1000, Loss: 1.6330337581248675\n",
      "Epoch 781/1000, Loss: 1.6323472117069764\n",
      "Epoch 782/1000, Loss: 1.6316606838785224\n",
      "Epoch 783/1000, Loss: 1.6309741746178186\n",
      "Epoch 784/1000, Loss: 1.6302876839031764\n",
      "Epoch 785/1000, Loss: 1.6296012117129086\n",
      "Epoch 786/1000, Loss: 1.6289147580253278\n",
      "Epoch 787/1000, Loss: 1.6282283228187489\n",
      "Epoch 788/1000, Loss: 1.6275419060714844\n",
      "Epoch 789/1000, Loss: 1.6268555077618503\n",
      "Epoch 790/1000, Loss: 1.6261691278681598\n",
      "Epoch 791/1000, Loss: 1.6254827663687297\n",
      "Epoch 792/1000, Loss: 1.6247964232418752\n",
      "Epoch 793/1000, Loss: 1.6241100984659131\n",
      "Epoch 794/1000, Loss: 1.6234237920191594\n",
      "Epoch 795/1000, Loss: 1.6227375038799323\n",
      "Epoch 796/1000, Loss: 1.6220512340265487\n",
      "Epoch 797/1000, Loss: 1.6213649824373266\n",
      "Epoch 798/1000, Loss: 1.6206787490905852\n",
      "Epoch 799/1000, Loss: 1.6199925339646437\n",
      "Epoch 800/1000, Loss: 1.619306337037821\n",
      "Epoch 801/1000, Loss: 1.6186201582884379\n",
      "Epoch 802/1000, Loss: 1.6179339976948135\n",
      "Epoch 803/1000, Loss: 1.6172478552352694\n",
      "Epoch 804/1000, Loss: 1.6165617308881277\n",
      "Epoch 805/1000, Loss: 1.6158756246317088\n",
      "Epoch 806/1000, Loss: 1.6151895364443352\n",
      "Epoch 807/1000, Loss: 1.6145034663043298\n",
      "Epoch 808/1000, Loss: 1.613817414190016\n",
      "Epoch 809/1000, Loss: 1.6131313800797171\n",
      "Epoch 810/1000, Loss: 1.6124453639517569\n",
      "Epoch 811/1000, Loss: 1.61175936578446\n",
      "Epoch 812/1000, Loss: 1.6110733855561514\n",
      "Epoch 813/1000, Loss: 1.610387423245156\n",
      "Epoch 814/1000, Loss: 1.6097014788297996\n",
      "Epoch 815/1000, Loss: 1.609015552288409\n",
      "Epoch 816/1000, Loss: 1.6083296435993106\n",
      "Epoch 817/1000, Loss: 1.6076437527408312\n",
      "Epoch 818/1000, Loss: 1.6069578796912987\n",
      "Epoch 819/1000, Loss: 1.6062843738636112\n",
      "Epoch 820/1000, Loss: 1.6056241189320872\n",
      "Epoch 821/1000, Loss: 1.6049638820139152\n",
      "Epoch 822/1000, Loss: 1.6043036630890428\n",
      "Epoch 823/1000, Loss: 1.6036434621374172\n",
      "Epoch 824/1000, Loss: 1.6029832791389869\n",
      "Epoch 825/1000, Loss: 1.602323114073701\n",
      "Epoch 826/1000, Loss: 1.6016629669215077\n",
      "Epoch 827/1000, Loss: 1.6010028376623566\n",
      "Epoch 828/1000, Loss: 1.6003427262761982\n",
      "Epoch 829/1000, Loss: 1.5996826327429823\n",
      "Epoch 830/1000, Loss: 1.5990225570426595\n",
      "Epoch 831/1000, Loss: 1.5983624991551817\n",
      "Epoch 832/1000, Loss: 1.5977024590605002\n",
      "Epoch 833/1000, Loss: 1.597042436738567\n",
      "Epoch 834/1000, Loss: 1.5963824321693345\n",
      "Epoch 835/1000, Loss: 1.5957224453327556\n",
      "Epoch 836/1000, Loss: 1.595062476208784\n",
      "Epoch 837/1000, Loss: 1.594402524777373\n",
      "Epoch 838/1000, Loss: 1.5937425910184773\n",
      "Epoch 839/1000, Loss: 1.5930826749120504\n",
      "Epoch 840/1000, Loss: 1.5924227764380487\n",
      "Epoch 841/1000, Loss: 1.5917628955764263\n",
      "Epoch 842/1000, Loss: 1.5911030323071407\n",
      "Epoch 843/1000, Loss: 1.5904431866101463\n",
      "Epoch 844/1000, Loss: 1.589783358465401\n",
      "Epoch 845/1000, Loss: 1.589123547852862\n",
      "Epoch 846/1000, Loss: 1.5884637547524862\n",
      "Epoch 847/1000, Loss: 1.5878039791442318\n",
      "Epoch 848/1000, Loss: 1.5871442210080575\n",
      "Epoch 849/1000, Loss: 1.586484480323921\n",
      "Epoch 850/1000, Loss: 1.5858247570717834\n",
      "Epoch 851/1000, Loss: 1.585165051231602\n",
      "Epoch 852/1000, Loss: 1.584505362783339\n",
      "Epoch 853/1000, Loss: 1.5838456917069532\n",
      "Epoch 854/1000, Loss: 1.5831860379824063\n",
      "Epoch 855/1000, Loss: 1.5825264015896598\n",
      "Epoch 856/1000, Loss: 1.5818667825086743\n",
      "Epoch 857/1000, Loss: 1.5812071807194132\n",
      "Epoch 858/1000, Loss: 1.580547596201838\n",
      "Epoch 859/1000, Loss: 1.579888028935912\n",
      "Epoch 860/1000, Loss: 1.5792284789015982\n",
      "Epoch 861/1000, Loss: 1.5785689460788606\n",
      "Epoch 862/1000, Loss: 1.5779094304476635\n",
      "Epoch 863/1000, Loss: 1.577249931987971\n",
      "Epoch 864/1000, Loss: 1.5765904506797486\n",
      "Epoch 865/1000, Loss: 1.5759309865029607\n",
      "Epoch 866/1000, Loss: 1.5752715394375745\n",
      "Epoch 867/1000, Loss: 1.5746121094635546\n",
      "Epoch 868/1000, Loss: 1.5739526965608686\n",
      "Epoch 869/1000, Loss: 1.573293300709483\n",
      "Epoch 870/1000, Loss: 1.572633921889365\n",
      "Epoch 871/1000, Loss: 1.571974560080483\n",
      "Epoch 872/1000, Loss: 1.571315215262805\n",
      "Epoch 873/1000, Loss: 1.5706558874162986\n",
      "Epoch 874/1000, Loss: 1.5699965765209338\n",
      "Epoch 875/1000, Loss: 1.5693372825566798\n",
      "Epoch 876/1000, Loss: 1.568678005503506\n",
      "Epoch 877/1000, Loss: 1.5680187453413825\n",
      "Epoch 878/1000, Loss: 1.56735950205028\n",
      "Epoch 879/1000, Loss: 1.56670027561017\n",
      "Epoch 880/1000, Loss: 1.5660410660010224\n",
      "Epoch 881/1000, Loss: 1.5653818732028106\n",
      "Epoch 882/1000, Loss: 1.5647226971955057\n",
      "Epoch 883/1000, Loss: 1.5640635379590804\n",
      "Epoch 884/1000, Loss: 1.563404395473508\n",
      "Epoch 885/1000, Loss: 1.562745269718761\n",
      "Epoch 886/1000, Loss: 1.5620861606748135\n",
      "Epoch 887/1000, Loss: 1.5614270683216396\n",
      "Epoch 888/1000, Loss: 1.5607679926392137\n",
      "Epoch 889/1000, Loss: 1.560105860194864\n",
      "Epoch 890/1000, Loss: 1.5594395219974126\n",
      "Epoch 891/1000, Loss: 1.5587732004571202\n",
      "Epoch 892/1000, Loss: 1.5581068955533615\n",
      "Epoch 893/1000, Loss: 1.5574406072655098\n",
      "Epoch 894/1000, Loss: 1.5567743355729395\n",
      "Epoch 895/1000, Loss: 1.556108080455026\n",
      "Epoch 896/1000, Loss: 1.5554418418911435\n",
      "Epoch 897/1000, Loss: 1.5547756198606688\n",
      "Epoch 898/1000, Loss: 1.5541094143429766\n",
      "Epoch 899/1000, Loss: 1.5534432253174433\n",
      "Epoch 900/1000, Loss: 1.5527774435629171\n",
      "Epoch 901/1000, Loss: 1.5521222388871558\n",
      "Epoch 902/1000, Loss: 1.5514670502756511\n",
      "Epoch 903/1000, Loss: 1.5508118777084963\n",
      "Epoch 904/1000, Loss: 1.5501567211657867\n",
      "Epoch 905/1000, Loss: 1.5495015806276176\n",
      "Epoch 906/1000, Loss: 1.5488464560740833\n",
      "Epoch 907/1000, Loss: 1.5481913474852804\n",
      "Epoch 908/1000, Loss: 1.5475362548413047\n",
      "Epoch 909/1000, Loss: 1.5468811781222531\n",
      "Epoch 910/1000, Loss: 1.5462261173082221\n",
      "Epoch 911/1000, Loss: 1.545571072379309\n",
      "Epoch 912/1000, Loss: 1.544916043315611\n",
      "Epoch 913/1000, Loss: 1.5442610300972275\n",
      "Epoch 914/1000, Loss: 1.5436060327042547\n",
      "Epoch 915/1000, Loss: 1.5429510511167925\n",
      "Epoch 916/1000, Loss: 1.54229608531494\n",
      "Epoch 917/1000, Loss: 1.5416411352787958\n",
      "Epoch 918/1000, Loss: 1.5409862009884607\n",
      "Epoch 919/1000, Loss: 1.5403312824240343\n",
      "Epoch 920/1000, Loss: 1.5396763795656168\n",
      "Epoch 921/1000, Loss: 1.5390214923933094\n",
      "Epoch 922/1000, Loss: 1.5383666208872133\n",
      "Epoch 923/1000, Loss: 1.5377117650274295\n",
      "Epoch 924/1000, Loss: 1.5370569247940604\n",
      "Epoch 925/1000, Loss: 1.5364021001672084\n",
      "Epoch 926/1000, Loss: 1.5357472911269758\n",
      "Epoch 927/1000, Loss: 1.5350924976534654\n",
      "Epoch 928/1000, Loss: 1.5344377197267804\n",
      "Epoch 929/1000, Loss: 1.533782957327025\n",
      "Epoch 930/1000, Loss: 1.533128210434303\n",
      "Epoch 931/1000, Loss: 1.532473479028718\n",
      "Epoch 932/1000, Loss: 1.5318187630903761\n",
      "Epoch 933/1000, Loss: 1.531164062599381\n",
      "Epoch 934/1000, Loss: 1.530509377535839\n",
      "Epoch 935/1000, Loss: 1.5298609710994575\n",
      "Epoch 936/1000, Loss: 1.5292253754497382\n",
      "Epoch 937/1000, Loss: 1.5285897951854888\n",
      "Epoch 938/1000, Loss: 1.5279627217915432\n",
      "Epoch 939/1000, Loss: 1.5273524414700448\n",
      "Epoch 940/1000, Loss: 1.526742176720332\n",
      "Epoch 941/1000, Loss: 1.526131927525125\n",
      "Epoch 942/1000, Loss: 1.5255216938671452\n",
      "Epoch 943/1000, Loss: 1.5249114757291131\n",
      "Epoch 944/1000, Loss: 1.5243012730937502\n",
      "Epoch 945/1000, Loss: 1.523691085943778\n",
      "Epoch 946/1000, Loss: 1.5230809142619186\n",
      "Epoch 947/1000, Loss: 1.5224707580308943\n",
      "Epoch 948/1000, Loss: 1.521860617233428\n",
      "Epoch 949/1000, Loss: 1.5212504918522423\n",
      "Epoch 950/1000, Loss: 1.5206403818700605\n",
      "Epoch 951/1000, Loss: 1.520030287269606\n",
      "Epoch 952/1000, Loss: 1.5194202080336037\n",
      "Epoch 953/1000, Loss: 1.5188101441447772\n",
      "Epoch 954/1000, Loss: 1.5182000955858512\n",
      "Epoch 955/1000, Loss: 1.5175900623395508\n",
      "Epoch 956/1000, Loss: 1.5169800443886017\n",
      "Epoch 957/1000, Loss: 1.5163700417157282\n",
      "Epoch 958/1000, Loss: 1.5157600543036578\n",
      "Epoch 959/1000, Loss: 1.5151500821351165\n",
      "Epoch 960/1000, Loss: 1.51454012519283\n",
      "Epoch 961/1000, Loss: 1.5139301834595258\n",
      "Epoch 962/1000, Loss: 1.5133202569179314\n",
      "Epoch 963/1000, Loss: 1.5127103455507742\n",
      "Epoch 964/1000, Loss: 1.512100449340782\n",
      "Epoch 965/1000, Loss: 1.5114905682706838\n",
      "Epoch 966/1000, Loss: 1.510880702323207\n",
      "Epoch 967/1000, Loss: 1.510270851481081\n",
      "Epoch 968/1000, Loss: 1.509661015727035\n",
      "Epoch 969/1000, Loss: 1.5090511950437995\n",
      "Epoch 970/1000, Loss: 1.5084413894141029\n",
      "Epoch 971/1000, Loss: 1.5078315988206763\n",
      "Epoch 972/1000, Loss: 1.5072218232462493\n",
      "Epoch 973/1000, Loss: 1.5066120626735542\n",
      "Epoch 974/1000, Loss: 1.5060023170853212\n",
      "Epoch 975/1000, Loss: 1.505392586464281\n",
      "Epoch 976/1000, Loss: 1.504782870793167\n",
      "Epoch 977/1000, Loss: 1.5041731700547105\n",
      "Epoch 978/1000, Loss: 1.5035634842316437\n",
      "Epoch 979/1000, Loss: 1.5029538133067002\n",
      "Epoch 980/1000, Loss: 1.5023441572626122\n",
      "Epoch 981/1000, Loss: 1.5017345160821134\n",
      "Epoch 982/1000, Loss: 1.5011248897479377\n",
      "Epoch 983/1000, Loss: 1.5005152782428184\n",
      "Epoch 984/1000, Loss: 1.4999056815494913\n",
      "Epoch 985/1000, Loss: 1.4992960996506897\n",
      "Epoch 986/1000, Loss: 1.4986865325291485\n",
      "Epoch 987/1000, Loss: 1.498076980167604\n",
      "Epoch 988/1000, Loss: 1.4974674425487908\n",
      "Epoch 989/1000, Loss: 1.4968579196554455\n",
      "Epoch 990/1000, Loss: 1.4962484114703036\n",
      "Epoch 991/1000, Loss: 1.4956389179761023\n",
      "Epoch 992/1000, Loss: 1.4950294391555783\n",
      "Epoch 993/1000, Loss: 1.4944199749914682\n",
      "Epoch 994/1000, Loss: 1.49381052546651\n",
      "Epoch 995/1000, Loss: 1.4932010905634414\n",
      "Epoch 996/1000, Loss: 1.4925916702650002\n",
      "Epoch 997/1000, Loss: 1.4919835455977313\n",
      "Epoch 998/1000, Loss: 1.4913817133311613\n",
      "Epoch 999/1000, Loss: 1.4907798955803118\n",
      "Epoch 1000/1000, Loss: 1.4901780923283736\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "input_size = X_train.shape[1]\n",
    "output_size = y_train.reshape(205, 1).shape[1]\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "hidden_size = 10\n",
    "\n",
    "# Train the model\n",
    "weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output = train_model(X_train, y_train.reshape(205,1), hidden_size, learning_rate, epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c10198-f6ce-4e1b-9846-549d0744de1d",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b5772596-6d9d-4530-acfb-573681e0b38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4400.0           4523.648157\n",
      "1              4000.0           5167.941225\n",
      "2              3475.0           4400.999461\n",
      "3              5250.0           4669.181167\n",
      "4              3950.0           4889.631199\n",
      "..                ...                   ...\n",
      "63             2850.0           5146.043933\n",
      "64             4450.0           3232.335649\n",
      "65             5500.0           3219.210410\n",
      "66             4500.0           4311.731287\n",
      "67             4750.0           3400.817492\n",
      "\n",
      "[68 rows x 2 columns]\n",
      "Validation Loss: 1237.5200093824064\n"
     ]
    }
   ],
   "source": [
    "# Perform forward pass on validation set\n",
    "val_output, val_hidden_layer_input, val_hidden_layer_output = forward_pass(X_val, weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output)\n",
    "\n",
    "# Calculate validation loss\n",
    "val_loss = mae(val_normal_actual, val_normal_predict)\n",
    "\n",
    "val_normal_actual = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
    "val_normal_predict = scaler_y.inverse_transform(val_output.reshape(-1, 1))\n",
    "\n",
    "results_val = np.concatenate((val_normal_actual, val_normal_predict), axis=1)\n",
    "\n",
    "results_val_df = pd.DataFrame(results_val, columns=['Actual Weight (g)', 'Predicted Weight (g)'])\n",
    "\n",
    "print(results_val_df)\n",
    "print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0dc7efed-d8a9-4a2b-83df-aabad7f6f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4900.0           4671.171739\n",
      "1              4750.0           4035.223160\n",
      "2              5750.0           4283.023273\n",
      "3              3700.0           5723.124348\n",
      "4              4300.0           4997.433260\n",
      "..                ...                   ...\n",
      "64             4725.0           3822.062473\n",
      "65             3800.0           5323.481061\n",
      "66             4250.0           4838.184753\n",
      "67             6000.0           4576.063260\n",
      "68             3200.0           5506.365171\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "Test Loss: 1195.2115854379038\n"
     ]
    }
   ],
   "source": [
    "test_output, _, _ = forward_pass(X_test, weights_input_hidden, biases_input_hidden, weights_hidden_output, biases_hidden_output)\n",
    "\n",
    "test_normal_actual =  scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "test_normal_predict = scaler_y.inverse_transform(test_output.reshape(-1, 1))\n",
    "\n",
    "# Calculate test loss\n",
    "test_loss = mae(test_normal_actual, test_normal_predict)\n",
    "\n",
    "results_test = np.concatenate((test_normal_actual, test_normal_predict), axis=1)\n",
    "\n",
    "results_test_df = pd.DataFrame(results_test, columns=['Actual Weight (g)', 'Predicted Weight (g)'])\n",
    "\n",
    "print(results_test_df)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0005454-60c9-4c62-a7d7-ebb126736071",
   "metadata": {},
   "source": [
    "# 3 Keras fully connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d573c7-bd7d-4f3c-b442-60b565a9873e",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2c4a2dbf-2f33-46e5-bd8f-3700103e0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(16,input_shape = (X_train.shape[1],),activation= 'relu'))\n",
    "model_1.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "23fac53d-e5f1-4d51-bbe4-0e258e36d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compile the model \n",
    "\n",
    "model_1.compile(loss = tf.keras.losses.MeanAbsoluteError() ,\n",
    "               optimizer= keras.optimizers.Adam(learning_rate=.001) , \n",
    "               metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "173097ea-2ab7-48eb-93a3-1da18996705b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 1.2492 - mae: 1.2492 - val_loss: 1.1897 - val_mae: 1.1897\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.1966 - mae: 1.1966 - val_loss: 1.1421 - val_mae: 1.1421\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1458 - mae: 1.1458 - val_loss: 1.0961 - val_mae: 1.0961\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.0963 - mae: 1.0963 - val_loss: 1.0512 - val_mae: 1.0512\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.0485 - mae: 1.0485 - val_loss: 1.0083 - val_mae: 1.0083\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.0005 - mae: 1.0005 - val_loss: 0.9664 - val_mae: 0.9664\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.9544 - mae: 0.9544 - val_loss: 0.9239 - val_mae: 0.9239\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.9091 - mae: 0.9091 - val_loss: 0.8818 - val_mae: 0.8818\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.8634 - mae: 0.8634 - val_loss: 0.8423 - val_mae: 0.8423\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.8186 - mae: 0.8186 - val_loss: 0.8027 - val_mae: 0.8027\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.7735 - mae: 0.7735 - val_loss: 0.7617 - val_mae: 0.7617\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.7289 - mae: 0.7289 - val_loss: 0.7210 - val_mae: 0.7210\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.6847 - mae: 0.6847 - val_loss: 0.6813 - val_mae: 0.6813\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.6397 - mae: 0.6397 - val_loss: 0.6428 - val_mae: 0.6428\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5984 - mae: 0.5984 - val_loss: 0.6045 - val_mae: 0.6045\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.5559 - mae: 0.5559 - val_loss: 0.5673 - val_mae: 0.5673\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5173 - mae: 0.5173 - val_loss: 0.5303 - val_mae: 0.5303\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.4810 - mae: 0.4810 - val_loss: 0.4958 - val_mae: 0.4958\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.4505 - mae: 0.4505 - val_loss: 0.4657 - val_mae: 0.4657\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4242 - mae: 0.4242 - val_loss: 0.4413 - val_mae: 0.4413\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3998 - mae: 0.3998 - val_loss: 0.4219 - val_mae: 0.4219\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.3831 - mae: 0.3831 - val_loss: 0.4044 - val_mae: 0.4044\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.3698 - mae: 0.3698 - val_loss: 0.3916 - val_mae: 0.3916\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3616 - mae: 0.3616 - val_loss: 0.3815 - val_mae: 0.3815\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3566 - mae: 0.3566 - val_loss: 0.3756 - val_mae: 0.3756\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3529 - mae: 0.3529 - val_loss: 0.3721 - val_mae: 0.3721\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3512 - mae: 0.3512 - val_loss: 0.3682 - val_mae: 0.3682\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3483 - mae: 0.3483 - val_loss: 0.3654 - val_mae: 0.3654\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.3465 - mae: 0.3465 - val_loss: 0.3631 - val_mae: 0.3631\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.3452 - mae: 0.3452 - val_loss: 0.3616 - val_mae: 0.3616\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.3442 - mae: 0.3442 - val_loss: 0.3606 - val_mae: 0.3606\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 0s 48ms/step - loss: 0.3434 - mae: 0.3434 - val_loss: 0.3595 - val_mae: 0.3595\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3429 - mae: 0.3429 - val_loss: 0.3589 - val_mae: 0.3589\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3424 - mae: 0.3424 - val_loss: 0.3586 - val_mae: 0.3586\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3420 - mae: 0.3420 - val_loss: 0.3581 - val_mae: 0.3581\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 0.3413 - mae: 0.3413 - val_loss: 0.3579 - val_mae: 0.3579\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3411 - mae: 0.3411 - val_loss: 0.3577 - val_mae: 0.3577\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3408 - mae: 0.3408 - val_loss: 0.3576 - val_mae: 0.3576\n",
      "Epoch 39/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3401 - mae: 0.3401 - val_loss: 0.3572 - val_mae: 0.3572\n",
      "Epoch 40/1000\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.3403 - mae: 0.3403 - val_loss: 0.3572 - val_mae: 0.3572\n",
      "Epoch 41/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3397 - mae: 0.3397 - val_loss: 0.3568 - val_mae: 0.3568\n",
      "Epoch 42/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3392 - mae: 0.3392 - val_loss: 0.3566 - val_mae: 0.3566\n",
      "Epoch 43/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3388 - mae: 0.3388 - val_loss: 0.3567 - val_mae: 0.3567\n",
      "Epoch 44/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3388 - mae: 0.3388 - val_loss: 0.3566 - val_mae: 0.3566\n",
      "Epoch 45/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3386 - mae: 0.3386 - val_loss: 0.3559 - val_mae: 0.3559\n",
      "Epoch 46/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3380 - mae: 0.3380 - val_loss: 0.3553 - val_mae: 0.3553\n",
      "Epoch 47/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3377 - mae: 0.3377 - val_loss: 0.3551 - val_mae: 0.3551\n",
      "Epoch 48/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3374 - mae: 0.3374 - val_loss: 0.3554 - val_mae: 0.3554\n",
      "Epoch 49/1000\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 0.3370 - mae: 0.3370 - val_loss: 0.3548 - val_mae: 0.3548\n",
      "Epoch 50/1000\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3366 - mae: 0.3366 - val_loss: 0.3542 - val_mae: 0.3542\n",
      "Epoch 51/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3364 - mae: 0.3364 - val_loss: 0.3537 - val_mae: 0.3537\n",
      "Epoch 52/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3361 - mae: 0.3361 - val_loss: 0.3537 - val_mae: 0.3537\n",
      "Epoch 53/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3358 - mae: 0.3358 - val_loss: 0.3531 - val_mae: 0.3531\n",
      "Epoch 54/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3359 - mae: 0.3359 - val_loss: 0.3527 - val_mae: 0.3527\n",
      "Epoch 55/1000\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.3353 - mae: 0.3353 - val_loss: 0.3525 - val_mae: 0.3525\n",
      "Epoch 56/1000\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.3353 - mae: 0.3353 - val_loss: 0.3524 - val_mae: 0.3524\n",
      "Epoch 57/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3348 - mae: 0.3348 - val_loss: 0.3521 - val_mae: 0.3521\n",
      "Epoch 58/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3348 - mae: 0.3348 - val_loss: 0.3515 - val_mae: 0.3515\n",
      "Epoch 59/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3346 - mae: 0.3346 - val_loss: 0.3507 - val_mae: 0.3507\n",
      "Epoch 60/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3341 - mae: 0.3341 - val_loss: 0.3506 - val_mae: 0.3506\n",
      "Epoch 61/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3338 - mae: 0.3338 - val_loss: 0.3500 - val_mae: 0.3500\n",
      "Epoch 62/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3335 - mae: 0.3335 - val_loss: 0.3498 - val_mae: 0.3498\n",
      "Epoch 63/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3334 - mae: 0.3334 - val_loss: 0.3498 - val_mae: 0.3498\n",
      "Epoch 64/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3331 - mae: 0.3331 - val_loss: 0.3496 - val_mae: 0.3496\n",
      "Epoch 65/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3328 - mae: 0.3328 - val_loss: 0.3491 - val_mae: 0.3491\n",
      "Epoch 66/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3325 - mae: 0.3325 - val_loss: 0.3488 - val_mae: 0.3488\n",
      "Epoch 67/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3324 - mae: 0.3324 - val_loss: 0.3484 - val_mae: 0.3484\n",
      "Epoch 68/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3322 - mae: 0.3322 - val_loss: 0.3481 - val_mae: 0.3481\n",
      "Epoch 69/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3324 - mae: 0.3324 - val_loss: 0.3486 - val_mae: 0.3486\n",
      "Epoch 70/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3320 - mae: 0.3320 - val_loss: 0.3486 - val_mae: 0.3486\n",
      "Epoch 71/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3318 - mae: 0.3318 - val_loss: 0.3486 - val_mae: 0.3486\n",
      "Epoch 72/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3316 - mae: 0.3316 - val_loss: 0.3483 - val_mae: 0.3483\n",
      "Epoch 73/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3313 - mae: 0.3313 - val_loss: 0.3483 - val_mae: 0.3483\n",
      "Epoch 74/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3310 - mae: 0.3310 - val_loss: 0.3480 - val_mae: 0.3480\n",
      "Epoch 75/1000\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.3308 - mae: 0.3308 - val_loss: 0.3476 - val_mae: 0.3476\n",
      "Epoch 76/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3304 - mae: 0.3304 - val_loss: 0.3479 - val_mae: 0.3479\n",
      "Epoch 77/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3303 - mae: 0.3303 - val_loss: 0.3483 - val_mae: 0.3483\n",
      "Epoch 78/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3301 - mae: 0.3301 - val_loss: 0.3485 - val_mae: 0.3485\n",
      "Epoch 79/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3299 - mae: 0.3299 - val_loss: 0.3484 - val_mae: 0.3484\n",
      "Epoch 80/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3296 - mae: 0.3296 - val_loss: 0.3486 - val_mae: 0.3486\n",
      "Epoch 81/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3296 - mae: 0.3296 - val_loss: 0.3486 - val_mae: 0.3486\n",
      "Epoch 82/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3293 - mae: 0.3293 - val_loss: 0.3484 - val_mae: 0.3484\n",
      "Epoch 83/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3288 - mae: 0.3288 - val_loss: 0.3478 - val_mae: 0.3478\n",
      "Epoch 84/1000\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3286 - mae: 0.3286 - val_loss: 0.3469 - val_mae: 0.3469\n",
      "Epoch 85/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3285 - mae: 0.3285 - val_loss: 0.3462 - val_mae: 0.3462\n",
      "Epoch 86/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3281 - mae: 0.3281 - val_loss: 0.3460 - val_mae: 0.3460\n",
      "Epoch 87/1000\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.3279 - mae: 0.3279 - val_loss: 0.3455 - val_mae: 0.3455\n",
      "Epoch 88/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3274 - mae: 0.3274 - val_loss: 0.3453 - val_mae: 0.3453\n",
      "Epoch 89/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3274 - mae: 0.3274 - val_loss: 0.3453 - val_mae: 0.3453\n",
      "Epoch 90/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3270 - mae: 0.3270 - val_loss: 0.3452 - val_mae: 0.3452\n",
      "Epoch 91/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3266 - mae: 0.3266 - val_loss: 0.3455 - val_mae: 0.3455\n",
      "Epoch 92/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3264 - mae: 0.3264 - val_loss: 0.3461 - val_mae: 0.3461\n",
      "Epoch 93/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3266 - mae: 0.3266 - val_loss: 0.3466 - val_mae: 0.3466\n",
      "Epoch 94/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3261 - mae: 0.3261 - val_loss: 0.3462 - val_mae: 0.3462\n",
      "Epoch 95/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3259 - mae: 0.3259 - val_loss: 0.3459 - val_mae: 0.3459\n",
      "Epoch 96/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3256 - mae: 0.3256 - val_loss: 0.3460 - val_mae: 0.3460\n",
      "Epoch 97/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3252 - mae: 0.3252 - val_loss: 0.3453 - val_mae: 0.3453\n",
      "Epoch 98/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3249 - mae: 0.3249 - val_loss: 0.3451 - val_mae: 0.3451\n",
      "Epoch 99/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3247 - mae: 0.3247 - val_loss: 0.3447 - val_mae: 0.3447\n",
      "Epoch 100/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3242 - mae: 0.3242 - val_loss: 0.3445 - val_mae: 0.3445\n",
      "Epoch 101/1000\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.3241 - mae: 0.3241 - val_loss: 0.3443 - val_mae: 0.3443\n",
      "Epoch 102/1000\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.3239 - mae: 0.3239 - val_loss: 0.3437 - val_mae: 0.3437\n",
      "Epoch 103/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3238 - mae: 0.3238 - val_loss: 0.3435 - val_mae: 0.3435\n",
      "Epoch 104/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3236 - mae: 0.3236 - val_loss: 0.3438 - val_mae: 0.3438\n",
      "Epoch 105/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3229 - mae: 0.3229 - val_loss: 0.3437 - val_mae: 0.3437\n",
      "Epoch 106/1000\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.3227 - mae: 0.3227 - val_loss: 0.3435 - val_mae: 0.3435\n",
      "Epoch 107/1000\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.3225 - mae: 0.3225 - val_loss: 0.3431 - val_mae: 0.3431\n",
      "Epoch 108/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3224 - mae: 0.3224 - val_loss: 0.3427 - val_mae: 0.3427\n",
      "Epoch 109/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3221 - mae: 0.3221 - val_loss: 0.3426 - val_mae: 0.3426\n",
      "Epoch 110/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3217 - mae: 0.3217 - val_loss: 0.3422 - val_mae: 0.3422\n",
      "Epoch 111/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3216 - mae: 0.3216 - val_loss: 0.3420 - val_mae: 0.3420\n",
      "Epoch 112/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3213 - mae: 0.3213 - val_loss: 0.3419 - val_mae: 0.3419\n",
      "Epoch 113/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3213 - mae: 0.3213 - val_loss: 0.3415 - val_mae: 0.3415\n",
      "Epoch 114/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3209 - mae: 0.3209 - val_loss: 0.3416 - val_mae: 0.3416\n",
      "Epoch 115/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3207 - mae: 0.3207 - val_loss: 0.3412 - val_mae: 0.3412\n",
      "Epoch 116/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3208 - mae: 0.3208 - val_loss: 0.3415 - val_mae: 0.3415\n",
      "Epoch 117/1000\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.3202 - mae: 0.3202 - val_loss: 0.3410 - val_mae: 0.3410\n",
      "Epoch 118/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3200 - mae: 0.3200 - val_loss: 0.3408 - val_mae: 0.3408\n",
      "Epoch 119/1000\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3197 - mae: 0.3197 - val_loss: 0.3407 - val_mae: 0.3407\n",
      "Epoch 120/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3195 - mae: 0.3195 - val_loss: 0.3406 - val_mae: 0.3406\n",
      "Epoch 121/1000\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3198 - mae: 0.3198 - val_loss: 0.3407 - val_mae: 0.3407\n",
      "Epoch 122/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3197 - mae: 0.3197 - val_loss: 0.3405 - val_mae: 0.3405\n",
      "Epoch 123/1000\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.3192 - mae: 0.3192 - val_loss: 0.3404 - val_mae: 0.3404\n",
      "Epoch 124/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3189 - mae: 0.3189 - val_loss: 0.3402 - val_mae: 0.3402\n",
      "Epoch 125/1000\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 0.3186 - mae: 0.3186 - val_loss: 0.3400 - val_mae: 0.3400\n",
      "Epoch 126/1000\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3184 - mae: 0.3184 - val_loss: 0.3398 - val_mae: 0.3398\n",
      "Epoch 127/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3184 - mae: 0.3184 - val_loss: 0.3398 - val_mae: 0.3398\n",
      "Epoch 128/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3184 - mae: 0.3184 - val_loss: 0.3398 - val_mae: 0.3398\n",
      "Epoch 129/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3180 - mae: 0.3180 - val_loss: 0.3404 - val_mae: 0.3404\n",
      "Epoch 130/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3179 - mae: 0.3179 - val_loss: 0.3403 - val_mae: 0.3403\n",
      "Epoch 131/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3183 - mae: 0.3183 - val_loss: 0.3402 - val_mae: 0.3402\n",
      "Epoch 132/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3178 - mae: 0.3178 - val_loss: 0.3398 - val_mae: 0.3398\n",
      "Epoch 133/1000\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.3172 - mae: 0.3172 - val_loss: 0.3396 - val_mae: 0.3396\n",
      "Epoch 134/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3173 - mae: 0.3173 - val_loss: 0.3397 - val_mae: 0.3397\n",
      "Epoch 135/1000\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.3173 - mae: 0.3173 - val_loss: 0.3395 - val_mae: 0.3395\n",
      "Epoch 136/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3169 - mae: 0.3169 - val_loss: 0.3395 - val_mae: 0.3395\n",
      "Epoch 137/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3167 - mae: 0.3167 - val_loss: 0.3401 - val_mae: 0.3401\n",
      "Epoch 138/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3169 - mae: 0.3169 - val_loss: 0.3401 - val_mae: 0.3401\n",
      "Epoch 139/1000\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.3165 - mae: 0.3165 - val_loss: 0.3393 - val_mae: 0.3393\n",
      "Epoch 140/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3164 - mae: 0.3164 - val_loss: 0.3387 - val_mae: 0.3387\n",
      "Epoch 141/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3163 - mae: 0.3163 - val_loss: 0.3387 - val_mae: 0.3387\n",
      "Epoch 142/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3164 - mae: 0.3164 - val_loss: 0.3388 - val_mae: 0.3388\n",
      "Epoch 143/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3164 - mae: 0.3164 - val_loss: 0.3387 - val_mae: 0.3387\n",
      "Epoch 144/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3162 - mae: 0.3162 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 145/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3162 - mae: 0.3162 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 146/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3161 - mae: 0.3161 - val_loss: 0.3391 - val_mae: 0.3391\n",
      "Epoch 147/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3162 - mae: 0.3162 - val_loss: 0.3396 - val_mae: 0.3396\n",
      "Epoch 148/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3155 - mae: 0.3155 - val_loss: 0.3394 - val_mae: 0.3394\n",
      "Epoch 149/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3154 - mae: 0.3154 - val_loss: 0.3390 - val_mae: 0.3390\n",
      "Epoch 150/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3154 - mae: 0.3154 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 151/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3151 - mae: 0.3151 - val_loss: 0.3390 - val_mae: 0.3390\n",
      "Epoch 152/1000\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.3151 - mae: 0.3151 - val_loss: 0.3391 - val_mae: 0.3391\n",
      "Epoch 153/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3151 - mae: 0.3151 - val_loss: 0.3395 - val_mae: 0.3395\n",
      "Epoch 154/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3147 - mae: 0.3147 - val_loss: 0.3396 - val_mae: 0.3396\n",
      "Epoch 155/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3147 - mae: 0.3147 - val_loss: 0.3395 - val_mae: 0.3395\n",
      "Epoch 156/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3147 - mae: 0.3147 - val_loss: 0.3393 - val_mae: 0.3393\n",
      "Epoch 157/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3145 - mae: 0.3145 - val_loss: 0.3396 - val_mae: 0.3396\n",
      "Epoch 158/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3143 - mae: 0.3143 - val_loss: 0.3403 - val_mae: 0.3403\n",
      "Epoch 159/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3145 - mae: 0.3145 - val_loss: 0.3402 - val_mae: 0.3402\n",
      "Epoch 160/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3141 - mae: 0.3141 - val_loss: 0.3402 - val_mae: 0.3402\n",
      "Epoch 161/1000\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.3142 - mae: 0.3142 - val_loss: 0.3398 - val_mae: 0.3398\n",
      "Epoch 162/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3143 - mae: 0.3143 - val_loss: 0.3403 - val_mae: 0.3403\n",
      "Epoch 163/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3137 - mae: 0.3137 - val_loss: 0.3404 - val_mae: 0.3404\n",
      "Epoch 164/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3136 - mae: 0.3136 - val_loss: 0.3400 - val_mae: 0.3400\n",
      "Epoch 165/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3139 - mae: 0.3139 - val_loss: 0.3401 - val_mae: 0.3401\n"
     ]
    }
   ],
   "source": [
    "## fit the model \n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 25)\n",
    "# Create a ModelCheckpoint callback\n",
    "checkpoint_1 = ModelCheckpoint(\"best_model_1.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "train_1 = model_1.fit(X_train,y_train, epochs =1000,verbose =1,validation_data=(X_val,y_val), batch_size= 32,callbacks=[callback,checkpoint_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5da19f93-b6d2-40c2-b2f5-2c3c1b71bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(train_1.history['mae'])\n",
    "plt.title(\"Model MAE\")\n",
    "plt.ylabel(\"mae\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d70c3-122a-411b-bc82-b266c70b84f9",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "497750dd-17a2-47d3-b5df-54f3f27df2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(64,input_shape = (X_train.shape[1],),activation= 'relu'))\n",
    "model_2.add(Dense(32,activation='relu'))\n",
    "model_2.add(Dense(1,activation='linear'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d95bab2d-04e3-4942-8eff-3f3fd980e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compile the model \n",
    "\n",
    "model_2.compile(loss = tf.keras.losses.MeanAbsoluteError() ,\n",
    "               optimizer= keras.optimizers.Adam(learning_rate=.001) , \n",
    "               metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0b08730c-d2e7-4fc3-8786-83018810c6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "7/7 [==============================] - 1s 61ms/step - loss: 0.7721 - mae: 0.7721 - val_loss: 0.6484 - val_mae: 0.6484\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.6405 - mae: 0.6405 - val_loss: 0.5450 - val_mae: 0.5450\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.5353 - mae: 0.5353 - val_loss: 0.4731 - val_mae: 0.4731\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4627 - mae: 0.4627 - val_loss: 0.4157 - val_mae: 0.4157\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.3951 - mae: 0.3951 - val_loss: 0.3726 - val_mae: 0.3726\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.3515 - mae: 0.3515 - val_loss: 0.3455 - val_mae: 0.3455\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3346 - mae: 0.3346 - val_loss: 0.3390 - val_mae: 0.3390\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3326 - mae: 0.3326 - val_loss: 0.3379 - val_mae: 0.3379\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3274 - mae: 0.3274 - val_loss: 0.3387 - val_mae: 0.3387\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 0.3229 - mae: 0.3229 - val_loss: 0.3356 - val_mae: 0.3356\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 0s 44ms/step - loss: 0.3191 - mae: 0.3191 - val_loss: 0.3352 - val_mae: 0.3352\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.3173 - mae: 0.3173 - val_loss: 0.3338 - val_mae: 0.3338\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.3161 - mae: 0.3161 - val_loss: 0.3331 - val_mae: 0.3331\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3155 - mae: 0.3155 - val_loss: 0.3357 - val_mae: 0.3357\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3151 - mae: 0.3151 - val_loss: 0.3366 - val_mae: 0.3366\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3156 - mae: 0.3156 - val_loss: 0.3375 - val_mae: 0.3375\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3135 - mae: 0.3135 - val_loss: 0.3371 - val_mae: 0.3371\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3123 - mae: 0.3123 - val_loss: 0.3375 - val_mae: 0.3375\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3118 - mae: 0.3118 - val_loss: 0.3352 - val_mae: 0.3352\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3119 - mae: 0.3119 - val_loss: 0.3362 - val_mae: 0.3362\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3105 - mae: 0.3105 - val_loss: 0.3360 - val_mae: 0.3360\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3093 - mae: 0.3093 - val_loss: 0.3368 - val_mae: 0.3368\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3102 - mae: 0.3102 - val_loss: 0.3351 - val_mae: 0.3351\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3099 - mae: 0.3099 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3094 - mae: 0.3094 - val_loss: 0.3384 - val_mae: 0.3384\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3074 - mae: 0.3074 - val_loss: 0.3378 - val_mae: 0.3378\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3087 - mae: 0.3087 - val_loss: 0.3351 - val_mae: 0.3351\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3068 - mae: 0.3068 - val_loss: 0.3349 - val_mae: 0.3349\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3059 - mae: 0.3059 - val_loss: 0.3363 - val_mae: 0.3363\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3053 - mae: 0.3053 - val_loss: 0.3349 - val_mae: 0.3349\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3049 - mae: 0.3049 - val_loss: 0.3354 - val_mae: 0.3354\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3043 - mae: 0.3043 - val_loss: 0.3371 - val_mae: 0.3371\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3044 - mae: 0.3044 - val_loss: 0.3388 - val_mae: 0.3388\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3038 - mae: 0.3038 - val_loss: 0.3401 - val_mae: 0.3401\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3053 - mae: 0.3053 - val_loss: 0.3395 - val_mae: 0.3395\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3037 - mae: 0.3037 - val_loss: 0.3414 - val_mae: 0.3414\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.3027 - mae: 0.3027 - val_loss: 0.3429 - val_mae: 0.3429\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3038 - mae: 0.3038 - val_loss: 0.3433 - val_mae: 0.3433\n"
     ]
    }
   ],
   "source": [
    "## fit the model \n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 25)\n",
    "# Create a ModelCheckpoint callback\n",
    "checkpoint_2 = ModelCheckpoint(\"best_model_2.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "train_2 = model_2.fit(X_train,y_train, epochs =1000,verbose =1,validation_data=(X_val,y_val), batch_size= 32,callbacks=[callback,checkpoint_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a4fc7eaf-1393-44bb-85ba-7ae36271d071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABBTElEQVR4nO3deXxU9b3/8feZmcxM9gQSEpZAWBRFBWQ1YqvWKC5FtFWhbhSV/qporbG2ohWs9ja2Xi11pVrRti7gWq1aq6J4r4pFiVxFEWUNCNkg+zJJZs7vj2QGxiRIkpk5yczr+Xicx0zOnDP5HI6P5t3vdgzTNE0BAABECZvVBQAAAIQS4QYAAEQVwg0AAIgqhBsAABBVCDcAACCqEG4AAEBUIdwAAICoQrgBAABRhXADAACiCuEGQJ9lGIZuvfXWbp+3fft2GYahxx57LOQ1Aej7CDcADuqxxx6TYRgyDEPvvvtuh89N01ROTo4Mw9D3v/99CyrsudWrVweu7fHHH+/0mBkzZsgwDB199NGdfu71ejVkyBAZhqF//etfnR5z6623Bn5PZ1tJSUnIrgmA5LC6AAD9g9vt1pNPPqkTTjghaP8777yjXbt2yeVyWVRZ7/mv7eKLLw7av337dr3//vtyu91dnvvWW29pz549ys3N1RNPPKEzzjijy2MffPBBJSUlddiflpbW49oBdES4AXBIzjzzTD3zzDO655575HDs/5+OJ598UpMnT1ZFRYWF1fXOmWeeqZdeekkVFRXKyMgI7H/yySeVlZWlww47TJWVlZ2e+/jjj2vSpEmaN2+ebrrpJtXX1ysxMbHTY88777yg7wcQHnRLATgkP/rRj7R371698cYbgX3Nzc169tlndeGFF3Z6Tn19va6//nrl5OTI5XJp7Nix+u///m+Zphl0nMfj0XXXXafMzEwlJyfr7LPP1q5duzr9zq+//lqXXXaZsrKy5HK5dNRRR2n58uW9urbZs2fL5XLpmWeeCdr/5JNP6oILLpDdbu/0vMbGRr3wwguaO3euLrjgAjU2NurFF1/sVS0Aeo9wA+CQ5ObmKi8vT0899VRg37/+9S9VV1dr7ty5HY43TVNnn322/vjHP+r000/X3XffrbFjx+qGG25QQUFB0LFXXHGFli5dqtNOO0133HGH4uLidNZZZ3X4ztLSUh133HF68803dfXVV+tPf/qTxowZo8svv1xLly7t8bUlJCRo9uzZQdf2f//3f/rss8+6DG6S9NJLL6murk5z585Vdna2TjrpJD3xxBNdHr9v3z5VVFQEbVVVVT2uG0AXTAA4iEcffdSUZH744YfmfffdZyYnJ5sNDQ2maZrm+eefb5588smmaZrmiBEjzLPOOitw3j/+8Q9Tkvnb3/426PvOO+880zAMc/PmzaZpmub69etNSeZVV10VdNyFF15oSjKXLFkS2Hf55ZebgwcPNisqKoKOnTt3rpmamhqoa9u2baYk89FHHz3otb399tumJPOZZ54xX375ZdMwDLO4uNg0TdO84YYbzFGjRpmmaZonnniiedRRR3U4//vf/745Y8aMwM8PPfSQ6XA4zLKysqDjlixZYkrqdBs7duxBawTQfbTcADhk/q6Xl19+WbW1tXr55Ze7bNl49dVXZbfb9bOf/Sxo//XXXy/TNAMzi1599VVJ6nDcz3/+86CfTdPUc889p1mzZsk0zaDWj5kzZ6q6ulpFRUU9vrbTTjtNAwYM0IoVK2SaplasWKEf/ehHXR6/d+9e/fvf/w465oc//KEMw9DTTz/d6TnPPfec3njjjaDt0Ucf7XHNADrHgGIAhywzM1P5+fl68skn1dDQIK/Xq/POO6/TY3fs2KEhQ4YoOTk5aP+RRx4Z+Nz/arPZNHr06KDjxo4dG/RzeXm5qqqq9NBDD+mhhx7q9HeWlZX16LokKS4uTueff76efPJJTZs2TTt37jxol9TKlSvV0tKiY489Vps3bw7snz59up544gktXLiwwznf/e53GVAMRADhBkC3XHjhhVqwYIFKSkp0xhlnRGwas8/nkyRdfPHFmjdvXqfHjB8/vle/48ILL9SyZct06623asKECRo3blyXx/rH1syYMaPTz7du3apRo0b1qh4APUO4AdAt5557rv7f//t/+uCDD7Ry5coujxsxYoTefPNN1dbWBrXefPHFF4HP/a8+n09btmwJaq3ZtGlT0Pf5Z1J5vV7l5+eH8pICTjjhBA0fPlyrV6/W73//+y6P27Ztm95//31dffXVOvHEE4M+8/l8uuSSS/Tkk0/q17/+dVjqBHBwjLkB0C1JSUl68MEHdeutt2rWrFldHnfmmWfK6/XqvvvuC9r/xz/+UYZhBBa787/ec889Qcd9c/aT3W7XD3/4Qz333HPasGFDh99XXl7ek8sJYhiG7rnnHi1ZskSXXHJJl8f5W21++ctf6rzzzgvaLrjgAp144okHnTUFILxouQHQbV11Cx1o1qxZOvnkk3XzzTdr+/btmjBhgl5//XW9+OKL+vnPfx4YYzNx4kT96Ec/0gMPPKDq6modf/zxWrVqVdA4Fr877rhDb7/9tqZPn64FCxZo3Lhx2rdvn4qKivTmm29q3759vb622bNna/bs2Qc95oknntDEiROVk5PT6ednn322rrnmGhUVFWnSpEmB/c8++2ynKxSfeuqpysrK6l3hAAIINwDCwmaz6aWXXtLixYu1cuVKPfroo8rNzdWdd96p66+/PujY5cuXKzMzU0888YT+8Y9/6Hvf+55eeeWVDuEhKytLa9eu1W233abnn39eDzzwgAYOHKijjjrqoN1IoVRUVKQvvvhCt9xyS5fHzJo1S9dcc01g9WK/K6+8stPj3377bcINEEKGaX5jqVAAAIB+jDE3AAAgqhBuAABAVCHcAACAqEK4AQAAUYVwAwAAogrhBgAARJWYW+fG5/Np9+7dSk5OlmEYVpcDAAAOgWmaqq2t1ZAhQ2SzHbxtJubCze7du7tcVRQAAPRtO3fu1LBhww56TMyFG/8D/Hbu3KmUlBSLqwEAAIeipqZGOTk5QQ/i7UrMhRt/V1RKSgrhBgCAfuZQhpQwoBgAAEQVwg0AAIgqhBsAABBVCDcAACCqEG4AAEBUIdwAAICoQrgBAABRhXADAACiCuEGAABEFcINAACIKoQbAAAQVQg3AAAgqhBuQsQ0TZXXerS1vM7qUgAAiGmEmxB558tyTf2vN3XVE0VWlwIAQEwj3ITIsPQESdLOfQ0yTdPiagAAiF2EmxAZlh4vSapv9qqyocXiagAAiF2EmxBxx9k1KNklqa31BgAAWINwE0I5A9q7pioJNwAAWIVwE0LD/eFmX6PFlQAAELsINyGU0z7uhpYbAACsQ7gJoWED9s+YAgAA1iDchFBOOuEGAACrEW5CKGdAW7fU11WN8vpY6wYAACsQbkJocGq8HDZDLV5TpTVNVpcDAEBMItyEkN1maEha+6BiuqYAALAE4SbEAtPBK5kODgCAFQg3IeYfd0PLDQAA1iDchFjgAZqsdQMAgCUINyGWw1o3AABYinATYoFVinkEAwAAliDchJi/5aa0tkmeVq/F1QAAEHsINyE2MNGp+Di7TFP6mhlTAABEHOEmxAzD2D9jinADAEDEEW7CYDiDigEAsAzhJgyYDg4AgHUIN2HgH1S8ixlTAABEHOEmDPzTwYvplgIAIOIIN2EQWMiPbikAACKOcBMG/nBT1dCi2qYWi6sBACC2EG7CIMnlUHpCnCRWKgYAINIIN2EynK4pAAAsQbgJk2GsdQMAgCUIN2GS077WzS5WKQYAIKIIN2ESeAQDLTcAAEQU4SZM/C03rHUDAEBkEW7CJLBKcWWjTNO0uBoAAGIH4SZMhqS5ZRhSY4tXFXXNVpcDAEDMINyEicth1+AUtySmgwMAEEmEmzBiOjgAAJFHuAkjpoMDABB5hJswYjo4AACRR7gJI3/LDWNuAACIHMJNGPmng7PWDQAAkUO4CSN/t9Tuqia1en0WVwMAQGwg3IRRVrJbTrtNXp+pPdVNVpcDAEBMINyEkc1maFh6+6Bixt0AABARhJsw8691s2sf08EBAIgEwk2Y5dByAwBARBFuwiyHVYoBAIgowk2Y7V/rhm4pAAAigXATZv7p4Kx1AwBAZPSJcHP//fcrNzdXbrdb06dP19q1a7s89qSTTpJhGB22s846K4IVHzp/y015rUdNLV6LqwEAIPpZHm5WrlypgoICLVmyREVFRZowYYJmzpypsrKyTo9//vnntWfPnsC2YcMG2e12nX/++RGu/NCkJcQp2eWQJO1iUDEAAGFnebi5++67tWDBAs2fP1/jxo3TsmXLlJCQoOXLl3d6/IABA5SdnR3Y3njjDSUkJPTZcGMYRmA6+E6mgwMAEHaWhpvm5matW7dO+fn5gX02m035+flas2bNIX3HI488orlz5yoxMTFcZfYa08EBAIgch5W/vKKiQl6vV1lZWUH7s7Ky9MUXX3zr+WvXrtWGDRv0yCOPdHmMx+ORx+MJ/FxTU9PzgnuI6eAAAESO5d1SvfHII4/omGOO0bRp07o8prCwUKmpqYEtJycnghW2CbTc0C0FAEDYWRpuMjIyZLfbVVpaGrS/tLRU2dnZBz23vr5eK1as0OWXX37Q4xYtWqTq6urAtnPnzl7X3V3+lhumgwMAEH6Whhun06nJkydr1apVgX0+n0+rVq1SXl7eQc995pln5PF4dPHFFx/0OJfLpZSUlKAt0gLdUoy5AQAg7CzvliooKNDDDz+sv/71r9q4caOuvPJK1dfXa/78+ZKkSy+9VIsWLepw3iOPPKJzzjlHAwcOjHTJ3eZf66a2qVXVDS0WVwMAQHSzdECxJM2ZM0fl5eVavHixSkpKNHHiRL322muBQcbFxcWy2YIz2KZNm/Tuu+/q9ddft6Lkbot32pWR5FJFnUc7KxuUmpBqdUkAAEQtwzRN0+oiIqmmpkapqamqrq6OaBfVuQ+8p4+Lq/TgRZN0xjGDI/Z7AQCIBt35+215t1Ss2P8ATcbdAAAQToSbCPE/QJPp4AAAhBfhJkJouQEAIDIINxHCWjcAAEQG4SZC/C03uyob5fPF1BhuAAAiinATIYPT3LLbDDW3+lRe5/n2EwAAQI8QbiIkzm7T4FS3JB6gCQBAOBFuIohBxQAAhB/hJoKYDg4AQPgRbiIo0HJDtxQAAGFDuIkgng4OAED4EW4iiG4pAADCj3ATQf6Wmz3VjWrx+iyuBgCA6ES4iaDMJJfccTb5TGl3Fa03AACEA+EmggzD0LDAoGLCDQAA4UC4ibCc9PZxNwwqBgAgLAg3ERaYMcV0cAAAwoJwE2H7VymmWwoAgHAg3ETY/ungtNwAABAOhJsIo1sKAIDwItxEmD/c7K1vVr2n1eJqAACIPoSbCEtxxyk1Pk6StItxNwAAhBzhxgKMuwEAIHwINxbYP2OKcAMAQKgRbiywf1Ax3VIAAIQa4cYCrFIMAED4EG4swHRwAADCh3BjgQPDjWmaFlcDAEB0IdxYYGhaW7dUfbNXlQ0tFlcDAEB0IdxYwB1nV1aKSxJdUwAAhBrhxiJMBwcAIDwINxZhOjgAAOFBuLEI08EBAAgPwo1FhjEdHACAsCDcWGR4e7jh4ZkAAIQW4cYiOYFw0yCvj7VuAAAIFcKNRbJT3HI6bGrxmtpdResNAAChQrixiN1maFRGoiTpq7Jai6sBACB6EG4sNGZQkiRpc1mdxZUAABA9CDcWItwAABB6hBsLHTYoWZL0FeEGAICQIdxY6MCWG54ODgBAaBBuLJSbkSC7zVBtU6vKaj1WlwMAQFQg3FjI5bBrRPt6N4y7AQAgNAg3Fhvd3jX1VSnTwQEACAXCjcUO84+7KaflBgCAUCDcWIzp4AAAhBbhxmL+6eCEGwAAQoNwY7HRg9oewVBR16zK+maLqwEAoP8j3FgswenQ0LR4SYy7AQAgFAg3fQDjbgAACB3CTR9AuAEAIHQIN32Afzo4z5gCAKD3CDd9gL/lZgvhBgCAXiPc9AH+cPN1VaPqPa0WVwMAQP9GuOkD0hKcykhySZK2MGMKAIBeIdz0EWPa17thUDEAAL1DuOkj/CsVM6gYAIDeIdz0EUwHBwAgNAg3fQThBgCA0CDc9BH+tW527K2Xp9VrcTUAAPRfloeb+++/X7m5uXK73Zo+fbrWrl170OOrqqq0cOFCDR48WC6XS4cffrheffXVCFUbPpnJLiW7HfKZ0vaKBqvLAQCg37I03KxcuVIFBQVasmSJioqKNGHCBM2cOVNlZWWdHt/c3KxTTz1V27dv17PPPqtNmzbp4Ycf1tChQyNceegZhnHASsW1FlcDAED/5bDyl999991asGCB5s+fL0latmyZXnnlFS1fvlw33nhjh+OXL1+uffv26f3331dcXJwkKTc3N5Ilh9WYQUkqKq5i3A0AAL1gWctNc3Oz1q1bp/z8/P3F2GzKz8/XmjVrOj3npZdeUl5enhYuXKisrCwdffTR+t3vfievt+sxKh6PRzU1NUFbXzWGZ0wBANBrloWbiooKeb1eZWVlBe3PyspSSUlJp+ds3bpVzz77rLxer1599VXdcsstuuuuu/Tb3/62y99TWFio1NTUwJaTkxPS6wgl/1o3PGMKAICes3xAcXf4fD4NGjRIDz30kCZPnqw5c+bo5ptv1rJly7o8Z9GiRaqurg5sO3fujGDF3eNvudlaUa9Wr8/iagAA6J8sG3OTkZEhu92u0tLSoP2lpaXKzs7u9JzBgwcrLi5Odrs9sO/II49USUmJmpub5XQ6O5zjcrnkcrlCW3yYDE2LlzvOpqYWn3ZWNmpkRqLVJQEA0O9Y1nLjdDo1efJkrVq1KrDP5/Np1apVysvL6/ScGTNmaPPmzfL59rdqfPnllxo8eHCnwaa/sdkMjc5kMT8AAHrD0m6pgoICPfzww/rrX/+qjRs36sorr1R9fX1g9tSll16qRYsWBY6/8sortW/fPl177bX68ssv9corr+h3v/udFi5caNUlhNwYpoMDANArlk4FnzNnjsrLy7V48WKVlJRo4sSJeu211wKDjIuLi2Wz7c9fOTk5+ve//63rrrtO48eP19ChQ3XttdfqV7/6lVWXEHKH8RgGAAB6xTBN07S6iEiqqalRamqqqqurlZKSYnU5Hby2YY9++niRxg9L1UtXn2B1OQAA9And+fvdr2ZLxYIxB0wHj7HcCQBASBBu+pgRAxPksBmqb/ZqT3WT1eUAANDvEG76mDi7TbntU8BZqRgAgO4j3PRBDCoGAKDnCDd90JhAuGE6OAAA3UW46YPG0HIDAECPEW76oAOfDs6MKQAAuodw0weNzkySYUhVDS3aW99sdTkAAPQrhJs+yB1nV056giS6pgAA6C7CTR91YNcUAAA4dISbPso/HXwL4QYAgG4h3PRRo5kxBQBAjxBu+qjDAt1SrHUDAEB3EG76KH/LTWmNRzVNLRZXAwBA/0G46aNS3HHKSnFJomsKAIDuINz0YYcNSpZEuAEAoDsIN33YGGZMAQDQbYSbPoy1bgAA6D7CTR/GAzQBAOg+wk0f5g83Oysb1NTitbgaAAD6B8JNHzYw0an0hDiZprSlnNYbAAAOBeGmDzMMg64pAAC6iXDTx41hOjgAAN1CuOnjaLkBAKB7CDd9HNPBAQDoHsJNH+d/gOb2inq1eH0WVwMAQN/X43Dz97//XTNmzNCQIUO0Y8cOSdLSpUv14osvhqw4SINT3Up02tXqM7Vjb73V5QAA0Of1KNw8+OCDKigo0Jlnnqmqqip5vW1rsKSlpWnp0qWhrC/mMWMKAIDu6VG4uffee/Xwww/r5ptvlt1uD+yfMmWKPv3005AVhzajCTcAAByyHoWbbdu26dhjj+2w3+Vyqb6erpNQY1AxAACHrkfhZuTIkVq/fn2H/a+99pqOPPLI3taEbziMtW4AADhkjp6cVFBQoIULF6qpqUmmaWrt2rV66qmnVFhYqL/85S+hrjHm+VtutpTXyeczZbMZFlcEAEDf1aNwc8UVVyg+Pl6//vWv1dDQoAsvvFBDhgzRn/70J82dOzfUNca8nPR4OR02NbX49HVVo3IGJFhdEgAAfVaPwo0kXXTRRbrooovU0NCguro6DRo0KJR14QAOu02jMhL1RUmtNpfVEW4AADiIXi/il5CQQLCJgNGBQcW1FlcCAEDf1uOWm2effVZPP/20iouL1dzcHPRZUVFRrwtDsMOYDg4AwCHpUcvNPffco/nz5ysrK0sff/yxpk2bpoEDB2rr1q0644wzQl0jxHRwAAAOVY/CzQMPPKCHHnpI9957r5xOp375y1/qjTfe0M9+9jNVV1eHukYoeDq4aZoWVwMAQN/Vo3BTXFys448/XpIUHx+v2tq2cSCXXHKJnnrqqdBVh4DcjATZDKm2qVXltR6rywEAoM/qUbjJzs7Wvn37JEnDhw/XBx98IKlt5WJaFcLD5bArNyNRkvTZnhqLqwEAoO/qUbj53ve+p5deekmSNH/+fF133XU69dRTNWfOHJ177rkhLRD7HZuTLklat73S4koAAOi7ejRb6qGHHpLP55MkLVy4UBkZGXrvvfd09tln66c//WlIC8R+U3PT9VzRLn24fZ/VpQAA0Gf1KNzYbDY1NzerqKhIZWVlio+PV35+vqS250vNmjUrpEWizZTcAZKk9Tur1Nzqk9PR62WKAACIOj0KN6+99pouueQS7d27t8NnhmHI6/X2ujB0NDozUekJcapsaNGG3dWaNDzd6pIAAOhzevR//a+55hpdcMEF2rNnj3w+X9BGsAkfwzACrTcf0TUFAECnehRuSktLVVBQoKysrFDXg28xNbetteZDBhUDANCpHoWb8847T6tXrw5xKTgUB7bcMO0eAICOejTm5r777tP555+v//3f/9UxxxyjuLi4oM9/9rOfhaQ4dHT0kFS542yqbGjRlvL6wGMZAABAmx6Fm6eeekqvv/663G63Vq9eLcMwAp8ZhkG4CSOnw6aJOWn6YOs+fbR9H+EGAIBv6FG31M0336zf/OY3qq6u1vbt27Vt27bAtnXr1lDXiG+Y2t41xbgbAAA66lG4aW5u1pw5c2Szsc6KFQLjbnYwYwoAgG/qUTqZN2+eVq5cGepacIgmDU+TzZB27G1QWU2T1eUAANCn9GjMjdfr1R/+8Af9+9//1vjx4zsMKL777rtDUhw6l+yO0xHZKfp8T40+2lGpM48ZbHVJAAD0GT0KN59++qmOPfZYSdKGDRuCPjtwcDHCZ2puuj7fU6MPt+8j3AAAcIAehZu333471HWgm6bkDtBf1+zgIZoAAHwDI4L7qSntKxV/vrtGdZ5Wi6sBAKDvINz0U4NT4zUsPV4+U/q4mCnhAAD4EW76Mda7AQCgI8JNP+bvmuIJ4QAA7Ee46cf8LTcfF1epxeuzuBoAAPqGPhFu7r//fuXm5srtdmv69Olau3Ztl8c+9thjMgwjaHO73RGstu8Yk5mk1Pg4NbZ49fnuGqvLAQCgT7A83KxcuVIFBQVasmSJioqKNGHCBM2cOVNlZWVdnpOSkqI9e/YEth07dkSw4r7DZjM0ZURb1xRTwgEAaGN5uLn77ru1YMECzZ8/X+PGjdOyZcuUkJCg5cuXd3mOYRjKzs4ObFlZWRGsuG+ZOrL9OVMMKgYAQJLF4aa5uVnr1q1Tfn5+YJ/NZlN+fr7WrFnT5Xl1dXUaMWKEcnJyNHv2bH322WeRKLdPmuofVLxjn0zTtLgaAACsZ2m4qaiokNfr7dDykpWVpZKSkk7PGTt2rJYvX64XX3xRjz/+uHw+n44//njt2rWr0+M9Ho9qamqCtmhy9NBUOR02VdQ1a/veBqvLAQDAcpZ3S3VXXl6eLr30Uk2cOFEnnniinn/+eWVmZurPf/5zp8cXFhYqNTU1sOXk5ES44vByOeyaOCxNEuNuAACQLA43GRkZstvtKi0tDdpfWlqq7OzsQ/qOuLg4HXvssdq8eXOnny9atEjV1dWBbefOnb2uu69hvRsAAPazNNw4nU5NnjxZq1atCuzz+XxatWqV8vLyDuk7vF6vPv30Uw0e3PmTsV0ul1JSUoK2aONf74ZBxQAA9PCp4KFUUFCgefPmacqUKZo2bZqWLl2q+vp6zZ8/X5J06aWXaujQoSosLJQk3XbbbTruuOM0ZswYVVVV6c4779SOHTt0xRVXWHkZlpo0PF2GIW2tqFdFnUcZSS6rSwIAwDKWh5s5c+aovLxcixcvVklJiSZOnKjXXnstMMi4uLhYNtv+BqbKykotWLBAJSUlSk9P1+TJk/X+++9r3LhxVl2C5VIT4jQ2K1lflNTqo+37dPrRnbdiAQAQCwwzxuYP19TUKDU1VdXV1VHVRfXrf3yqxz8o1uUnjNQt34/doAcAiE7d+fvd72ZLoXP7x90wqBgAENsIN1FiSnu42bC7Rg3NrRZXAwCAdQg3UWJoWryGpLrl9ZlaX1xldTkAAFiGcBNF/K03HzIlHAAQwwg3UeTA50wBABCrCDdRxN9yU7SjUq1en8XVAABgDcJNFBmblaxkt0P1zV59UVJrdTkAAFiCcBNFbDZDU0a0dU3xEE0AQKwi3ESZKTxnCgAQ4wg3UWZqYMbUPsXY4tMAAEgi3ESd8cNS5bTbVFbr0c59jVaXAwBAxBFuoow7zq5jhqVKYtwNACA2EW6i0JRcBhUDAGIX4SYKTR2xf9wNAACxhnAThSa3TwffUl6vvXUei6sBACCyCDdRKD3RqcMGJUmS1u1gSjgAILYQbqJUYL0bwg0AIMYQbqLUVAYVAwBiFOEmSvkX89vwdbUam70WVwMAQOQQbqLUsPR4ZaW41OI19X+7qqwuBwCAiCHcRCnDMALjbtZuo2sKABA7CDdR7LhRAyVJH2zda3ElAABEDuEmiuW1h5t1OyrlaWXcDQAgNhBuotjozERlJrvkafVpfXGV1eUAABARhJsoZhhGoGtqDV1TAIAYQbiJcseNahtUzLgbAECsINxEOf+4m6LiKjW1MO4GABD9CDdRbmRGogYlu9Tc6tPHjLsBAMQAwk2UMwxDeaMZdwMAiB2EmxjAejcAgFhCuIkB/nE36xl3AwCIAYSbGDBiYIKyU9xq9vpUtKPS6nIAAAgrwk0MYNwNACCWEG5iBOvdAABiBeEmRuSNypAkrd9ZpcZmxt0AAKIX4SZG5AyI15BUt1q8ptYx7gYAEMUINzEi+DlTFRZXAwBA+BBuYshxo/3r3eyzuBIAAMKHcBND/Ovd/N/OKtV7Wi2uBgCA8CDcxJCcAQkamhavVh/jbgAA0YtwE2P2j7thSjgAIDoRbmJM3mieMwUAiG6EmxjjX8zvk13VqmPcDQAgChFuYsyw9ATlDIiX12fqo+3MmgIARB/CTQw6biTjbgAA0YtwE4PyWO8GABDFCDcxyD9jasPX1aptarG4GgAAQotwE4OGpMVrxMCE9nE3rHcDAIguhJsYxbgbAEC0ItzEKNa7AQBEK8JNjDpw3E0N424AAFGEcBOjslPdGpmRKJ8pfbiNWVMAgOhBuIlh/tWK12yhawoAED0INzHM3zX1wTbCDQAgehBuYlhee7j5bHeNqhsYdwMAiA6Emxg2KMWtUZmJMk1pLc+ZAgBECcJNjPN3TTHuBgAQLQg3Mc7fNcV6NwCAaEG4iXHT22dMbSypUVVDs8XVAADQe4SbGDco2a0xg5JkmtJ/WO8GABAFCDdgvRsAQFTpE+Hm/vvvV25urtxut6ZPn661a9ce0nkrVqyQYRg655xzwltglMsblSGJcTcAgOhgebhZuXKlCgoKtGTJEhUVFWnChAmaOXOmysrKDnre9u3b9Ytf/ELf+c53IlRp9PKPu/mipFb76hl3AwDo3ywPN3fffbcWLFig+fPna9y4cVq2bJkSEhK0fPnyLs/xer266KKL9Jvf/EajRo2KYLXRKSPJpcOzkiRJa1mtGADQz1kabpqbm7Vu3Trl5+cH9tlsNuXn52vNmjVdnnfbbbdp0KBBuvzyy7/1d3g8HtXU1ARt6Ij1bgAA0cLScFNRUSGv16usrKyg/VlZWSopKen0nHfffVePPPKIHn744UP6HYWFhUpNTQ1sOTk5va47Gu1f74YZUwCA/s3ybqnuqK2t1SWXXKKHH35YGRkZh3TOokWLVF1dHdh27twZ5ir7p+nt4WZTaa321nksrgYAgJ5zWPnLMzIyZLfbVVpaGrS/tLRU2dnZHY7fsmWLtm/frlmzZgX2+Xw+SZLD4dCmTZs0evTooHNcLpdcLlcYqo8uAxKdOiI7WV+U1Oo/2/bpzGMGW10SAAA9YmnLjdPp1OTJk7Vq1arAPp/Pp1WrVikvL6/D8UcccYQ+/fRTrV+/PrCdffbZOvnkk7V+/Xq6nHrJP+7mf74st7gSAAB6ztKWG0kqKCjQvHnzNGXKFE2bNk1Lly5VfX295s+fL0m69NJLNXToUBUWFsrtduvoo48OOj8tLU2SOuxH9512VJYee3+7XvlkjxbPGqcEp+X/eQAA0G2W//WaM2eOysvLtXjxYpWUlGjixIl67bXXAoOMi4uLZbP1q6FB/dZxIwdqxMAE7djboFc/LdF5k4dZXRIAAN1mmKZpWl1EJNXU1Cg1NVXV1dVKSUmxupw+5/63N+vOf2/S1Nx0PfPT460uBwAASd37+02TCIKcN3mY7DZDH26v1OayOqvLAQCg2wg3CJKV4tbJYwdJkp7+iGnzAID+h3CDDuZObZt19nzRLjW3+iyuBgCA7iHcoIOTxmZqULJLFXXNeuuL0m8/AQCAPoRwgw4cdltgptSKD+maAgD0L4QbdOqCKW1dU+98Wa7dVY0WVwMAwKEj3KBTuRmJyhs1UKYpPfPRLqvLAQDgkBFu0KW509pab57+aKe8vphaDgkA0I8RbtClmUdlKzU+Tl9XNeq9zRVWlwMAwCEh3KBL7ji7zj12qCRpJQOLAQD9BOEGBzWnfc2b1z8v0d46j8XVAADw7Qg3OKgjB6do/LBUtXhNvfDx11aXAwDAtyLc4Fv5W29WfLhTMfacVQBAP0S4wbc6e8IQxcfZtbmsTkXFlVaXAwDAQRFu8K2S3XE6a/xgSdKKtQwsBgD0bYQbHBL/wzRf/mSPaptaLK4GAICuEW5wSCaPSNfozEQ1tnj18id7rC4HAIAuEW5wSAzD0NypwyXxME0AQN9GuMEhO3fSUMXZDf3fzipt3FNjdTkAAHSKcINDlpHkUv6RWZJYsRgA0HcRbtAt/jVvXvj4azW1eC2uBgCAjgg36JbvHJapIaluVTe26N+flVhdDgAAHRBu0C12m6Hzp7S13tA1BQDoiwg36LbzpwyTYUjvb9mrHXvrrS4HAIAghBt027D0BH3nsExJ0tMf0XoDAOhbCDfoEf+Kxc98tEutXp/F1QAAsB/hBj2Sf2SWBiQ6VVbr0epN5VaXAwBAAOEGPeJ02PSDY4dKkh5YvZlp4QCAPoNwgx67JG+EEpx2FRVX6Yq/fqTGZgIOAMB6hBv02IiBifrrZdOU6LTr3c0VuvyvHxJwAACWI9ygV6bmDggEnPe37NVlj32ohuZWq8sCAMQwwg16bUruAP3t8mlKcjm0ZisBBwBgLcINQmLyiLYWnCSXQx9s3acfP/qh6j0EHABA5BFuEDKTR6Trb5dPU7LLobXb9mk+AQcAYAHCDUJq0vB0/f2K6Up2O7R2+z79+NG1qiPgAAAiiHCDkJuYk6bHL28LOB9ur9S85WtV29RidVkAgBhBuEFYTMhJ0xNXTFeK26F1Owg4AIDIIdwgbMYPS9MTVxyn1Pg4FRVX6dLla1VDwAEAhBnhBmF1zLBUPXHFdKXGx+nj4ipd+ggBBwAQXoZpmqbVRURSTU2NUlNTVV1drZSUFKvLiRkbvq7WxY/8R1UNLUpxOzR8YIKGpMZrSFq8hqS521/jNSQ1XpnJLtlthtUlAwD6kO78/SbcIGI+312jeY+uVXmt56DHOWyGslPd7eHHrdGZSfrekYM0bnCKDIPQAwCxiHBzEIQbazW1eLWlvE57qpq0u7pRX1c1tr2vatSe6iaV1DTJ6+v8P8mhafE6dVyWTh2XpWkjByjOTq8qAMQKws1BEG76tlavT2W1Hu2pbtTX7aFn3Y5K/e9X5Wpq8QWOS3E7dPIRg3TquCydeHimkt1xFlYNAAg3ws1BEG76p8Zmr97dXKE3Pi/Rqo1l2lvfHPgszm7ouFEDddq4LOWPy9Lg1HgLKwUAhAPh5iAIN/2f12fq4+JKvfF5qd74vFRbK+qDPj96aIq+c1imZozO0JTcdLnj7BZVCgAIFcLNQRBuos+W8rpA0CkqrtSB/0U7HTZNGZGuGWMyNGNMho4ZmspMLADohwg3B0G4iW7ltR79z5flem9Lhd7fvFclNU1Bnye7HTpu1ECdMCZDM8YM1OjMJGZgAUA/QLg5CMJN7DBNU1vK6/X+lgq9t7lCa7bsVU1T8EM8s1JcOn50hqbmDtDkEek6bFCSbLTsAECfQ7g5CMJN7PL6TG34ujrQqvPh9n3ytPqCjkl2OTRxeJomj0jX5BHpmpiTxkwsAOgDCDcHQbiBX1OLV0U7KrVm614VFVfq4+IqNTR7g44xDGlsVrImjUjX5OFtgWfEwAS6sgAgwgg3B0G4QVdavT5tKq1V0Y5KrdtRqaLiKhXva+hw3MBEp3IzEpWd4lZWilvZqS5l+d+nuJWd6maGFgCEGOHmIAg36I6y2iYV7ahSUXGlinZU6pOvq9X8ja6szqS4HcpO3R94BiQ5lRbvVGp8nFLj45SWEBd4nxIfp2SXg7E+AHAQhJuDINygNzytXn2xp1ZfVzWqpLpJpbVNKm1/bERpjUcl1U1qbPF++xd9g82QUuL3B55kt0Nuh11up73tNc6m+Di73HFt792B9/b2/TbF2W1y2A057W3v4+w2OR1G+36b4g74zG4z1Oz1ydPik6fVK09r2/u2fe0/t7Z/1uJTi9enjCSXhqbHa1h6POOQAERcd/5+OyJUExAVXA67JuSkaUJOWqefm6apmqZWldY0qbSmqS0A1TRpX32Lqhv9W/MB71vU1OKTz5SqGlpU1dAS2QvqoRS3Q8PSEwJhZ2havIalJ2hY+8+p8XGMSwJgGcINEEKGYQRaXw7PSj6kc5pavKppbAkKPHWeVjW1eNXU4lNjizfwvqn9fWPgtW2fp8WrFq+pFq+vfTPV3P6+tf19V91pLodNLodNToe97X2cTS7/e4dNrji7HDZDZbVN+rqyUZUNLappatXne2r0+Z6aTr8z0WlXZrJLaQlODUh0Ki0hTukJTqUnxCktwRn8PrHtM8YpAQgVwg1gMX/30qAUd1h/j2ma8vpMtXhNtfp8cjpsctpt3W5hqfe06uuqRu2qbNDXlY3aVdmoXVVtr19XNqqizqP6Zq/q9zZIezsOyO6Kw2a01dRelyuu7dXpsMvpsMlltwV97rAbMk3JlCmfr/3VVNs+05QpyWea7ce0df0NSHBqYJJTAxJdGpjkVIb/faJTGUkuxTsJWEA0INwAMcIwDDnshhx2Ser5H/FEl0OHZyV32TLV1OLV11WN2lvXrMqGZlU1NKuyoUWV9W0/Vza0BPb5X70+U60+U63N3g7T8SMpPs6ugUlODUxqCzxp7QO+DxwP1dnmjgsOic2tPtV7WlXnaVVDs7f9tVX1nlbVe7yqb2579Zmm7DZDDptxwKtt/892Qw6bLfCZf0xVIOS1B704u7/1bf8+u82gaxAxi3ADIKTccXaNzkzS6MxDO94/TqmhuVXNrW3dZ55WX6ArLbAd8LPH65PX65NhGLIZktpfDbW/Gm1hzpBkMwwZhtTqM1VZ36y99c3aW9esvfUe7a1r1r76ZlXUeeRpbesC3NXeGtUdTrtNKfEOtfpM1Xta1eK1fp6GYUiJToeGpLk1NC1eQ9PjNTStbVzU0PR4DUuLV0aS65Bn6TW1eLWvvu3fq7Kh7bW6sUUp7jgNTnVrcGq8slJdcjlo/YL1CDcALHXgOCWrmKap+mav9tU1q6Leo33t4ac6aCxUa+D9gWOkvL62MU0Vdc0dvtflsCnJ5VCCy65Ep0OJrvbNaVeC0yGHzVCrz5TX52t/Nb/x2jZmyv9zS3vA8782e9uDYPv7A+e+mqZU52nVl6V1+rK0rtPrdjpsbcGnfctKcanO49W+eo/2tbe2+QPNoc4CzEhyKjvVreyU+LbQk+bW4Pafh6S5lZ7oVKLTwQNsEVZMBQeAHvKHIn/gibMbSnQ5lOBsCzAOuy2itfiDlr+Fq6apVburGvV1Vdt4qAPHSpXUNMnXzf/1j7MbSm8fJJ6e0LZuU3Vji/ZUN2pPdVOHx5kcTILTriSXQ0luh5LbQ1+Hn90OJTodcjlsgWUQXA67XP7lEILetw1+dztiq0vObB9XFgvrZDEVHAAiwDCMtj/ILoeGpsVbXkvbmCqbEpxt+walSGMGJXV6fIvXp5LqpraB4O3hp7yuScnuOA1IcCo90akBiXEakOhq/zlOSS5Hl6HBNE1VNbRod3XbGlB7qpsCoaekfdtd3aimlrYA1NA+vqqs1hOGfwsFxia5HP41n2yBfXEHfOYfvP7NGYL+sBTY1x6k2gbht7WM+doHtLe9N9v/HQ4c5N727+JsD2fxcXYlONvXp3K2/Rzf/t7lCB631dDcqvJajyrqPCqvbd/qmjvsq6jzqNVnamCiU4NSXMpMcmlQsluZya79P6e4lJnk1qAUV8RmJZqmaWnA7BMtN/fff7/uvPNOlZSUaMKECbr33ns1bdq0To99/vnn9bvf/U6bN29WS0uLDjvsMF1//fW65JJLDul30XIDANYwTVOeAwZb13laVdd0wPv2n+s9raptf9/QvtSBfykET2v7kgitbfs8LV41tXa91EF/4g86TS3hG1if7HIoI9mlFLejbXV0t0Mp7rbXZHecUtpfkw/4PNHpUH1zq6ob2rpiqw7olq1qaGu1rGpfv6uq/ZgJw9L0+BXTQ1p7v2q5WblypQoKCrRs2TJNnz5dS5cu1cyZM7Vp0yYNGjSow/EDBgzQzTffrCOOOEJOp1Mvv/yy5s+fr0GDBmnmzJkWXAEA4FAYhhFY+mBgkiuk3+3zmYFVtYMGoHt9amk11ez1BsYntXjN9s+8gQHsQat1t/rXj+pkX3uI8g9gN/wD2GXIZjtwX9uAdsNoayVrbN6/LlXb+7btwFDm3+cXH9e2XlRGkrP91RV49b/PTHLJ6bCpos6jstomldd6VFbjUVl7y05ZbZPK69r2eVp9qm0PjuFW2dBxDFokWd5yM336dE2dOlX33XefJMnn8yknJ0fXXHONbrzxxkP6jkmTJumss87S7bff/q3H0nIDAOgrvD4zsDCnP/Q47TZlJruU6Apd+4Npmqr1tKqspq0rq7apVbVNLaptalVNY0tb6GlqW6CzprEl8HlNU6saPK1KdDmCno3nXx6h7Zl5DqW1j8FKaf/cPzYrlPpNy01zc7PWrVunRYsWBfbZbDbl5+drzZo133q+aZp66623tGnTJv3+97/v9BiPxyOPZ3+fbk1N5yuqAgAQaXabEZhFF06GYSjFHacUd1yX47CiSeSG8neioqJCXq9XWVlZQfuzsrJUUlLS5XnV1dVKSkqS0+nUWWedpXvvvVennnpqp8cWFhYqNTU1sOXk5IT0GgAAQN9iabjpqeTkZK1fv14ffvih/uu//ksFBQVavXp1p8cuWrRI1dXVgW3nzp2RLRYAAESUpd1SGRkZstvtKi0tDdpfWlqq7OzsLs+z2WwaM2aMJGnixInauHGjCgsLddJJJ3U41uVyyeUK7cA1AADQd1nacuN0OjV58mStWrUqsM/n82nVqlXKy8s75O/x+XxB42oAAEDssnwqeEFBgebNm6cpU6Zo2rRpWrp0qerr6zV//nxJ0qWXXqqhQ4eqsLBQUtsYmilTpmj06NHyeDx69dVX9fe//10PPviglZcBAAD6CMvDzZw5c1ReXq7FixerpKREEydO1GuvvRYYZFxcXCybbX8DU319va666irt2rVL8fHxOuKII/T4449rzpw5Vl0CAADoQyxf5ybSWOcGAID+pzt/v/vlbCkAAICuEG4AAEBUIdwAAICoQrgBAABRhXADAACiCuEGAABEFcINAACIKpYv4hdp/mV9ampqLK4EAAAcKv/f7UNZni/mwk1tba0kKScnx+JKAABAd9XW1io1NfWgx8TcCsU+n0+7d+9WcnKyDMMI6XfX1NQoJydHO3fujJnVj7lmrjlaxdo1x9r1Slxzf7tm0zRVW1urIUOGBD2WqTMx13Jjs9k0bNiwsP6OlJSUfvcfTW9xzbGBa45+sXa9Etfcn3xbi40fA4oBAEBUIdwAAICoQrgJIZfLpSVLlsjlclldSsRwzbGBa45+sXa9EtcczWJuQDEAAIhutNwAAICoQrgBAABRhXADAACiCuEGAABEFcJNiNx///3Kzc2V2+3W9OnTtXbtWqtLCptbb71VhmEEbUcccYTVZYXU//zP/2jWrFkaMmSIDMPQP/7xj6DPTdPU4sWLNXjwYMXHxys/P19fffWVNcWGyLdd849//OMO9/3000+3ptgQKSws1NSpU5WcnKxBgwbpnHPO0aZNm4KOaWpq0sKFCzVw4EAlJSXphz/8oUpLSy2quPcO5ZpPOumkDvf6pz/9qUUV996DDz6o8ePHBxauy8vL07/+9a/A59F2j6Vvv+Zou8ffRLgJgZUrV6qgoEBLlixRUVGRJkyYoJkzZ6qsrMzq0sLmqKOO0p49ewLbu+++a3VJIVVfX68JEybo/vvv7/TzP/zhD7rnnnu0bNky/ec//1FiYqJmzpyppqamCFcaOt92zZJ0+umnB933p556KoIVht4777yjhQsX6oMPPtAbb7yhlpYWnXbaaaqvrw8cc9111+mf//ynnnnmGb3zzjvavXu3fvCDH1hYde8cyjVL0oIFC4Lu9R/+8AeLKu69YcOG6Y477tC6dev00Ucf6Xvf+55mz56tzz77TFL03WPp269Ziq573IGJXps2bZq5cOHCwM9er9ccMmSIWVhYaGFV4bNkyRJzwoQJVpcRMZLMF154IfCzz+czs7OzzTvvvDOwr6qqynS5XOZTTz1lQYWh981rNk3TnDdvnjl79mxL6omUsrIyU5L5zjvvmKbZdl/j4uLMZ555JnDMxo0bTUnmmjVrrCozpL55zaZpmieeeKJ57bXXWldUBKSnp5t/+ctfYuIe+/mv2TSj/x7TctNLzc3NWrdunfLz8wP7bDab8vPztWbNGgsrC6+vvvpKQ4YM0ahRo3TRRRepuLjY6pIiZtu2bSopKQm656mpqZo+fXpU33NJWr16tQYNGqSxY8fqyiuv1N69e60uKaSqq6slSQMGDJAkrVu3Ti0tLUH3+ogjjtDw4cOj5l5/85r9nnjiCWVkZOjoo4/WokWL1NDQYEV5Ief1erVixQrV19crLy8vJu7xN6/ZL1rvsRSDD84MtYqKCnm9XmVlZQXtz8rK0hdffGFRVeE1ffp0PfbYYxo7dqz27Nmj3/zmN/rOd76jDRs2KDk52erywq6kpESSOr3n/s+i0emnn64f/OAHGjlypLZs2aKbbrpJZ5xxhtasWSO73W51eb3m8/n085//XDNmzNDRRx8tqe1eO51OpaWlBR0bLfe6s2uWpAsvvFAjRozQkCFD9Mknn+hXv/qVNm3apOeff97Canvn008/VV5enpqampSUlKQXXnhB48aN0/r166P2Hnd1zVJ03uMDEW7QbWeccUbg/fjx4zV9+nSNGDFCTz/9tC6//HILK0M4zZ07N/D+mGOO0fjx4zV69GitXr1ap5xyioWVhcbChQu1YcOGqBs/djBdXfNPfvKTwPtjjjlGgwcP1imnnKItW7Zo9OjRkS4zJMaOHav169erurpazz77rObNm6d33nnH6rLCqqtrHjduXFTe4wPRLdVLGRkZstvtHUbWl5aWKjs726KqIistLU2HH364Nm/ebHUpEeG/r7F8zyVp1KhRysjIiIr7fvXVV+vll1/W22+/rWHDhgX2Z2dnq7m5WVVVVUHHR8O97uqaOzN9+nRJ6tf32ul0asyYMZo8ebIKCws1YcIE/elPf4rqe9zVNXcmGu7xgQg3veR0OjV58mStWrUqsM/n82nVqlVBfZvRrK6uTlu2bNHgwYOtLiUiRo4cqezs7KB7XlNTo//85z8xc88ladeuXdq7d2+/vu+maerqq6/WCy+8oLfeeksjR44M+nzy5MmKi4sLutebNm1ScXFxv73X33bNnVm/fr0k9et7/U0+n08ejycq73FX/Nfcmai7x1aPaI4GK1asMF0ul/nYY4+Zn3/+ufmTn/zETEtLM0tKSqwuLSyuv/56c/Xq1ea2bdvM9957z8zPzzczMjLMsrIyq0sLmdraWvPjjz82P/74Y1OSeffdd5sff/yxuWPHDtM0TfOOO+4w09LSzBdffNH85JNPzNmzZ5sjR440GxsbLa685w52zbW1teYvfvELc82aNea2bdvMN99805w0aZJ52GGHmU1NTVaX3mNXXnmlmZqaaq5evdrcs2dPYGtoaAgc89Of/tQcPny4+dZbb5kfffSRmZeXZ+bl5VlYde982zVv3rzZvO2228yPPvrI3LZtm/niiy+ao0aNMr/73e9aXHnP3XjjjeY777xjbtu2zfzkk0/MG2+80TQMw3z99ddN04y+e2yaB7/maLzH30S4CZF7773XHD58uOl0Os1p06aZH3zwgdUlhc2cOXPMwYMHm06n0xw6dKg5Z84cc/PmzVaXFVJvv/22KanDNm/ePNM026aD33LLLWZWVpbpcrnMU045xdy0aZO1RffSwa65oaHBPO2008zMzEwzLi7OHDFihLlgwYJ+H+A7u15J5qOPPho4prGx0bzqqqvM9PR0MyEhwTz33HPNPXv2WFd0L33bNRcXF5vf/e53zQEDBpgul8scM2aMecMNN5jV1dXWFt4Ll112mTlixAjT6XSamZmZ5imnnBIINqYZfffYNA9+zdF4j7/JME3TjFw7EQAAQHgx5gYAAEQVwg0AAIgqhBsAABBVCDcAACCqEG4AAEBUIdwAAICoQrgBAABRhXADIOatXr1ahmF0eL4QgP6JcAMAAKIK4QYAAEQVwg0Ay/l8PhUWFmrkyJGKj4/XhAkT9Oyzz0ra32X0yiuvaPz48XK73TruuOO0YcOGoO947rnndNRRR8nlcik3N1d33XVX0Ocej0e/+tWvlJOTI5fLpTFjxuiRRx4JOmbdunWaMmWKEhISdPzxx2vTpk3hvXAAYUG4AWC5wsJC/e1vf9OyZcv02Wef6brrrtPFF1+sd955J3DMDTfcoLvuuksffvihMjMzNWvWLLW0tEhqCyUXXHCB5s6dq08//VS33nqrbrnlFj322GOB8y+99FI99dRTuueee7Rx40b9+c9/VlJSUlAdN998s+666y599NFHcjgcuuyyyyJy/QBCiwdnArCUx+PRgAED9OabbyovLy+w/4orrlBDQ4N+8pOf6OSTT9aKFSs0Z84cSdK+ffs0bNgwPfbYY7rgggt00UUXqby8XK+//nrg/F/+8pd65ZVX9Nlnn+nLL7/U2LFj9cYbbyg/P79DDatXr9bJJ5+sN998U6eccook6dVXX9VZZ52lxsZGud3uMP8rAAglWm4AWGrz5s1qaGjQqaeeqqSkpMD2t7/9TVu2bAkcd2DwGTBggMaOHauNGzdKkjZu3KgZM2YEfe+MGTP01Vdfyev1av369bLb7TrxxBMPWsv48eMD7wcPHixJKisr6/U1Aogsh9UFAIhtdXV1kqRXXnlFQ4cODfrM5XIFBZyeio+PP6Tj4uLiAu8Nw5DUNh4IQP9Cyw0AS40bN04ul0vFxcUaM2ZM0JaTkxM47oMPPgi8r6ys1JdffqkjjzxSknTkkUfqvffeC/re9957T4cffrjsdruOOeYY+Xy+oDE8AKIXLTcALJWcnKxf/OIXuu666+Tz+XTCCSeourpa7733nlJSUjRixAhJ0m233aaBAwcqKytLN998szIyMnTOOedIkq6//npNnTpVt99+u+bMmaM1a9bovvvu0wMPPCBJys3N1bx583TZZZfpnnvu0YQJE7Rjxw6VlZXpggsusOrSAYQJ4QaA5W6//XZlZmaqsLBQW7duVVpamiZNmqSbbrop0C10xx136Nprr9VXX32liRMn6p///KecTqckadKkSXr66ae1ePFi3X777Ro8eLBuu+02/fjHPw78jgcffFA33XSTrrrqKu3du1fDhw/XTTfdZMXlAggzZksB6NP8M5kqKyuVlpZmdTkA+gHG3AAAgKhCuAEAAFGFbikAABBVaLkBAABRhXADAACiCuEGAABEFcINAACIKoQbAAAQVQg3AAAgqhBuAABAVCHcAACAqEK4AQAAUeX/A+urYQ3UCc1uAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_2.history['mae'])\n",
    "plt.title(\"Model MAE\")\n",
    "plt.ylabel(\"mae\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407445c8-203d-4fd0-8088-3e24896b6615",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "06bb54ab-0617-4ff2-aab2-f07a7ae5c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Dense(128,input_shape = (X_train.shape[1],),activation= 'relu'))\n",
    "model_3.add(Dense(64,activation='relu'))\n",
    "model_3.add(Dense(32,activation = 'relu'))\n",
    "model_3.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b8a81957-357d-4f15-af0e-81f444956f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Compile the model \n",
    "\n",
    "\n",
    "model_3.compile(loss = tf.keras.losses.MeanAbsoluteError() ,\n",
    "               optimizer= keras.optimizers.Adam(learning_rate=.001) , \n",
    "               metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "74579828-fb3d-4227-b800-18a1ef8b4960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "7/7 [==============================] - 2s 80ms/step - loss: 0.7388 - mae: 0.7388 - val_loss: 0.5547 - val_mae: 0.5547\n",
      "Epoch 2/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.5130 - mae: 0.5130 - val_loss: 0.4139 - val_mae: 0.4139\n",
      "Epoch 3/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3749 - mae: 0.3749 - val_loss: 0.3424 - val_mae: 0.3424\n",
      "Epoch 4/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3491 - mae: 0.3491 - val_loss: 0.3588 - val_mae: 0.3588\n",
      "Epoch 5/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3300 - mae: 0.3300 - val_loss: 0.3441 - val_mae: 0.3441\n",
      "Epoch 6/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3373 - mae: 0.3373 - val_loss: 0.3457 - val_mae: 0.3457\n",
      "Epoch 7/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3307 - mae: 0.3307 - val_loss: 0.3461 - val_mae: 0.3461\n",
      "Epoch 8/1000\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.3321 - mae: 0.3321 - val_loss: 0.3407 - val_mae: 0.3407\n",
      "Epoch 9/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3301 - mae: 0.3301 - val_loss: 0.3519 - val_mae: 0.3519\n",
      "Epoch 10/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3232 - mae: 0.3232 - val_loss: 0.3428 - val_mae: 0.3428\n",
      "Epoch 11/1000\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.3180 - mae: 0.3180 - val_loss: 0.3380 - val_mae: 0.3380\n",
      "Epoch 12/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3153 - mae: 0.3153 - val_loss: 0.3439 - val_mae: 0.3439\n",
      "Epoch 13/1000\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.3169 - mae: 0.3169 - val_loss: 0.3328 - val_mae: 0.3328\n",
      "Epoch 14/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3139 - mae: 0.3139 - val_loss: 0.3377 - val_mae: 0.3377\n",
      "Epoch 15/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3083 - mae: 0.3083 - val_loss: 0.3331 - val_mae: 0.3331\n",
      "Epoch 16/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3093 - mae: 0.3093 - val_loss: 0.3358 - val_mae: 0.3358\n",
      "Epoch 17/1000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.3136 - mae: 0.3136 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 18/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3070 - mae: 0.3070 - val_loss: 0.3417 - val_mae: 0.3417\n",
      "Epoch 19/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3105 - mae: 0.3105 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 20/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3085 - mae: 0.3085 - val_loss: 0.3393 - val_mae: 0.3393\n",
      "Epoch 21/1000\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.3065 - mae: 0.3065 - val_loss: 0.3405 - val_mae: 0.3405\n",
      "Epoch 22/1000\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.3042 - mae: 0.3042 - val_loss: 0.3391 - val_mae: 0.3391\n",
      "Epoch 23/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3096 - mae: 0.3096 - val_loss: 0.3374 - val_mae: 0.3374\n",
      "Epoch 24/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3058 - mae: 0.3058 - val_loss: 0.3488 - val_mae: 0.3488\n",
      "Epoch 25/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3069 - mae: 0.3069 - val_loss: 0.3415 - val_mae: 0.3415\n",
      "Epoch 26/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3050 - mae: 0.3050 - val_loss: 0.3390 - val_mae: 0.3390\n",
      "Epoch 27/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3068 - mae: 0.3068 - val_loss: 0.3412 - val_mae: 0.3412\n",
      "Epoch 28/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3008 - mae: 0.3008 - val_loss: 0.3414 - val_mae: 0.3414\n",
      "Epoch 29/1000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.3090 - mae: 0.3090 - val_loss: 0.3441 - val_mae: 0.3441\n",
      "Epoch 30/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3018 - mae: 0.3018 - val_loss: 0.3562 - val_mae: 0.3562\n",
      "Epoch 31/1000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.3133 - mae: 0.3133 - val_loss: 0.3519 - val_mae: 0.3519\n",
      "Epoch 32/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3002 - mae: 0.3002 - val_loss: 0.3418 - val_mae: 0.3418\n",
      "Epoch 33/1000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.3076 - mae: 0.3076 - val_loss: 0.3389 - val_mae: 0.3389\n",
      "Epoch 34/1000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.3001 - mae: 0.3001 - val_loss: 0.3493 - val_mae: 0.3493\n",
      "Epoch 35/1000\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.3016 - mae: 0.3016 - val_loss: 0.3428 - val_mae: 0.3428\n",
      "Epoch 36/1000\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.2968 - mae: 0.2968 - val_loss: 0.3430 - val_mae: 0.3430\n",
      "Epoch 37/1000\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.2960 - mae: 0.2960 - val_loss: 0.3427 - val_mae: 0.3427\n",
      "Epoch 38/1000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.2982 - mae: 0.2982 - val_loss: 0.3488 - val_mae: 0.3488\n"
     ]
    }
   ],
   "source": [
    "## fit the model \n",
    "callback = EarlyStopping(monitor = 'val_loss', patience = 25)\n",
    "# Create a ModelCheckpoint callback\n",
    "checkpoint_3 = ModelCheckpoint(\"best_model_3.h5\", save_best_only=True, monitor='val_loss')\n",
    "\n",
    "train_3 = model_3.fit(X_train,y_train, epochs =1000,verbose =1,validation_data=(X_val,y_val), batch_size= 32,callbacks=[callback,checkpoint_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9c581ee2-68ae-4059-acd1-b0d4686eae52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_3.history['mae'])\n",
    "plt.title(\"Model MAE\")\n",
    "plt.ylabel(\"mae\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e52df9-d2de-4041-a268-d62cfec4f726",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b1be7d48-ea97-4e62-9ff5-c3ecc7713b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the best weights from training \n",
    "model_1.load_weights(\"best_model_1.h5\")\n",
    "model_2.load_weights(\"best_model_2.h5\")\n",
    "model_3.load_weights(\"best_model_3.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "11234398-1175-4a55-8b5b-cccf142388ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step\n",
      "Keras Sequential Model 1 MAE: 0.3617742813059919\n",
      "3/3 [==============================] - 0s 7ms/step\n",
      "Keras Sequential Model 2 MAE: 0.36177983276332537\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "Keras Sequential Model 3 MAE: 0.3685168016907347\n"
     ]
    }
   ],
   "source": [
    "##Model 1 predictions \n",
    "predictions_keras_1_standard= model_1.predict(X_test)\n",
    "predictions_keras_1 = scaler_y.inverse_transform(predictions_keras_1_standard)\n",
    "predictions_keras_1 = np.round(predictions_keras_1,3)\n",
    "\n",
    "\n",
    "\n",
    "keras_model_mae_1 = mae(y_test,predictions_keras_1_standard)\n",
    "keras_model_mae_normal1 = mae(test_normal_actual,predictions_keras_1)\n",
    "print(f\"Keras Sequential Model 1 MAE: {keras_model_mae_1}\")\n",
    "\n",
    "\n",
    "#Model 2 predictions\n",
    "predictions_keras_2_standard = model_2.predict(X_test)\n",
    "predictions_keras_2 = scaler_y.inverse_transform(predictions_keras_2_standard)\n",
    "predictions_keras_2 = np.round(predictions_keras_2,3)\n",
    "\n",
    "keras_model_mae_2 = mae(y_test,predictions_keras_2_standard)\n",
    "keras_model_mae_normal2 = mae(test_normal_actual,predictions_keras_2)\n",
    "print(f\"Keras Sequential Model 2 MAE: {keras_model_mae_2}\")\n",
    "\n",
    "#3Model 3 Predictions \n",
    "predictions_keras_3_standard = model_3.predict(X_test)\n",
    "predictions_keras_3 = scaler_y.inverse_transform(predictions_keras_3_standard)\n",
    "predictions_keras_3 = np.round(predictions_keras_3,3)\n",
    "\n",
    "keras_model_mae_3 = mae(y_test,predictions_keras_3_standard)\n",
    "keras_model_mae_normal3 = mae(test_normal_actual,predictions_keras_3)\n",
    "print(f\"Keras Sequential Model 3 MAE: {keras_model_mae_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b0bb6-7581-41b4-a682-2871d2345546",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e671b07e-fae0-4738-a830-3fdbb84ae862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4900.0           4861.268066\n",
      "1              4750.0           4902.505859\n",
      "2              5750.0           5424.021973\n",
      "3              3700.0           3366.647949\n",
      "4              4300.0           4099.086914\n",
      "..                ...                   ...\n",
      "64             4725.0           5072.271973\n",
      "65             3800.0           3472.678955\n",
      "66             4250.0           3706.414062\n",
      "67             6000.0           5356.810059\n",
      "68             3200.0           3383.594971\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4900.0           4877.730957\n",
      "1              4750.0           4922.409180\n",
      "2              5750.0           5396.698242\n",
      "3              3700.0           3395.897949\n",
      "4              4300.0           4213.819824\n",
      "..                ...                   ...\n",
      "64             4725.0           5083.926758\n",
      "65             3800.0           3453.418945\n",
      "66             4250.0           3685.170898\n",
      "67             6000.0           5321.006836\n",
      "68             3200.0           3486.874023\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4900.0           4894.344238\n",
      "1              4750.0           4933.943848\n",
      "2              5750.0           5433.276855\n",
      "3              3700.0           3375.501953\n",
      "4              4300.0           4100.967773\n",
      "..                ...                   ...\n",
      "64             4725.0           5111.916016\n",
      "65             3800.0           3460.728027\n",
      "66             4250.0           3709.496094\n",
      "67             6000.0           5363.407227\n",
      "68             3200.0           3313.809082\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "    Actual Weight (g)  Predicted Weight (g)\n",
      "0              4900.0           4671.171739\n",
      "1              4750.0           4035.223160\n",
      "2              5750.0           4283.023273\n",
      "3              3700.0           5723.124348\n",
      "4              4300.0           4997.433260\n",
      "..                ...                   ...\n",
      "64             4725.0           3822.062473\n",
      "65             3800.0           5323.481061\n",
      "66             4250.0           4838.184753\n",
      "67             6000.0           4576.063260\n",
      "68             3200.0           5506.365171\n",
      "\n",
      "[69 rows x 2 columns]\n",
      "Keras1 Loss: 289.70206351902175\n",
      "Keras2 Loss: 289.7065182008605\n",
      "Keras3 Loss: 295.1013962013134\n",
      "Custom Loss: 1195.2115854379038\n"
     ]
    }
   ],
   "source": [
    "results_keras1 = np.concatenate((test_normal_actual, predictions_keras_1), axis=1)\n",
    "results_keras2 = np.concatenate((test_normal_actual, predictions_keras_2), axis=1)\n",
    "results_keras3 = np.concatenate((test_normal_actual, predictions_keras_3), axis=1)\n",
    "\n",
    "results_keras1_df = pd.DataFrame(results_keras1, columns=['Actual Weight (g)', 'Predicted Weight (g)'])\n",
    "results_keras2_df = pd.DataFrame(results_keras2, columns=['Actual Weight (g)', 'Predicted Weight (g)'])\n",
    "results_keras3_df = pd.DataFrame(results_keras3, columns=['Actual Weight (g)', 'Predicted Weight (g)'])\n",
    "\n",
    "print(results_keras1_df)\n",
    "print(results_keras2_df)\n",
    "print(results_keras3_df)\n",
    "print(results_test_df)\n",
    "\n",
    "print(f\"Keras1 Loss: {keras_model_mae_normal1}\")\n",
    "print(f\"Keras2 Loss: {keras_model_mae_normal2}\")\n",
    "print(f\"Keras3 Loss: {keras_model_mae_normal3}\")\n",
    "print(f\"Custom Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281bb0db-6994-4ff0-a985-2efef13a6654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47469ff6-f318-4388-8105-7dc6a67a3be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
